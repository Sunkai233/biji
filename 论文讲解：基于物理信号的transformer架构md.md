## 基于物理信号的transformer架构：



# 时频 Transformer

本节将详细介绍所提出的时频 Transformer（TFT），其整体图如图 2 所示。网络架构主要由分词器、编码器和分类器组成。

### 分词器

原始 Transformer 接受一维序列标记，并通过字典查询使用分词器获得标记嵌入。然而，这里要处理的输入标记是时频表示（TFR），而不是可以在字典中查询的单词。因此，需要设计一个特定的分词器。为了处理二维 TFR 数据，我们将设计一个新的分词器模块，主要包括扁平化、分割、线性映射和添加位置编码。

#### 标记嵌入

为了获得标记序列，TFR 沿时间方向被分割成若干个补丁。具体地，给定一个 TFR $x \in \mathbb{R}^{N_t \times N_f \times C}$，其中 $N_t$ 和 $N_f$ 分别是时间和频率方向的长度。$C$ 是通道数，通常指多传感器信号的堆叠层。我们首先将 $x$ 重塑为序列：

$$
x' \in \mathbb{R}^{N_t \times (N_f \cdot C)}
$$

来扁平化多个通道，并沿时间方向切割它以获得长度为 $N_t$ 的补丁序列：

$$
[x_1^p, x_2^p, \dots, x_{N_t}^p]
$$

其中，

$$
x_i^p \in \mathbb{R}^{N_f \cdot C}, \quad i = 1, \dots, N_t
$$

然后，为了获得维度为 $d_{\text{model}}$ 的标记嵌入序列，使用可学习的线性变换来获得投影补丁序列 $x_t$。这个过程表示为：

$$
x_{\text{reshape}} \longrightarrow x_p
$$

$$
x_t = x_p W_t
$$

其中，$W_t \in \mathbb{R}^{N_f \times d_{\text{model}}}$ 表示可学习的线性映射。此处理基于 TFR 是通过在一段时间内拼接信号的瞬时频谱形成的这一观点。这与 ViT 对图像的网格分割不同，因为我们认为网格切割 TFR 不会在每个补丁中保留某个时间的瞬时频谱估计。分割的 TFR 补丁可以被视为一段时间内瞬时频谱的序列，处理这样的时间序列是基于 Transformer 结构的优势。

- 具体步骤

  假设你有一个时频图，大小是 **时间×频率×通道** (Nt × Nf × C)：

  1. 重塑（Reshape）

  - 原始：Nt × Nf × C （比如 100个时间点 × 64个频率 × 3个传感器）
  - 重塑后：Nt × (Nf·C) （变成 100 × 192）
  - **目的**：把多通道的频率信息拍平成一维

  2. 时间切片

  - 沿着时间轴切片，每个时间点就是一个"patch"
  - 得到序列：[x₁ᵖ, x₂ᵖ, ..., x₁₀₀ᵖ]
  - 每个 xᵢᵖ 的大小是 192（包含了该时刻所有频率和通道的信息）

  3. 线性映射

  - 用一个可学习的矩阵 Wₜ (192 × dₘₒdₑₗ)
  - 把每个192维的patch映射到dₘₒdₑₗ维（比如512维）
  - 这样就得到了Transformer能处理的标准嵌入向量

#### 类标记

类似于 BERT 中的类标记，一个随机初始化的可训练嵌入：

$$
x_0^t = x_{\text{class}} \in \mathbb{R}^{d_{\text{model}}}
$$

被添加到嵌入标记序列的开头。因此，得到一个长度为 $N_t + 1$ 的嵌入序列：

$$
x_t = [x_{\text{class}}; x_1^p W_t, x_2^p W_t, \dots, x_{N_t}^p W_t]
$$

注意，类标记在经过后续 Transformer 编码器处理后的输出 $z_0^N$ 将作为 TFR 的隐藏表示。

#### 位置编码

由于 Transformer 不包含递归或卷积操作，为了充分利用序列顺序，我们应该向嵌入序列中注入一些关于标记相对或绝对位置的信息。原始 Transformer 采用一种基于语料字典和标记位置的正弦位置编码，这不适合我们试图解决的问题。我们提议使用可学习的位置编码：

$$
E_{\text{pos}} \in \mathbb{R}^{(N_t+1) \times d_{\text{model}}}
$$

通过学习过程更灵活地提取位置信息。位置编码和标记嵌入相加得到输入嵌入：

$$
z_0 = [x_{\text{class}}; x_1^p W_t, x_2^p W_t, \dots, x_{N_t}^p W_t] + E_{\text{pos}}
$$

考虑两种类型的可学习位置编码，1D 和 2D：

1) **1D 位置编码**

$$
E_{1d_{\text{pos}}} \in \mathbb{R}^{(N_t+1) \times d_{\text{model}}}
$$

从向量：

$$
e_{1d} \in \mathbb{R}^{N_t+1}
$$

广播而来，即：

$$
E_{1d_{\text{pos}}} = \text{broadcast}(e_{1d})
$$

2) **2D 位置编码**

$$
E_{2d_{\text{pos}}} \in \mathbb{R}^{(N_t+1) \times d_{\text{model}}}
$$

是一个维度为 $(N_t + 1) \times d_{\text{model}}$ 的学习矩阵，因此它可以同时编码标记之间和标记内部的位置信息。后续案例研究将分析这两种编码的性能。



# 数据增强

为了满足 Transformer 架构的数据需求并防止过拟合，提出了以下随机数据增强方法，其灵感来自自监督对比学习的进展。这些包括添加高斯噪声、切除部分信号、裁剪和调整信号大小以及跨时间移动信号，如图 2 所述。每次增强的参数，例如高斯噪声的标准差或移动的步长，是从一个值范围内均匀采样的，这使我们能够在每次增强后生成一个独特的样本。在训练期间，我们可以通过定义一个增强训练样本的概率并保持测试样本不变来改变增强的量。有关所用参数的更多详细信息可以在附录 A 中找到。

#### 数据增强策略（APPENDIX A）

这是提升模型泛化能力的关键技术。系统随机选择以下8种增强方式之一：

##### 单独操作：

1. **高斯噪声**：添加标准差为0-0.05的随机噪声，模拟真实环境的干扰
2. **时移**：将信号在时间轴上前后移动，模拟采样时间的微小差异
3. **遮挡**：将100-500个连续数据点置零，模拟传感器短暂失效
4. **裁剪**：截取一半长度的信号段，然后拉伸回原长度，模拟不同转速

##### 组合操作：

5-8. 将上述基础操作两两组合，创造更复杂的变化

**目的**：让模型在各种干扰条件下都能准确识别故障模式。



### 信号标记化（Tokenization）（线性映射加强版）

当考虑输入振动信号的长度时，对标记器（tokenizer）的需求是显而易见的。长达数千个数据点，直接使用这些数据对于 Transformer 来说计算成本将高得令人望而却步；此外，并非所有数据点都代表有用的信息。因此，提出了三种标记化策略：常数（constant）、CNN 和傅里叶（Fourier）。

常数标记器只是通过增加一个通道维度来重塑输入信号，除了时间维度。这通过将来自多个数据点的信息堆叠到每个令牌的向量中来减少序列长度。CNN 标记器更复杂，它增加了一个通道维度，并通过卷积输入信号来减少序列长度。最后，傅里叶标记器提取每个令牌的顶部傅里叶模式。这些模式的实部和虚部振幅以及频率在通道维度上堆叠。关于提出的标记器的更多细节也可以在附录 B 中找到。

#### 信号分词方法（APPENDIX B）

这是将原始信号转换为Transformer可处理格式的不同策略：

##### 1. 常量分词器（Constant）

```
简单重塑：(批次, 序列长度, 1) → (批次, 序列长度/d, d)
就像把一长串数字按固定长度切段
```

##### 2. CNN分词器

```
通过两层1D卷积提取特征：
原始信号 → 4通道特征 → 8通道特征
相当于先用CNN"理解"信号，再交给Transformer
```

##### 3. 傅里叶分词器

```
(批次, 序列长度, 1) → (批次, 40, 3)
提取频域的前40个主要模式，每个模式包含：实部、虚部、频率
直接从频率角度理解信号
```



## FiLM的本质机制

##### γ（缩放参数）的作用

```
γ > 1：放大该特征维度 → "在当前条件下这个特征很重要"
γ < 1：缩小该特征维度 → "在当前条件下这个特征不重要"
γ ≈ 0：几乎忽略该特征
```

##### β（偏移参数）的作用

```
调整基线激活水平，补偿不同条件下振动幅度的系统性差异
比如：高风速下整体振动增强，β调整基础偏移量
```

##### 实际例子

假设检测轴承故障：

```
低风速条件：
- γ可能放大低频振动特征（故障信号明显）
- β调整较小偏移

高风速条件：  
- γ可能放大高频特征（需要从噪声中提取故障）
- β调整较大偏移（补偿整体振动增强）
```

#### 语言模型中的类似机制

#### Conditional Layer Normalization

python

```python
# 类似FiLM，根据条件动态调整归一化参数
h_norm = (h - μ) / σ
h_conditioned = γ(condition) * h_norm + β(condition)
```

### 基本原理

python

```python
# 标准Layer Norm
h_norm = (h - mean(h)) / std(h)

# 条件化Layer Norm  
γ, β = condition_network(condition)  # 根据条件生成参数
h_conditioned = γ * h_norm + β
```

### 应用场景

- **对话系统**：根据用户情感状态调整回复风格
- **多任务学习**：不同任务使用不同的归一化参数

### 实际例子

python

```python
# 情感条件化
if emotion == "angry":
    γ, β = anger_modulation_net(emotion_embedding)
    # γ可能放大激进词汇的特征，β调整整体语调
```