# 许可证：arXiv.org 永久性非独占许可
arXiv:2503.05810v2 [cs.LG] 2025年3月11日
# 一种基于通用模板预测化学反应产物的Transformer模型
Derin OZER1, Sylvain LAMPRIER1, Thomas CAUCHY2, Nicolas GUTOWSKI1, Benoit DA MOTA1
1法国昂热大学，LERIA，SFR MATRIX，F-49000 昂热
电子邮件：{derin.ozer, sylvain.lamprier, nicolas.gutowski, benoit.damota}@univ-angers.fr
2法国昂热大学，CNRS，MOLTECH-ANJOU，SFR MATRIX，F-49000 昂热
电子邮件：thomas.cauchy@univ-angers.fr

## 摘要
准确预测化学反应结果（产物）是计算化学中的一个主要挑战。当前的模型严重依赖于高度特定的反应模板或无模板方法，这两种方法都存在局限性。为了解决这些局限性，本工作提出了 Broad Reaction Set (BRS)，这是一个包含 20 个通用反应模板的数据集，允许高效探索化学空间。此外，还介绍了 ProPreT5，这是一个为化学领域定制的 T5 模型，它在刚性模板和无模板方法之间取得了平衡。ProPreT5 展示了其生成准确、有效且现实的反应产物的能力，使其成为一个超越当前最先进水平的、有前途的解决方案，可用于复杂的反应产物预测任务。

（

#### 问题背景

想象你是一个厨师，想要预测把不同食材混合在一起会做出什么菜。在化学中，科学家们面临类似的挑战——当把不同的化学物质混合在一起时，会产生什么新的化学物质？

#### 现有方法的局限性

目前有两种主要的预测方法，就像两种不同的烹饪方式：

**方法一：严格按食谱（模板方法）**

- 就像有一本详细的食谱书，每个食谱都很具体
- 优点：按食谱做，结果比较准确
- 缺点：食谱有限，遇到新的食材组合就不知道怎么办了

**方法二：自由发挥（无模板方法）**

- 就像一个厨师凭经验和直觉随意搭配
- 优点：可以处理任何食材组合
- 缺点：结果不太可靠，可能做出奇怪的菜

#### 这项研究的创新

研究者们提出了一个更好的解决方案：

**BRS (Broad Reaction Set)：万能食谱集**

- 创建了20个"万能食谱"，每个都很通用
- 就像有了20个基础烹饪技法（炒、煮、蒸、烤等），可以应对大部分情况

**ProPreT5：智能厨师助手**

- 这是一个专门为化学"烹饪"训练的人工智能
- 它既会使用通用食谱，又能灵活变通
- 就像一个既懂基础技法，又能创新的智能厨师

#### 成果

这个"智能厨师"能够：

- 准确预测化学反应会产生什么物质
- 生成的结果既科学合理又实用
- 比现有的方法都要好

简单来说，这项研究就是开发了一个更聪明的化学反应预测系统，让科学家们能够更准确地预知化学实验的结果。

）

## I 引言
准确预测化学反应结果（产物）是化学中的一项重要任务，因为它允许构建有机合成路线。给定一组反应分子（即反应物），目标是确定结果，即产物。这项任务尤其具有挑战性，需要彻底理解化学物质、化合物类别、反应、潜在的反应性模式以及反应条件。这种理解对于各种应用至关重要，包括：药物发现 [1]、材料科学 [2] 和绿色化学 [3]。

有机化学合成路线规划通常是一种专家驱动和基于规则的方法。然而，机器学习的进步已经改变了化学信息学领域。该领域最有前途的技术之一是 Transformer 模型 [4]，它最初是为自然语言处理（NLP）中的序列到序列任务而开发的 [5]。Transformer 架构通过自注意力机制使模型能够理解上下文，从而彻底改变了 NLP。这种能力帮助模型学会动态地为输入数据的不同部分分配权重，以产生更准确和与上下文相关的输出。Transformer 架构也已成功应用于化学领域的各种任务，例如单步化学反应预测 [6, 7, 8]、逆合成 [8, 9, 10]、分子生成 [11, 12, 13] 和分子性质预测 [14, 15, 16]。然而，这些模型高度依赖于用于训练的数据的质量和多样性。

分子数据可以以其分子结构的图遍历（graph traversal）所衍生的字符串形式表示。这种紧凑的表示称为简化分子输入行条目系统（SMILES）[17]，它编码分子，其中每个字符代表一个原子或键，提供了一种对计算目的和数据库存储有用的线性表示。当图遍历遵循符号（notation）建立的特定顺序时，生成的 SMILES 是规范的（canonical），并且一个分子只有一个规范的 SMILES，但可以有多个非规范的 SMILES 表示。

SMILES 符号也可用于表示化学反应，编码反应物（起始物料）、试剂（辅助反应但不发生转化的物质）和产物（产生的最终分子）。另一方面，SMARTS [18] 符号通过表示原子和键的模式或分子内的子结构来扩展 SMILES。它允许识别反应物中的官能团，创建捕获一般反应性模式的反应模板，并改进化学行为的预测和识别。

本文研究的反应产物预测任务通常使用两种不同的方法来解决：基于模板的方法，依赖于预定义的规则（例如由反应 SMILES 或 SMARTS 表示的反应）；以及无模板方法，直接从大型数据集中学习反应性模式，而不依赖于预定义的规则或模式。

公开可用的反应数据集大多是通过从美国专利商标局（USPTO）[19] 发布的专利中提取数据获得的。这些反应使用 SMILES 表示法及其反应物和试剂来表示。先前的工作通过将其过滤到 470,000 个示例并将其拆分为训练/测试/验证集，创建了一个名为 USPTO MIT [20] 的子数据集。USPTO MIT 数据集在文献中被广泛使用。

由于 USPTO 数据集源自专利，它们提供了高度特定的反应，专门针对特定的反应物-产物组合。此外，使用这些数据集探索的化学空间仅限于专利反应中包含的信息。因此，我们假设仅依赖专利会在训练期间引入偏差，导致化学空间的很大一部分未被考虑。虽然先前的工作 [8] 已经证明了使用该数据集预测反应结果具有很高的准确性，但此类模型缺乏分子发现和实际应用，突出了一个关键限制：仅基于专利化学空间训练的模型存在泛化问题，并且不适合实际应用 [21]。此外，当将反应视为强化学习中的动作时，现有数据集的局限性变得尤为明显。反应的特异性限制了使用这些算法探索所有可能的组合空间。

这些局限性和观察结果促使我们提出了一个名为 Broad Reaction Set (BRS) 的新数据集，它弥合了正则表达式和过于具体的反应 SMILES 之间的差距。这些反应使用 SMARTS 语法表达，并表示更广泛的转化模式，而不是单一的、特定的反应。这种方法使我们能够探索更全面的化学空间部分，这是有机合成和分子发现任务的基本步骤。

反应模板数据集经常被认为难以维护，因为反应物-产物组合的数量是无穷无尽且难以管理的 [7]，这导致许多人倾向于将无模板方法视为该领域的未来。虽然维护高度特异性反应（例如 USPTO 中的反应）的模板无疑具有挑战性，但我们认为，使用通用反应（如提议的 BRS 中的反应）结合了反应模板的好处，同时解决了维护问题。

为了研究 BRS 的特性，我们发布了一个定制的 T5 [22] 模型 ProPreT5，专为反应预测问题设计。ProPreT5 分别使用 BRS 和 USPTO MIT 数据集在基于模板和无模板的设置下进行了训练。这种方法带来了几个关键见解。当使用反应模板和 USPTO MIT 数据集预测产物时，该任务被证明是微不足道的，因为反应模板中的产物模式基本上列举了产物分子，显著简化了预测。本研究的目标不是改进 USPTO MIT 基线，而是强调其局限性并提出一个更现实的替代方案。为此，我们选择了一个经济高效的训练设置，使用 USPTO MIT 数据集作为 ProPreT5 的健全性检查（sanity check）。最终，我们证明 ProPreT5 能够生成现实、有效和准确的反应产物，同时允许在基于模板的设置中探索整个化学空间，解决了关键挑战，并超越了当前反应预测中最先进的方法。还提出了一项旨在提高基于模板的 ProPreT5 模型可解释性的分析，得出的结论是，通用反应模板为模型提供了产物生成的关键信息。

本文提出的贡献有三个方面：

• 我们通过引入一个新的反应集 BRS 来解决公开可用反应数据集的局限性，该反应集允许更广泛地探索化学空间。
• 我们发布了 ProPreT5，一个改进的、高度灵活的基于 T5 的模型，能够生成现实、有效和准确的产物。
• 我们对模型进行了可解释性分析，以识别影响产物预测准确性的关键上下文信息。

（

#### 这项研究的解决方案

**BRS (Broad Reaction Set)：通用反应集**

- 创建了20个"万能模板"，每个都能处理多种相似的反应
- 就像学会了20种基础烹饪技法，可以应对大部分菜谱

**ProPreT5：智能化学助手**

- 基于T5模型开发的专门化学AI
- 既能使用模板，又能灵活变通
- 就像一个既懂基础技法，又能创新的智能厨师

#### 主要贡献

1. **更好的数据集**：BRS让我们能探索更广阔的化学空间
2. **更强的模型**：ProPreT5能生成更准确、现实的化学反应产物
3. **更好的理解**：通过分析模型，我们能理解哪些信息对预测最重要

）

**表 I：通用反应模式**
大于号 (`>>`) 将反应物与产物分开，而点 (`.`) 区分单个分子。`#n` 代表原子序数为 `n` 的任何原子，特定字母表示特定的化学环境；`:n` 用于映射和跟踪反应中的特定子图。有关 SMARTS 符号的更多信息，请参阅 [18]。

| #    | SMARTS 符号 (构造性反应)                                     | #    | SMARTS 符号 (破坏性反应)                                     |
| ---- | ------------------------------------------------------------ | ---- | ------------------------------------------------------------ |
| 1    | `[#6,#7,#8;h:1].[O,N,F,C:2]>>[#6,#7,#8:1][O,N,F,C:2]`        | 11   | `[#6,#7,#8:1][O,N,F,C:2]>>[#6,#7,#8;h:1]`                    |
| 2    | `[O,N,C;h:1][O,N,C;h:2]>>[O,N,C:1]=[O,N,C:2]`                | 12   | `[O,N,C:1]=[O,N,C:2]>>[O,N,C;h:1][O,N,C;h:2]`                |
| 3    | `[N,C;h2:1][N,C;h2:2]>>[N,C:1]#[N,C:2]`                      | 13   | `[N,C:1]#[N,C:2]>>[N,C;h2:1][N,C;h2:2]`                      |
| 4    | `[C;h:1]=[N,C;h:2]>>[C:1]#[N,C:2]`                           | 14   | `[C:1]#[N,C:2]>>[C;h:1]=[N,C;h:2]`                           |
| 5    | `[#6,#7,#8;h:1]~[*:2]~[#6,#7,#8;h:3]>> [#6,#7,#8:1]1[*:2]~[#6,#7,#8:3]1` | 15   | `[#6,#7,#8:1]1[*:2]~[#6,#7,#8:3]1>> [#6,#7,#8;h:1]~[*:2]~[#6,#7,#8;h:3]` |
| 6    | `[#6,#7,#8;h:1]~[*:2]~[*:4]~[#6,#7,#8;h:3]>> [#6,#7,#8:1]1[*:2]~[*:4]~[#6,#7,#8:3]1` | 16   | `[#6,#7,#8:1]1[*:2]~[*:4]~[#6,#7,#8:3]1>> [#6,#7,#8;h:1]~[*:2]~[*:4]~[#6,#7,#8;h:3]` |
| 7    | `[#6,#7,#8;h:1]~[*:2]~[*:4]~[*:5]~[#6,#7,#8;h:3]>> [O,N,C:1]1[*:2]~[*:4]~[*:5]~[#6,#7,#8:3]1` | 17   | `[O,N,C:1]1[*:2]~[*:4]~[*:5]~[#6,#7,#8:3]1>> [#6,#7,#8;h:1]~[*:2]~[*:4]~[*:5]~[#6,#7,#8;h:3]` |
| 8    | `[#6,#7,#8;h:1]~[*:2]~[*:4]~[*:5]~[*:6]~[#6,#7,#8;h:3]>>[O,N,C:1]1[*:2]~[*:4]~[*:5]~[*:6]~[#6,#7,#8:3]1` | 18   | `[O,N,C:1]1[*:2]~[*:4]~[*:5]~[*:6]~[#6,#7,#8:3]1 >>[#6,#7,#8;h:1]~[*:2]~[*:4]~[*:5]~[*:6]~[#6,#7,#8;h:3]` |
| 9    | `[#6,#7,#8;h:1]~[*:2]~[*:4]~[*:5]~[*:6]~[*:7]~[#6,#7,#8;h:3]>>[O,N,C:1]1[*:2]~[*:4]~[*:5]~[*:6]~[*:7]~[#6,#7,#8:3]1` | 19   | `[O,N,C:1]1[*:2]~[*:4]~[*:5]~[*:6]~[*:7]~[#6,#7,#8:3]1>>[#6,#7,#8;h:1]~[*:2]~[*:4]~[*:5]~[*:6]~[*:7]~[#6,#7,#8;h:3]` |
| 10   | `[#6,#7,#8;h:1]~[*:2]~[*:4]~[*:5]~[*:6]~[*:7]~[*:8]~[#6,#7,#8;h:3]>>[O,N,C:1]1[*:2]~[*:4]~[*:5]~[*:6]~[*:7]~[*:8]~[#6,#7,#8:3]1` | 20   | `[O,N,C:1]1[*:2]~[*:4]~[*:5]~[*:6]~[*:7]~[*:8]~[#6,#7,#8:3]1>>[#6,#7,#8;h:1]~[*:2]~[*:4]~[*:5]~[*:6]~[*:7]~[*:8]~[#6,#7,#8;h:3]` |

（

我来用通俗的语言解释这个通用反应模式表：

#### 这个表格是什么？

这就像是化学反应的"万能公式集"，包含20个基础反应模板，每个模板都能处理一大类相似的化学反应。

#### 符号解释（化学的"编程语言"）

**基础符号：**

- `>>` = "变成"（箭头，表示反应方向）
- `.` = "和"（分隔不同的分子）
- `#6` = 碳原子，`#7` = 氮原子，`#8` = 氧原子
- `:1`, `:2`, `:3` = 给原子"编号"，用来追踪它们在反应中的变化

#### 反应类型分类

**构造性反应（1-10号）：组装分子** 就像用积木搭建更复杂的结构

**破坏性反应（11-20号）：拆解分子** 就像把复杂结构拆解成简单部件

#### 具体例子解释

让我用几个简单的例子说明：

**反应1和11（键的形成与断裂）**

- 反应1：两个分子连接在一起（就像拼接两块积木）
- 反应11：把连接的分子分开（就像把拼接的积木掰开）

**反应2和12（双键的形成与断裂）**

- 反应2：两个分子形成更强的双键连接（像用两根绳子绑定）
- 反应12：双键断裂变成单键（像剪断一根绳子）

**反应5-10和15-20（环的形成与打开）**

- 反应5-10：把直链分子"弯曲"成环形（像把直的铁丝弯成圆圈）
- 反应15-20：把环形分子"掰开"成直链（像把圆圈拉直）

#### 实际意义

这个表格就像是给AI提供了一个"化学反应工具箱"：

- **输入**：反应物分子
- **工具**：20个通用反应模板
- **输出**：可能的产物分子

这样，AI就能像一个熟练的化学家一样，知道用哪种"工具"来处理不同类型的分子，预测可能的反应结果。

简单来说，这就是把复杂的化学反应归纳成20个基本"套路"，让计算机也能"理解"化学反应的基本规律。

）

## II 相关工作

最近关于反应预测中基于模板的方法的文献大多集中在基于图的模型上 [23, 24]。然而，据我们所知，Transformer 模型在基于模板的序列预测方面的潜力尚未得到充分探索。这部分归因于基准数据集（USPTO MIT）中反应模板的特异性。

无模板的、基于序列的模型在 USPTO MIT 基准测试中取得了令人印象深刻的结果，保持了单步产物预测任务的最先进水平 [8]。然而，这些模型在适用性方面面临着重大挑战。USPTO MIT 数据集仅限于专利中发现的反应，难以捕捉现实世界有机合成的多样性和复杂性。因此，虽然无模板模型在生成反应结果方面表现良好，但它们难以泛化到新分子的发现或数据集中未包含的反应结果的预测 [21]。

该领域的显著进展包括 Molecular Transformer [6]，它使用了基础 Transformer 架构 [4] 的一个较小版本。该模型在正向和反向任务上进行了训练。这是 Transformer 在单步反应预测任务中的首次应用。Augmented Molformer [7] 通过使用非规范 SMILES 扩展输入来进行数据增强，并证明这样做提高了模型的泛化能力。另一方面，Chemformer [8] 通过提出一个具有特殊预训练的更大的 BART [25] 模型，将事情推得更远。预训练包括掩码（masking）和数据增强，使模型更加鲁棒并提高了预测准确性。虽然这些无模板 Transformer 模型在反应预测方面取得了显著成功，但探索基于模板的序列到序列模型用于此任务仍然是一个未被充分探索的领域，为该领域的进一步发展留下了空间。

先前模型缺乏实际应用，突显了需要一个具有通用反应的数据集，为反应预测模型提供一个更通用的训练场。这将允许模型探索整个化学空间，最终推动该领域超越当前基准测试的局限性。

## III 数据集
本工作使用了两个数据集：USPTO MIT [20] 和使用提议的 BRS 构建的数据集，后者是本工作的关键贡献之一。USPTO MIT 数据集包含从专利中提取的反应，以及相应的反应物、试剂和产物，以 SMILES 符号表示。USPTO MIT 数据集包含高度特定的反应模板，仅对确切的反应物、试剂和产物组合有效。它分为两个子集：USPTO MIT Mixed（不区分反应物和试剂，被认为是一个稍微更具挑战性的问题）和 USPTO MIT Separated（将试剂与反应物分开）。

在本研究中，我们使用 USPTO MIT Mixed 数据集，并使用 RDKit [26] 执行了验证过程，以确保反应物、试剂、产物及其相应的反应模板之间的一致性。在我们的特定配置下，我们发现了少量不一致的情况，其中反应物、试剂和产物的组合与反应模板不符。这些不匹配表明反应无法按定义进行，导致从数据集中移除了 142 个示例。这一调整对整体数据集大小的影响可以忽略不计：训练集从 409,035 减少到 408,916 个示例，验证集从 30,000 减少到 29,990，测试集从 40,000 减少到 39,987。提议的训练/验证/测试划分得以保留。

### III-A 提议的反应集：Broad Reaction Set (BRS)
对于提议的数据集，引入了 20 个新的通用反应，如表 I 所示。其中十个反应是构造性的（constructive），另外十个是破坏性的（destructive），以 SMARTS 符号表示。这些反应受到名为 EvoMol 的进化算法中使用的反应的启发，该算法在探索化学空间方面表现出效率 [27]。这些反应是构建类似于 USPTO 的预测数据集的基础构建块。由于这些反应可以应用于广泛的分子，它们为使用公开可用或商业数据集提供了显著的灵活性。

破坏性反应是构造性反应的逆反应，能够返回到化学空间的早期阶段，提供了探索替代途径的可能性。构造性和破坏性反应是对称的，并且由于本工作侧重于单步预测，因此本研究仅使用构造性反应。

以下是表 I 中每个反应的作用：

• 反应 1 & 11：构造性反应 (#1) 涉及一个具有与氢键合的原子（如 C、N 或 O）的分子，与一个含有 O、N、F 或 C 的官能团反应。该官能团被添加到反应物上。相反，破坏性反应 (#11) 移除一个含有 O、N、F 或 C 的官能团，并用一个氢原子替换它。
• 反应 2 & 12：构造性反应 (#2) 取一个在 O、N 或 C 原子之间具有单键（这些原子也与 H 键合）的分子，并形成一个双键来代替。破坏性反应 (#12) 断裂这些相同重原子之间的双键，用单键替换它。
• 反应 3 & 13：构造性反应 (#3) 取一个在 N 和 C 原子之间具有单键（每个原子与两个氢原子键合）的分子，并用这些原子之间的三键替换这个键，为每个重原子断裂一个氢键。破坏性反应 (#13) 断裂三键并向形成该键的每个重原子添加一个氢原子。
• 反应 4 & 14：构造性反应 (#4) 取一个具有与 H 键合的 C 原子的分子。该 C 原子与一个 N 或 C 原子含有双键，而该 N 或 C 原子又至少与一个 H 键合。该反应将双键转化为三键。破坏性反应 (#14) 取一个在 C 原子和 N 或 C 原子之间具有三键的分子，并将该三键断裂成双键。

剩余的反应侧重于各种大小环的创建或破坏，将一起解释：

• 反应 5 – 10 & 15 – 20：构造性反应 (#5 – #10) 涉及一个具有线性结构的分子，其中一个与 H 键合的 C、N 或 O 原子连接到一个（反应 5）到六个（反应 10）通配符原子（任何原子），通过将位置 `:1` 的原子与位置 `:3` 的原子键合来闭合环以形成环状结构。破坏性反应 (#15 – #20) 涉及一个具有大小为 3（反应 15）到 8（反应 20）的环的分子，它们会断裂该环。

凭借其高度通用的模式，这些反应可以应用于广泛的分子，以及同一分子内的不同子结构。这使得该反应集极其灵活，非常适合探索化学空间。我们认为这 20 个反应为从最简单的分子开始并探索化学空间的重要部分提供了坚实的基础。

为简单起见，这些反应中定义的转化目前仅限于原子 C、N、O 和 F。然而，这并不意味着反应不能应用于含有其他原子的分子。它只是意味着仅考虑这四种原子的反应性，并且转化将专门在分子内的它们之间发生。此外，定义了构建大小最多为 8 的环的反应。这种限制部分是由于使用 SMARTS 符号定义可以生成任意大小环的反应存在挑战，但也因为大于 8 的环很罕见。此外，化学家们提出了试图评估分子图可合成性的指标，例如 SAScore [28]，其中大环会受到惩罚。

尽管有这些限制，定义的反应使我们能够探索化学空间的重要部分。通过微小的修改，这些反应可以扩展到包括额外的原子，并覆盖比当前定义所设定的限制更大的化学空间部分。就目前而言，我们设定的限制仍然使我们能够探索化学空间。

### III-B 提议数据集的构建
为了构建本研究中使用的数据集，从几个公开可用的数据集中随机采样反应物：EVO10 [29]，它枚举了每个可能的分子，最多包含 10 个 C、N、O、F 或 S 原子；ZINC20 [30]；和 ChEMBL34 [31]，两者都被过滤到与 EVO10 相同的原子。为简单起见，提议的数据集将被称为 BRS 数据集，尽管 BRS 是用于构建数据集的反应集的名称。

为了创建数据集，从上述三个数据集中随机选择分子，同时确保 ZINC20（一个包含超过 10 亿个分子的数据集）不会被过度代表。在每次迭代中，要么从数据集中选择一个新分子，要么使用早期迭代的产物作为反应物，这允许为未来的研究构建多步合成路线。为了避免通用 BRS 生成不切实际的产物，应用了几个过滤步骤。首先，只保留规范的 SMILES。此外，实施了 [29] 中描述的过滤器：一个移除具有 ECFP4 子图的分子（这些子图在真实分子数据集如 ChEMBL 和 ZINC 中不存在），而另一个排除包含通用环特征（GCF）的产物——这些是在相同数据集中未观察到的分子环的支架。通过这些过滤器的产物被认为是现实的。为了增强多样性，对于相同的反应物-反应组合包含了多个产物，允许模型学习单个输入可以产生多个有效输出。我们还确保每个反应在训练、测试和验证数据集中出现的次数相等。最终的训练数据集包含 311,621 个示例，而验证和测试数据集都包含 10,000 个示例。通过使用 BRS 中的反应并结合来自不同数据集的分子，可以构建更大的数据集。

> **图 1 标题：** ProPreT5 架构。

（

#### 使用的两个数据集

这项研究使用了两个"教学材料"来训练AI：

#### 1. USPTO MIT 数据集（传统教材）

**来源：** 美国专利中的化学反应 **特点：**

- 就像一本非常具体的食谱书
- 每个食谱都详细到具体用什么食材、多少克、什么温度
- 很准确，但只能做书里有的菜

**问题：** 研究者发现这本"食谱书"里有142个错误的食谱（反应无法进行），所以把它们删掉了

#### 2. BRS 数据集（新式教材）

**这是研究的核心创新！**

#### BRS数据集的构建思路

#### 反应模板的设计哲学

**构造性反应（1-10号）：搭积木**

- 反应1：把两个分子粘在一起（像拼接积木）
- 反应2：把单键变成双键（像用更强的胶水粘接）
- 反应3：把单键变成三键（像用最强的胶水）
- 反应4：双键变三键的特殊情况
- 反应5-10：把直链分子弯成不同大小的环（3元环到8元环）

**破坏性反应（11-20号）：拆积木**

- 就是上面每个反应的逆过程
- 把组装好的结构重新拆解

）

## IV 方法
### IV-A 模型和实现细节
ProPreT5 的总体架构如图 1 所示。选择 T5 模型是因为其易用性、相对轻量级的架构以及即使使用适度的训练资源也能处理大型数据集的能力。该模型与原始 T5 [22] 非常相似，包含编码器-解码器结构。编码器和解码器块各由 6 层组成，隐藏大小为 512。每个块包括自注意力层和前馈层，具有 8 个注意力头。前馈层具有 2048 的中间大小并使用 ReLU 激活。除了这些层之外，解码器还包括交叉注意力层和掩码自注意力层。还使用相对位置嵌入来捕获 token 关系。

编码器和解码器共享一个词汇嵌入层，词汇大小为 243 个 token，因为输入和输出使用相同的符号——用于反应物和产物的 SMILES，以及用于反应的 SMILES 或 SMARTS。相对较小的词汇大小源于字符级标记化（character-level tokenization），这增强了 ProPreT5 的灵活性。这种方法允许使用大多数数据集而无需重新训练模型，除非添加了新字符——考虑到词汇表覆盖了已知化学的很大一部分，这不太可能发生。选择字符级标记化是为了解决确保预训练模型具有正确 token 的挑战，因为添加和微调新 token 的成本很高。此外，还训练和测试了一个更大的字节对编码（BPE）标记器 [32]，包含大约 10,000 个 token，但这导致了泛化问题。这个紧凑而高效的模型能够生成反应结果。

如图 1 所示，反应物和反应通过一个分隔 token 来区分，该 token 用于分隔反应物彼此之间以及反应与反应物。此外，按照 [11] 中的方法，对每个输入应用了类型嵌入（type embeddings）。这些嵌入区分不同的输入和输出类型，使得合并新的输入类型（例如试剂）变得更容易，而不会混淆模型。使用一个单独的可训练嵌入层将这些类型 token 映射到嵌入向量。ProPreT5 也可以在无模板设置下进行预测，这意味着反应模板将被排除在编码器输入序列之外。

**表 II：反应产物预测准确性比较**
未提供基于模板版本的比较，因为没有模型包含反应模板所需的 token。此外，Molecule Transformer 的代码不可用，我们无法运行 Augmented Transformer。(*) 结果是在没有微调的情况下获得的。带有引用的结果直接取自相应的论文。

| 数据集          | 反应模板 | Molecule Transformer | Augmented Transformer | Chemformer | ProPreT5 |
| --------------- | -------- | -------------------- | --------------------- | ---------- | -------- |
| USPTO MIT Mixed | 基于模板 | -                    | -                     | -          | 99.8%    |
| USPTO MIT Mixed | 无模板   | 88.6% [6]            | 90.0% [7]             | 90.9% [8]  | 87.2%    |
| BRS             | 基于模板 | -                    | -                     | -          | 85.8%    |
| BRS             | 无模板   | -                    | -                     | 5.6%*      | 8.8%*    |

### IV-B 计算资源和训练设置
ProPreT5 的训练和评估是在一个高性能计算集群上完成的，该集群配备了具有 32 GB 内存的 NVIDIA V100 GPU，提供了处理大型数据集的足够容量。ProPreT5 支持分布式训练，并在 16 个 GPU 上并行训练了 20 小时。与文献中的一些模型不同，这种设置是轻量级且省时的，然而即使是这样相对较短的 20 小时训练也提供了令人满意的结果。

（

#### ProPreT5 模型架构：

##### 模型规模对比

```
组件ProPreT5标准T5-small
编码器层数66
解码器层数66
隐藏维度512512
注意力头数88
前馈维度20482048
词汇表大小24332,128
```

##### 关键设计特点

**轻量化设计：**

- 相比大型模型（如GPT-3），参数量较小
- 训练和推理效率高

**化学专用优化：**

- 小词汇表专门针对化学符号
- 类型嵌入区分化学实体
- 字符级标记化适应化学表示

**灵活性：**

- 支持有模板和无模板两种模式
- 可以轻松扩展到新的化学符号

这个架构就像一个专门为化学反应设计的"翻译机器"——把反应物和反应条件"翻译"成产物，既保持了Transformer的强大能力，又针对化学领域做了专门优化。

#### 整体设计理念

**为什么选择T5？**

- 就像选择一辆经济实用的家用车：
  - 容易操作（易用性）
  - 不太重（轻量级）
  - 油耗低但能拉货（用适度资源处理大数据集）

#### 模型结构（编码器-解码器）

**编码器：理解输入的"翻译官"**

- 6层处理单元，就像6个专业翻译员
- 每层有8个"注意力头"（像8只眼睛同时观察不同角度）
- 理解反应物和反应条件的含义

**解码器：生成输出的"作家"**

- 也是6层，但功能不同
- 不仅要理解输入，还要生成新的化学产物
- 就像一个既懂外语又会写作的作家

#### 词汇表设计

**字符级标记化（Character-level tokenization）**

- 像学汉字一样，一个字符一个字符地理解
- 词汇表只有243个字符（相比之下，中文常用字约3000个）
- 优点：
  - 灵活性高：遇到新分子也能处理
  - 通用性强：不需要为每个新数据集重新训练
  - 成本低：不需要经常更新词汇表

**为什么不用更大的词汇表？**

- 研究者试过约10,000个词汇的方法，但发现：
  - 就像背太多生僻词，反而影响理解能力
  - 泛化能力变差

#### 输入格式设计

**信息分离：**

- 用特殊符号把不同信息分开
- 就像写地址时用逗号分隔：北京市，海淀区，清华大学
- 反应物 + 分隔符 + 反应条件 → 产物

**类型嵌入（Type embeddings）：**

- 给不同类型的输入贴"标签"
- 就像给文件分类：红色标签=反应物，蓝色标签=反应条件
- 这样模型就不会搞混不同类型的信息

）

## V 结果与讨论
ProPreT5 在无模板和基于模板的设置下都进行了训练。无模板训练主要用作健全性检查，使我们能够将 ProPreT5 的性能与文献中的现有模型进行比较。这一步对于确认模型按预期运行并可以可靠地与 BRS 一起使用至关重要。目标不是改进 USPTO MIT 基线，而是确保模型的性能与先前的工作合理一致，尽管使用了经济高效的训练方法和最少的数据增强。具体来说，我们使用 [33] 中提出的非规范 SMILES 将数据集增强了四倍，其中反应物的多种非规范形式与同一产物配对。此外，在这种情况下没有使用现有文献中常用的成本高昂的预训练方法。

表 II 展示了在不同数据集上训练的不同模型的精确匹配准确率（exact match accuracy）。在 USPTO MIT Mixed 数据集中，每个反应物-反应组合恰好对应一个产物，因此如果模型没有生成那个特定的产物，则预测被视为不正确。相反，在 BRS 数据集中，单个反应物-反应组合可能导致多个可能的产物，类似于机器翻译任务中可以存在多个正确翻译。因此，在 BRS 的情况下，如果模型预测了任何可能的产物之一，则生成被认为是正确的。

（

#### 性能比较（表II解读）

##### USPTO MIT Mixed 数据集

**基于模板：**

- ProPreT5达到99.8%准确率
- 几乎完美！但这其实说明这个任务"太简单了"

**无模板：**

- 各种模型都在87-91%左右
- ProPreT5的87.2%属于正常水平

#### BRS 数据集（关键创新）

**基于模板：**

- ProPreT5达到85.8%
- 这是一个更有挑战性和现实意义的结果

**无模板：**

- 其他模型表现很差（5.6%）
- ProPreT5虽然只有8.8%，但至少还能工作

#### 结果分析

**USPTO数据集的问题：**

- 基于模板时准确率99.8%说明"太简单"
- 就像考试题目泄露了答案，当然能考满分

**BRS数据集的价值：**

- 85.8%的准确率更有意义
- 说明模型真的在"思考"和"学习"化学反应规律

）

### V-A 无模板反应产物预测
从表 II 可以看出，在经过仅 20 个 epoch 的相对经济高效的设置训练后，ProPreT5 在无模板 USPTO MIT Mixed 基线上达到了 87.2% 的精确匹配准确率。考虑到其他三个模型需要数百个 epoch，这一结果尤其令人印象深刻。这个准确率足以确认模型按预期运行，并准备好处理手头新的、雄心勃勃的任务。

预测 BRS 数据集的反应产物本质上比预测 USPTO 数据集更复杂。不使用反应模板，准确预测产物变得更具挑战性。由于单个分子可以作为许多不同反应中的反应物，模型在预测正确转化方面面临显著限制。因此，模型只能学习一般的转化模式并随机应用它们，这使得生成的产物与预期结果匹配的可能性非常小。在 BRS 数据集上不使用反应模板获得的结果突显了在缺乏模板的情况下此任务的复杂性。

为了提供一个比较点，使用 [8] 的公开可用代码在 Chemformer 上运行了一个测试 epoch，没有任何微调。类似地，ProPreT5 也在没有微调的情况下进行了测试。我们本想与其他两个模型提供相同的比较，但 Molecule Transformer 的代码不是公开可用的，并且我们无法成功运行 Augmented Transformer 的代码。

两个模型的正确预测百分比较低，突显了此任务的难度，并表明在现实世界场景中，当一个反应物可能参与多个不同反应时，在无模板设置下训练的模型难以泛化并产生准确的预测。

> **(a) 重要性矩阵的平均聚合**
> **(b) 重要性矩阵的最大聚合**
> **图 2 标题：** 基于模板的 ProPreT5 在 USPTO 和 BRS 上生成的可解释性分析

> **(a) 反应物 Token**
> **(b) 反应物模板 Token**
> **(c) 产物模板 Token**
> **图 3 标题：** 来自 USPTO 的单个示例中每个 Token 在生成中的重要性

### V-B 基于模板的反应产物预测
在基于模板的预测方面，ProPreT5 在 USPTO MIT Mixed 基线上达到了接近完美的 99.8% 准确率，这表明任务变得微不足道，因为模板是高度特定的，并且基本上列举了产物中的原子。相反，对于 BRS 数据集，模板提供了生成正确产物所必需的关键信息。没有这些信息，做出准确的预测就成了一项 inherently illogical（ inherently illogical 意指“本质上不合逻辑的”或“从根本上说不通的”）的任务。鉴于相同的反应物可以参与 BRS 中的多个反应并产生不同的产物，模型必须有效地解释通用反应模板中编码的信息以做出准确的预测。尽管有这种复杂性，ProPreT5 仍达到了 85.8% 的令人印象深刻的准确率，突显了其即使在更困难的预测场景中的有效性。

为了确认关于 USPTO 和 BRS 上基于模板预测的直觉，我们专注于理解模型如何利用输入序列中的信息。对于分析，我们使用了 Inseq [34]，这是一个将重要性分数归因于输入 token 的工具，有助于理解模型如何生成其输出。使用积分梯度（Integrated gradients）作为归因方法。进行此分析是为了增强 ProPreT5 的可解释性。

在图 2 中，分析了基于模板的生成。输入序列分为三个子类别：反应物、反应的反应物模板和反应的产物模板（通常在 SMARTS 符号中用 `>>` 分隔）。因此，使用 Inseq 为每个示例获得了三个重要性分数矩阵，与三个输入部分中的每一个对齐。这些矩阵要么进行最大聚合（max-aggregated），要么进行平均聚合（mean-aggregated），以提供整个测试数据集的概览。图 2 中的小提琴图（violin plots）显示了 BRS 和 USPTO 数据集上基于模板预测的平均和最大重要性分数的分布。

通过查看图 2(a) 和图 2(b) 可以推断出，输入序列中的大部分信息并未用于 USPTO 的生成。只有少数 token 携带了基本信息。这些结果支持了先前的假设，即在生成产物时，模型主要关注输入的反应物部分和产物模板部分中的少数几个 token。模型主要关注特定的 token，而输入的其余部分对生成的贡献很小。

图 2 显示了整个数据集，而图 3 突出显示了一个示例，其中较深的颜色表示对输出的贡献较高，较浅的颜色表示贡献较小。这个来自 USPTO 的示例表明，反应物模式的贡献是最小的，因为目标产物几乎明确地编码在反应 SMILES 模板中。模型不需要找到模板反应物和实际反应物之间的对齐来产生结果。这解释了 99.8% 的近乎完美的准确率以及为什么该任务是微不足道的。

相比之下，BRS 数据集在产物和反应模板之间呈现了更隐式的关系。图 2(a) 中反应模板两个部分的平均聚合重要性图几乎相同，突显了模型需要同等地依赖这两个部分来进行正确预测。

### V-C 基于模板的 ProPreT5 的评估指标
基于模板的 ProPreT5 模型被证明是过于具体的模板和无模板方法之间的一个很好的折衷。其性能通过各种指标进行了进一步评估，如表 III 所示。

准确率（Accuracy），如前所定义，指的是精确匹配准确率。对于 BRS 数据集，其中单个反应物-反应组合可能产生多个产物，如果模型生成任何可能的产物之一，则预测被认为是正确的。然后，准确率计算为正确预测数与总预测数之比。

有效性（Validity）评估预测的分子是否可以使用 RDKit [26] 成功转化为分子图。如果一个分子违反了有机化学的基本构造规则，它就无法被 RDKit 解析，因此被认为是无效的。有效性确定为可解析分子数与总预测数之比。

另一方面，RDKit 无法检查产物的真实性（realism），仅仅因为一个分子是有效的并不一定意味着它是真实的。真实性评估预测的分子是否类似于已知的真实世界分子。这是通过使用子图过滤器 [29] 来实现的，这些过滤器消除了包含在数十亿真实分子数据集中从未遇到过的基团或环的分子。真实性计算为通过这些过滤器的预测数占总预测数的比例。

来自 BRS 的通用反应能够生成有效但不真实的产物。正如前面在 III-B 节中解释的，我们过滤掉了那些不真实的产物以构建数据集。表 III 表明，在该数据集上训练的 ProPreT5 具有生成有效、准确和真实产物的能力，突显了其在探索和扩展化学空间同时尊重有机化学规则方面的实用性。

**表 III：基于模板的 ProPreT5 的指标**
| 准确率 | 有效性 | 真实性 |
| ------ | ------ | ------ |
| 85.8%  | 99.3%  | 91.0%  |

## VI 结论
在本研究中，我们引入了 Broad Reaction Set (BRS)，一个新颖的通用反应集，旨在提供一个更现实和通用的基准。这一重要贡献解决了广泛使用的 USPTO 数据集的关键局限性。到目前为止，基于模板的反应预测模型依赖于过于具体的反应模板，限制了我们在化学信息学中寻求的实际应用性。

BRS 数据集的相关性得到了提出的 ProPreT5 的支持，这是一个灵活的反应产物预测模型，能够生成有效、真实和准确的产物。其灵活性不仅来自在具有通用反应的数据集上进行训练，还来自其字符级标记化，这使得无需昂贵的重新训练过程即可轻松适应其他数据集。ProPreT5 是一个主要贡献，因为它与现有模型不同，其预测产物的方式可以应用于实际的化学问题。我们还证明了在更轻量级的训练设置中获得令人满意的结果是可能的。

未来的工作将超越单步产物预测，探索多步有机合成路线，同时继续使用基于模板的方法与 ProPreT5。此外，我们将专注于提高预测准确性，以避免在多步设置中传播错误。

## 致谢
致谢部分此时将留空，以保持提交的匿名性。

## 代码和数据可用性
本工作的代码和数据将在未来提供，因为此时无法共享以保持提交的匿名性。

## 参考文献
[1] Blakemore, David C., et al. ”Organic synthesis provides opportunities to transform drug discovery.” Nature chemistry 10.4 (2018): 383-394.
[2] Stein, Helge S., and John M. Gregoire. ”Progress and prospects for accelerating materials science with automated and autonomous workflows.” Chemical science 10.42 (2019): 9640-9649.
[3] Day, Daniel M., et al. ”Reaction Optimization for Greener Chemistry with a Comprehensive Spreadsheet Tool.” Molecules 27.23 (2022): 8427.
[4] Vaswani, A. ”Attention is all you need.” Advances in Neural Information Processing Systems (2017).
[5] Cambria, Erik, and Bebo White. ”Jumping NLP curves: A review of natural language processing research.” IEEE Computational intelligence magazine 9.2 (2014): 48-57.
[6] Schwaller, Philippe, et al. ”Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction.” ACS central science 5.9 (2019): 1572-1583.
[7] Tetko, Igor V., et al. ”State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis.” Nature communications 11.1 (2020): 5575.
[8] Irwin, Ross, et al. ”Chemformer: a pre-trained transformer for computational chemistry.” Machine Learning: Science and Technology 3.1 (2022): 015022.
[9] Schwaller, Philippe, et al. ”Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy.” Chemical science 11.12 (2020): 3316-3325.
[10] Wang, Xiaorui, et al. ”RetroPrime: A Diverse, plausible and Transformer-based method for Single-Step retrosynthesis predictions.” Chemical Engineering Journal 420 (2021): 129845.
[11] Bagal, Viraj, et al. ”MolGPT: molecular generation using a transformer-decoder model.” Journal of Chemical Information and Modeling 62.9 (2021): 2064-2076.
[12] Mazuz, Eyal, et al. ”Molecule generation using transformers and policy gradient reinforcement learning.” Scientific Reports 13.1 (2023): 8799.
[13] Dobberstein, Niklas, Astrid Maass, and Jan Hamaekers. ”Llamol: a dynamic multi-conditional generative transformer for de novo molecular design.” Journal of Cheminformatics 16.1 (2024): 73.
[14] Wang, Sheng, et al. ”Smiles-bert: large scale unsupervised pre-training for molecular property prediction.” Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics. 2019.
[15] Chithrananda, Seyone, Gabriel Grand, and Bharath Ramsundar. ”ChemBERTa: large-scale self-supervised pretraining for molecular property prediction.” arXiv preprint arXiv:2010.09885 (2020).
[16] Ross, Jerret, et al. ”Large-scale chemical language representations capture molecular structure and properties.” Nature Machine Intelligence 4.12 (2022): 1256-1264.
[17] Weininger, David. ”SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules.” Journal of chemical information and computer sciences 28.1 (1988): 31-36.
[18] Daylight Theory: SMARTS - A Language for Describing Molecular Patterns. https://www.daylight.com/dayhtml/doc/theory/theory.smarts.html. 访问于 2025年1月10日.
[19] Lowe, Daniel Mark. Extraction of chemical structures and reactions from the literature. Diss. 2012.
[20] Jin, Wengong, et al. ”Predicting organic reaction outcomes with weisfeiler-lehman network.” Advances in neural information processing systems 30 (2017).
[21] Wei, Yixin, et al. ”Machine learning-assisted retrosynthesis planning: current status and future prospects.” Chinese Journal of Chemical Engineering (2024).
[22] Raffel, Colin, et al. ”Exploring the limits of transfer learning with a unified text-to-text transformer.” Journal of machine learning research 21.140 (2020): 1-67.
[23] Chen, Shuan, and Yousung Jung. ”Deep retrosynthetic reaction prediction using local reactivity and global attention.” JACS Au 1.10 (2021): 1612-1620.
[24] Yan, Chaochao, et al. ”RetroComposer: composing templates for template-based retrosynthesis prediction.” Biomolecules 12.9 (2022): 1325.
[25] Lewis, Mike. ”Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.” arXiv preprint arXiv:1910.13461 (2019).
[26] RDKit. https://rdkit.org/. 访问于 2025年1月23日.
[27] Leguy, Jules, et al. ”EvoMol: a flexible and interpretable evolutionary algorithm for unbiased de novo molecular generation.” Journal of cheminformatics 12 (2020): 1-19.
[28] Ertl, Peter, and Ansgar Schuffenhauer. ”Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions.” Journal of cheminformatics 1 (2009): 1-11.
[29] Cauchy, Thomas, Jules Leguy, and Benoit Da Mota. ”Definition and exploration of realistic chemical spaces using the connectivity and cyclic features of ChEMBL and ZINC.” Digital Discovery 2.3 (2023): 736-747.
[30] Irwin, John J., et al. ”ZINC20—a free ultralarge-scale chemical database for ligand discovery.” Journal of chemical information and modeling 60.12 (2020): 6065-6073.
[31] ChEMBL Database, https://www.ebi.ac.uk/chembl/
[32] Sennrich, Rico. ”Neural machine translation of rare words with subword units.” arXiv preprint arXiv:1508.07909 (2015).
[33] Bjerrum, Esben Jannik. ”SMILES enumeration as data augmentation for neural network modeling of molecules.” arXiv preprint arXiv:1703.07076 (2017).
[34] Sarti, Gabriele, et al. ”Inseq: An interpretability toolkit for sequence generation models.” arXiv preprint arXiv:2302.13942 (2023).