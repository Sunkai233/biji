# 基于强化学习的四旋翼容错控制：具有在线变换器适应性

**Dohyun Kim¹**, **Jayden Dongwoo Lee²**, **Hyochoong Bang²** 和 **Jungho Bae¹**

**摘要** — 多旋翼无人机在各种野外机器人应用中发挥重要作用，但仍高度容易受到执行器故障的影响，导致快速失稳和任务可靠性受损。虽然使用强化学习(RL)的各种容错控制(FTC)策略已被广泛探索，但大多数先前的方法需要多旋翼模型的先验知识或难以适应新配置。为了解决这些限制，我们提出了一种新颖的混合RL基础FTC框架，集成了基于变换器的在线适应模块。我们的框架利用变换器架构实时推断潜在表示，实现对先前未见系统模型的适应而无需重新训练。我们在PyBullet仿真中评估了我们的方法，在效力损失执行器故障下，实现了95%的成功率和0.129m的位置均方根误差(RMSE)，优于现有适应方法的86%成功率和0.153m RMSE。对具有不同配置的四旋翼的进一步评估证实了我们框架在未训练动力学中的鲁棒性。这些结果证明了我们框架增强多旋翼适应性和可靠性的潜力，在动态和不确定环境中实现高效故障管理。网站地址：http://00dhkim.me/paper/rl-ftc

{

#### 背景

- 多旋翼无人机（比如四旋翼）应用很广，但一个大问题是：
   **电机/螺旋桨一旦坏掉，飞机很快就会失控**。
- 以前很多研究尝试用 **强化学习 (RL)** 来做容错控制（FTC），
   但这些方法往往需要提前知道无人机的详细数学模型，或者在遇到新机型/新配置时就不管用了。

------

#### 他们的新方法

研究团队提出了一种 **混合式 RL 容错框架**，核心创新是：

- 引入一个 **基于 Transformer 的在线适应模块**。
- 这个模块可以：
  - 实时推断出无人机的“隐藏动力学特征”，
  - 在遇到没见过的系统时，也能快速调整控制策略，
  - 而且 **不用重新训练** 整个模型。

------

#### 实验效果

- 在 **PyBullet 仿真**里测试，当无人机电机推力变弱（常见的执行器故障）时：
  - 成功率达到 **95%**，位置误差只有 **0.129m**；
  - 远好于以前的方法（成功率 86%，误差 0.153m）。
- 他们还测试了不同配置的无人机（比如不同机臂长度、重量分布），结果表明这个方法 **泛化能力强**，在没见过的动力学环境中也能保持稳定。

------

#### 意义

- 证明了基于 Transformer 的 RL-FTC 框架，能让无人机在动态和不确定环境下更可靠。
- 换句话说：就算在野外任务中某个电机坏了，飞机也能快速适应，继续执行任务，而不是直接坠毁。

------

📌 **一句话总结**：
 这项研究用 **Transformer + 强化学习** 做了一个新的容错控制系统，让无人机即使部分电机失效，也能自适应地保持飞行，比现有方法更稳、更精准，适合在复杂真实环境中用。

}

## I. 引言

多旋翼无人机已成为各种野外机器人应用的有效工具，包括物流、监控和农业。在挑战性环境中多旋翼的稳定和鲁棒控制仍然至关重要。特别是，多旋翼通过电机旋转产生升力，使其特别容易受到执行器故障的影响，例如某些电机的效力损失(LoE)。这些故障立即导致失稳，使多旋翼比固定翼飞机更脆弱。因此，执行器故障在多旋翼飞行中是关键的，需要容错控制(FTC)策略来确保可靠性。

最近，使用强化学习(RL)的多旋翼FTC方法由于其学习复杂和灵活控制策略的潜力而受到关注。现有研究已成功展示了RL在各种故障条件下维持系统性能的能力，例如扭矩分配[1]和水下螺旋桨故障[2]，以及固定翼无人机控制[3]。广泛的研究特别探索了基于RL的多旋翼FTC。例如，提出了无模型RL基础FTC和原始控制器的并行组合作为对抗无人机信号欺骗的对策[4]。开发了集成高增益和低增益PID控制器输出的基于RL的策略[5]。引入了一种通过利用受故障影响的感知状态和来自预定义健康模型的状态之间差异来生成FTC信号的方法[6]。

然而，这些研究在部署到实际野外场景时面临限制。它们通常需要先验的系统特定健康模型，或在遇到先前未见的动力学时需要大量重新训练，例如任务中期载荷质量变化或多旋翼配置变化。这些约束显著降低了现实野外机器人应用中所需的适应性和灵活性。

为了解决这些挑战，我们提出了一种基于RL的混合FTC框架，结合了受快速运动适应(RMA)方法[7]启发的适应模块。与先前方法不同，我们的方法采用适应模块，根据传感器数据和指令历史采用适合当前无人机配置的控制策略。这种方法实现实时在线适应，无需额外训练，从而促进对先前未见动力学的适应，包括飞行中质量或惯性的变化。适应模块支持策略适应各种和不确定的环境。由[7]开发的RMA模块已在四足机器人控制任务中展示了成功的实时适应。从地面机器人扩展到空中系统，[8]、[9]的最近研究已实现四旋翼的在线适应，展示了对环境变化的鲁棒性而无需额外训练。此外，[10]成功地将四旋翼控制系统适应到外部干扰。虽然这些研究强调环境适应和对轻微振荡的弹性，但我们的工作独特地解决了重大执行器故障，例如高达36%的实质性推力损失。

进一步增强我们的框架，我们提出了一种新颖的基于变换器的适应模块，以增强超越现有方法的域推理能力。我们在使用PyBullet[11]的仿真悬停任务中验证我们的方法，尽管有实质性执行器故障，仍展示了鲁棒和可靠的位置控制。我们的结果强调了我们提出框架的实际适用性，推进了自主空中系统的鲁棒性和灵活性。

{

#### I. 引言（通俗版）

### 背景

多旋翼无人机（像常见的四旋翼）已经被广泛应用在 **物流、巡检、农业** 等领域。
 但它们有一个天然弱点：

- **靠电机旋转产生升力**。
- 一旦某个电机失效或推力下降（叫做效力损失 LoE），无人机很快就会 **失稳甚至坠毁**。
- 相比之下，固定翼飞机更稳，而多旋翼特别脆弱。

所以，必须有 **容错控制 (FTC)** 策略，来保证无人机即使部分电机失效，也能继续飞。

------

#### 现有方法（基于强化学习的 FTC）

近年来，大家开始用 **强化学习 (RL)** 来做容错：

- RL 能自动学到复杂的控制策略。
- 已经有人用它来应对无人机 **信号干扰**、**电机扭矩分配问题**，甚至应用到 **水下螺旋桨**和**固定翼无人机**。
- 在多旋翼上，也有一些研究：
  - 把 RL 策略和传统控制器（比如 PID）结合；
  - 或者用一个“健康模型”和“故障状态”的差异来生成修正信号。

------

#### 问题

这些方法在实验室里能跑，但到 **真实环境** 就有很大限制：

- 通常需要一个提前建好的 **无人机健康模型**；
- 一旦遇到 **新的配置**（比如无人机换了电池、载荷变重，甚至螺旋桨不同了），就要重新训练，成本很高；
- 适应性和灵活性不足，难以满足野外任务的需求。

------

#### 我们的方案

为了解决这些问题，作者提出了一个 **混合 RL 框架**，加入了一个 **适应模块**：

- 灵感来自四足机器人的 **快速运动适应 (RMA)** 方法。
- 适应模块根据 **传感器数据 + 控制历史**，自动调整控制策略，适配当前无人机状态。
- 好处：
  - 能够 **实时在线适应**，不需要重新训练；
  - 能应对飞行过程中 **载荷、质量、惯性变化**；
  - 对不确定环境和没见过的配置也能保持稳定。

------

#### 创新点：基于 Transformer 的适应模块

- 以往的方法大多只是普通的神经网络。
- 这篇工作提出用 **Transformer 模块**，因为它在序列推理和建模复杂依赖关系上更强。
- 结果：无人机在遇到严重电机故障（推力损失高达 **36%**）时，仍然能稳定悬停。

------

#### 实验验证

- 在 **PyBullet 仿真环境**里做了悬停实验。
- 即使电机严重损坏，框架依然能保持无人机的位置和稳定性。

------

📌 **一句话总结**：
 传统 RL 方法要么依赖先验模型，要么需要频繁重新训练，适应性差。本文提出的 **RL + Transformer 适应模块**，能让无人机在飞行中自动适应新的情况，甚至在严重电机故障下仍能稳定飞行，大大提高了可靠性。

}

## II. 方法

我们的框架包括标称控制器、策略和适应模块。本节描述了问题表述、两阶段训练过程和适应模块。详情请参见图1。

### A. 问题表述

**无人机模型描述：** 我们使用基于物理的gym-pybullet-drones环境[12]来仿真多旋翼动力学，包括姿态和位置变化。每个电机的推力和围绕多旋翼z轴产生的扭矩表达如下：

$$F_i = k_F \cdot u_{t,i}^2, \quad T = \sum_{i=1}^{4} (-1)^i k_T \cdot u_{t,i}^2$$  (1)

其中$$u_t = [u_{t,1}, u_{t,2}, u_{t,3}, u_{t,4}]^{\top}$$表示每个电机的每分钟转数(RPM)，$$k_F$$和$$k_T$$分别是升力和扭矩系数。

**故障表示：** 执行器受到磨损、螺旋桨损坏和电机过热的影响。这些故障被建模为效力损失，降低某些电机的RPM。故障矩阵$$\Lambda$$定义为：

$$\Lambda = \text{diag}(\lambda_1, \lambda_2, \lambda_3, \lambda_4), \quad \tilde{u}_t = \Lambda u_t$$ (2)

其中$$\lambda_i$$表示第i个电机的效率，标称值为1。有故障的RPM表示为$$\tilde{u}_t$$。我们考虑影响一个电机的故障，$$\lambda_i$$在0.8到0.9之间，导致高达36%的推力损失。这反映了现实的故障场景[6]。没有考虑完全故障，因为本研究专注于部分损伤情况。

### B. 混合控制器

我们提出了一种用于在LoE执行器故障下悬停的混合FTC控制器，结合标称控制器和策略π。我们还引入了环境因子编码器μ作为适应模块训练期间的教师模型。

**标称控制器：** 标称控制器使用级联比例-积分-微分(PID)结构进行位置和姿态控制。外环PID从位置误差计算z轴推力指令和期望姿态，而内环PID从姿态误差生成扭矩指令。这些指令被转换为RPM信号。这些PID增益针对无故障条件下的准确跟踪进行了优化，并在训练和测试中使用。

**RL策略：** 策略π辅助标称控制器在执行器故障下稳定悬停。在时间t，它接收状态$$x_t$$、先前控制输入$$u_{t-1}$$和潜在向量$$z_t$$来输出电机指令$$u_t$$。状态$$x_t$$包括位置、姿态、线速度和角速度。策略的控制被限制在标称指令的20%，确保在故障处理中的支持作用。从环境编码器或适应模块提取的$$z_t$$允许自适应控制。控制频率为30Hz。

**潜在表示：** 环境因子编码器μ压缩表示动力学特性的特权参数$$\xi_t$$，允许策略在各种域中保持一致的控制性能。这种方法与使用域适应方法的先前研究[7]、[8]、[9]一致，有效地捕获环境变化。此外，应用域随机化以促进策略π和编码器μ的泛化。具体地，$$\xi_t$$在±20%内随机变化，以防止策略过拟合到特定环境。

**奖励函数：** 奖励函数设计用于在故障情况下确保稳定性。它最小化位置和姿态误差，惩罚快速控制波动，并促进渐进成功。设p、v、θ和$$p_{target}$$分别表示多旋翼的位置、速度、姿态和目标位置。时间t的奖励定义为：

$$R_t = -k_{pos}\|p - p_{target}\| - k_{att}\|\theta\| - k_{vel}\|v\|^2 - k_{rate}\|\dot{u}_t\|^2 - k_{smooth}\|\ddot{u}_t\|^2 + 0.001 + R_{succ}$$ (3)

$$R_{succ} = \begin{cases}
1.0, & \text{如果 } \|p - p_{target}\| < \epsilon_{goal} \text{ 持续1秒} \\
0.1, & \text{如果 } \|p - p_{target}\| < \epsilon_{goal} \\
0.05, & \text{如果 } \|p - p_{target}\| < \epsilon_{near} \\
0, & \text{其他情况}
\end{cases}$$ (4)

其中缩放因子分别设置为0.5、0.1、0.001、0.5和0.1，阈值设置为$$\epsilon_{goal} = 0.173$$和$$\epsilon_{near} = 0.520$$。

### C. 基于变换器的适应模块

在现实世界中测量四旋翼和环境参数$$\xi_t$$是具有挑战性的。为了解决这个问题，我们用适应模块替换编码器μ。据我们所知，本文是RMA风格适应模块在容错控制中的首次应用，实现了无特权信息的鲁棒自适应控制。我们的适应模块h从1秒状态-动作轨迹推断潜在向量$$\hat{z}_t$$。该模块使用师生范式进行训练。冻结的环境编码器μ在不训练的情况下充当教师，h通过最小化均方误差$$\|\hat{z}_t - z_t\|^2$$学习模仿其输出。训练数据使用第一阶段学习的冻结策略收集。

与RMA[7]不同，我们引入了变换器架构[13]来改善推理性能。变换器利用自注意机制来捕获整个输入序列的时间相关性。这种方法有效地过滤噪声和振动，并识别关键物理特性。这促进了潜在表示的稳定性，确保即使在训练期间未见的动力学中也能保持鲁棒的推理性能。

{

#### II. 方法（通俗版）

这个框架主要由三部分组成：**标称控制器**（传统控制）、**RL 策略**（智能补偿）和**适应模块**（Transformer 在线学习）。

------

#### A. 问题表述

1. **无人机模型**
   - 我们用 **PyBullet 仿真环境**模拟四旋翼无人机。
   - 电机转得快，就能产生升力和扭矩，维持飞行。
2. **故障建模**
   - 电机会因为 **磨损、过热、桨叶损坏**等原因失效。
   - 我们用一个“效率系数” λ 来描述：正常是 λ=1，有故障时 λ 变成 0.8~~0.9，相当于推力下降 10~~36%。
   - 本文专注于 **部分失效**，没研究完全失效的情况。

👉 这样可以在仿真里逼真地再现现实中的电机故障。

------

#### B. 混合控制器（传统 + RL 策略）

1. **标称控制器（传统 PID）**
   - 外环 PID 控制位置 → 算出需要的推力和姿态；
   - 内环 PID 控制姿态 → 算出需要的扭矩；
   - 最后转成电机转速。
   - 优点：在 **无故障时很精准**。
2. **RL 策略 π（补偿作用）**
   - RL 策略用来 **辅助** PID，在电机出故障时帮忙稳住无人机。
   - 它的控制量被限制在 PID 输出的 **20%**，确保不会干扰正常控制，而是补偿作用。
   - RL 策略输入：无人机状态（位置、速度、姿态等）、上一步的动作，以及一个潜在向量 z。
   - 这个潜在向量相当于告诉策略 “当前无人机处在什么环境/故障状态”。
3. **潜在表示 z**
   - 为了让策略学会泛化，我们用一个 **环境编码器 μ** 来提取 z。
   - μ 输入一些特权参数（比如精确的动力学信息），输出压缩后的向量 z。
   - 训练时我们对环境随机扰动（比如 ±20% 的参数变化），避免过拟合到单一环境。
4. **奖励函数**
   - 奖励鼓励无人机保持在目标点稳定悬停。
   - 奖励项包括：位置误差小、姿态稳定、速度平缓、控制指令平滑。
   - 如果能长时间稳在目标点，会给额外奖励。

👉 总之，RL 策略学会在电机性能下降时 **给出适度修正**，和 PID 一起保持稳定。

------

#### C. 基于 Transformer 的适应模块

1. **问题：怎么得到 z？**
   - 在现实中，我们没法直接测量无人机的“真实动力学参数”。
   - 之前用的 μ 编码器依赖这些参数，现实中不可行。
2. **解决方案：适应模块 h**
   - 我们提出一个 **适应模块 h** 来替代 μ。
   - h 不依赖特权信息，只看 **过去 1 秒的状态和动作轨迹**，就能推断出一个潜在向量 $\hat{z}$。
   - 它相当于用历史表现来猜测 “无人机当前健康状态”。
3. **训练方式**
   - 用 **师生模式**训练：
     - 老师：冻结的 μ 编码器（有特权信息）。
     - 学生：适应模块 h，模仿 μ 的输出。
   - 数据来源：前面训练好的 RL 策略在仿真里飞行收集的数据。
4. **为什么用 Transformer？**
   - 以前的快速运动适应 (RMA) 多用 RNN/LSTM。
   - 我们改用 **Transformer**，因为它有：
     - 更强的 **时间序列建模能力**；
     - 自注意力机制能过滤掉噪声、振动；
     - 更容易捕捉关键动力学特征。
   - 好处：即使遇到没见过的动力学情况，适应模块也能快速推断并保持鲁棒性。

------

📌 **一句话总结**：
 这个框架结合了 **传统 PID 的稳定性**、**RL 策略的灵活性** 和 **Transformer 的在线适应能力**。这样即使无人机某个电机推力下降三成以上，也能靠 RL+Transformer 自动补偿，维持悬停稳定。

}

## III. 实验

### A. 实验设置

我们在gym-pybullet-drones[12]中评估了我们的框架，这是一个高保真仿真器。仿真使用Bitcraze Crazyflie 2.x(称为CF)作为基础四旋翼，并使用两个变体评估适应性：更大更重的UAV1和更轻的UAV2。它们的物理参数在表I中。

在第一阶段，策略π和环境编码器μ使用近端策略优化[14]算法联合训练。在第二阶段，适应模块h使用ADAM[15]优化器训练。环境编码器使用带有64维隐藏层的2层MLP。策略遵循Stable Baselines3[16]设置。为了比较适应模块结构，设计了基于CNN和基于变换器的模块。CNN模型具有3层1-D CNN架构，输入/输出通道数、核大小和步长分别设置为[32, 32, 4, 2]、[32, 32, 3, 1]和[32, 32, 3, 1]。变换器模型配置为16维输入、128模型维度、序列长度64、2个注意头、2个编码器层、128前馈维度和8维潜在输出。

### B. 实验结果

我们在仿真中验证我们的框架并与其他两种方法比较。RL-ours利用我们的混合控制器和基于变换器的适应模块。RL-RMA[7]共享相同控制器但采用基于1-D CNN的RMA风格适应模块进行基准测试。PID是没有故障处理的标准比例-积分-微分控制器。No Fault(oracle)评估无故障性能，代表RL-ours的理论上限，旨在在最优容错控制下匹配此水平。

**状态比较：** 我们比较了在电机2的30%推力损失下三种四旋翼模型的控制性能：CF、UAV1和UAV2。RL-ours展示了在边界内的成功收敛，而RL-RMA显示显著振荡并超出边界。PID变得不稳定并在约7秒后坠毁。图2显示了状态轨迹。

**定量测试：** 我们对100个随机初始状态进行定量测试，位置和故障幅度从表I采样。指标包括控制成功概率、位置的均方根误差(RMSE)和最大位置误差。成功定义为在至少1秒内保持0.173m内的位置误差。如表II所示，我们的方法在所有指标上优于其他方法，显示强大的容错能力。即使在未训练的无人机上也实现了精确控制和高成功率，展示了其适应性。

**鲁棒性测试：** 我们通过使用UAV1(一个未见模型)在故障范围内测试100个测试案例来评估鲁棒性。图3显示了状态轨迹。RL-ours稳定在目标附近，而其他方法经常显著偏离或振荡。图4展示了每个案例的位置RMSE和成功状态。RL-ours保持较低RMSE和较高成功率，即使在0.18以上的显著LoE故障中也是如此。其他方法随着故障幅度增加而困难。这表明我们方法的一致容错能力，即使在实质性故障和未见动力学中也是如此。

**潜在推理：** 最后，我们比较了提出的基于变换器的适应模块与RMA风格1-D CNN的潜在推理性能。在适应模块和环境因子编码器之间测量RMSE $$\|\hat{z}_t - z_t\|$$。RL-ours实现了0.1113 ± 0.0036的RMSE，远低于RL-RMA的5.5798 ± 0.0009，展示了改进的推理精度。值得注意的是，我们的方法保持精确的潜在推理，这证实了我们的模块有效地替换了环境编码器而无需特权参数。

{

## III. 实验（通俗版）

### A. 实验设置

- **仿真环境**：用 **gym-pybullet-drones** 仿真器（一个物理精度很高的无人机环境）。
- **测试机型**：
  - 基础机型：Crazyflie 2.x（CF，小型四旋翼）。
  - 变体：UAV1（更大更重），UAV2（更小更轻）。 → 用来测试适应性。
- **训练过程**：分两阶段：
  1. **阶段一**：用 **近端策略优化 (PPO)** 联合训练策略 π 和环境编码器 μ。
  2. **阶段二**：训练适应模块 h（用 ADAM 优化器）。
- **模型对比**：
  - **RL-ours**：我们的方法（混合控制器 + Transformer 适应模块）。
  - **RL-RMA**：对比方法（同样控制器，但适应模块是 CNN 版本）。
  - **PID**：普通 PID 控制（没有故障处理）。
  - **No Fault (oracle)**：无故障的理想情况，相当于“上限表现”。

👉 可以理解为：我们的方法（RL+Transformer） vs 传统 RL（CNN） vs 传统控制器。

------

### B. 实验结果

1. **状态比较（定性）**

   - 在电机 2 推力下降 **30%** 的情况下：
     - **RL-ours**：能稳定悬停在目标附近；
     - **RL-RMA**：出现明显振荡，飞行轨迹超出范围；
     - **PID**：直接失控，大约 7 秒就坠毁。

   👉 图2的轨迹结果清楚展示：只有我们的方法能“稳住”。

------

1. **定量测试**

   - 随机采样 100 个不同初始状态和故障强度，测试三个指标：
     - **成功率**（保持在 0.173m 范围内 ≥1s）；
     - **位置误差 RMSE**（平均偏差大小）；
     - **最大位置误差**（最糟的情况）。

   **结果**：

   - 我们的方法在所有指标上都最好；
   - 即使在没训练过的新机型上，也能保持高成功率和低误差。

👉 说明方法的泛化性和适应性很强。

------

1. **鲁棒性测试**
   - 在 **UAV1（没见过的更重机型）** 上，随机测试 100 个不同的故障情况：
     - **RL-ours**：稳定在目标点附近；
     - **其他方法**：要么大幅偏离，要么强烈振荡。
   - 随着故障加剧（超过 **18% 推力损失**），
     - **RL-ours** 仍能保持低误差和高成功率；
     - 其他方法表现迅速恶化。

👉 表明我们的方法在 **严重故障** 和 **未知动力学** 下仍然可靠。

------

1. **潜在推理对比（Transformer vs CNN）**
   - 适应模块的任务是预测潜在表示 $z$（描述环境/动力学特性）。
   - 对比误差：
     - **RL-ours (Transformer)**：RMSE ≈ **0.11**；
     - **RL-RMA (CNN)**：RMSE ≈ **5.58**。

👉 结果差距非常大！Transformer 适应模块能更精准地推断潜在信息，几乎完美替代了环境编码器，不需要依赖“特权参数”。

------

📌 **一句话总结**：
 实验结果显示：

- 我们的方法（RL+Transformer）在面对电机故障时比 CNN 方法和传统 PID 更稳；
- 能适应不同机型、不同程度的故障，表现一致且鲁棒；
- Transformer 模块大幅提升了“环境理解能力”，让无人机能在未知环境里依然保持稳定飞行。

}

## IV. 结论

我们提出了在不确定和变化的野外条件下基于强化学习的容错控制。受RMA最新进展启发，我们提出的框架实现了鲁棒的实时推理，无需额外的特权信息或重新训练。实验表明，我们的方法在各种执行器故障和未见动力学场景中优于PID和基于CNN的方法，包括效力损失故障和四旋翼物理参数变化。我们的方法即使在训练分布之外也保持优越的稳定性。这些优势使我们的框架在实际野外应用中可靠，即使载荷意外变化，从而降低任务复杂性。

## 参考文献

[1] H. Deng, Y. Zhao, A.-T. Nguyen, and C. Huang, "Fault-tolerant predictive control with deep-reinforcement-learning-based torque distribution for four in-wheel motor drive electric vehicles," IEEE/ASME Transactions on Mechatronics, vol. 28, no. 2, pp. 668–680, 2023.

[2] J. Qin, M. Zhong, W. Gai, and Z. Ding, "Fault-tolerant trajectory tracking control based on ddpg algorithm for underwater vehicle with propeller faults," International Journal of Control, Automation and Systems, vol. 22, no. 4, pp. 1418–1429, 2024.

[3] F. Giral, I. Gomez, R. Vinuesa, and S. Le-Clainche, "Transformer-based fault-tolerant control for fixed-wing uavs using knowledge distillation and in-context adaptation," arXiv preprint arXiv:2411.02975, 2024.

[4] F. Fei, Z. Tu, D. Xu, and X. Deng, "Learn-to-recover: Retrofitting uavs with reinforcement learning-assisted flight control under cyber-physical attacks," in 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 7358–7364.

[5] Y. Sohege, M. Quiñones-Grueiro, and G. Provan, "A novel hybrid approach for fault-tolerant control of uavs based on robust reinforcement learning," in 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 10 719–10 725.

[6] X. Liu, Z. Yuan, Z. Gao, and W. Zhang, "Reinforcement learning-based fault-tolerant control for quadrotor uavs under actuator fault," IEEE Transactions on Industrial Informatics, 2024.

[7] A. Kumar, Z. Fu, D. Pathak, and J. Malik, "Rma: Rapid motor adaptation for legged robots," arXiv preprint arXiv:2107.04034, 2021.

[8] D. Zhang, A. Loquercio, X. Wu, A. Kumar, J. Malik, and M. W. Mueller, "Learning a single near-hover position controller for vastly different quadcopters," in 2023 IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 1263–1269.

[9] D. Zhang, A. Loquercio, J. Tang, T.-H. Wang, J. Malik, and M. W. Mueller, "A learning-based quadcopter controller with extreme adaptation," arXiv preprint arXiv:2409.12949, 2024.

[10] H.-L. Hsu, H. Meng, S. Luo, J. Dong, V. Tarokh, and M. Pajic, "Reforma: Robust reinforcement learning via adaptive adversary for drones flying under disturbances," in 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024, pp. 5169–5175.

[11] E. Coumans and Y. Bai, "Pybullet, a python module for physics simulation for games, robotics and machine learning," http://pybullet.org, 2016–2021.

[12] J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P. Schoellig, "Learning to fly—a gym environment with pybullet physics for reinforcement learning of multi-agent quadcopter control," in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 7512–7519.

[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.

[14] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017.

[15] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.

[16] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and N. Dormann, "Stable-baselines3: Reliable reinforcement learning implementations," Journal of machine learning research, vol. 22, no. 268, pp. 1–8, 2021.