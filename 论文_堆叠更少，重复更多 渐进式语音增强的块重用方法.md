# 堆叠更少，重复更多：渐进式语音增强的块重用方法

**作者**：Jangyeon Kim¹, Ui-Hyeop Shin¹, Jaehyun Ko¹, Hyung-Min Park¹²

¹人工智能系，西江大学，韩国
 ²电子工程系，西江大学，韩国

邮箱：{jykim97, dmlguq123, jhko, hpark}@sogang.ac.kr

## 摘要

本文提出了一种高效的语音增强（SE）方法，该方法重复使用一个处理块，而非传统的堆叠方式。与增加块数量来学习深层潜在表示不同，重复单个块能够在减少参数冗余的同时实现渐进式细化。我们还通过保持编码器和解码器较浅并重用单个序列建模块来最小化域变换。实验结果表明，处理阶段的数量比具有不同权重的块数量对性能更为重要。此外，我们观察到所提出的方法在单个块内逐步细化噪声输入。进一步地，通过块重用方法，我们证明了深化编码器和解码器对于学习深层复杂表示可能是冗余的。因此，实验结果确认了所提出的块重用能够实现渐进式学习，并为SE提供了高效的替代方案。

**关键词**：语音增强，块重用，渐进式细化，双路径架构，参数效率

{

#### 核心想法：用"重复练习"代替"复杂装备"

#### 传统方法 vs 新方法

**传统语音增强方法**：

- 就像**用很多不同的工具**来修复声音
- 需要大量不同的处理模块（参数很多）
- 就像修车时用扳手、螺丝刀、钳子等各种工具

**这篇论文的新方法**：

- 就像**用同一把万能工具反复使用**
- 只用一个处理块，但重复使用多次
- 就像用一把好的多功能工具，反复处理同一个地方

#### 为什么这样做更好？

### 1. 更省资源

- **参数少**：不需要训练很多不同的模块
- **效率高**：一个模块反复用，而不是造很多个

#### 2. 渐进式改进

**就像修图软件**：

- **第1次处理**：去掉大噪音
- **第2次处理**：细化一些小噪音
- **第3次处理**：再精细调整
- ...

每次都用同样的"滤镜"，但每次都让声音更干净一点。

#### 实验发现

#### 重要发现1：次数比复杂度更重要

- **处理多少次** > **用多复杂的工具**
- 就像擦桌子：用简单抹布擦10次 > 用高级清洁剂擦1次

#### 重要发现2：简单架构就够了

- 不需要很深的编码器-解码器
- 保持简单，重复使用就行
- 就像用简单的方法反复做，效果更好

#### 生活化比喻

**传统方法** = 装修房子时买很多专业工具

- 电钻、切割机、打磨机、喷漆枪...
- 每个工具用一次，成本很高

**新方法** = 用一把好的多功能工具反复使用

- 一把优质的电动工具
- 不同步骤重复使用
- 成本低，效果好

#### 核心洞察

**关键发现**：

1. **重复 > 复杂**：同一个简单方法多次使用比复杂方法用一次效果更好
2. **渐进改进**：每次小幅度改善，累计效果显著
3. **参数效率**：用更少的参数达到更好的效果

这个思想其实和**CoT（思维链）**很相似：

- CoT：让AI重复使用"思考"这个动作
- 这篇论文：让网络重复使用同一个"处理"模块
- **共同点**：都是通过"重复简单动作"获得复杂能力

**实用价值**：

- 训练更快（参数少）
- 部署更轻量（模型小）
- 效果不差（甚至更好）

}

## 1 引言

随着深度学习的快速发展，语音增强（SE）作为各种应用中的关键预处理步骤得到了显著改善。特别是，从使用短时傅里叶变换（STFT）在时频（TF）域中的传统实值掩码，复数掩码的估计已经带来了SE的实质性改进[1, 2]。此外，从基于U-Net的架构[1, 2, 3, 4, 5]开始，TF域中的双路径建模进一步提升了语音分离（SS）[6, 7, 8]和SE[9, 10, 11, 12]的性能。最近，除了基于掩码的方法外，基于直接谱映射的研究也得到了探索，通过部分输出增强信号[11, 12]或完全输出[7, 8]来消除掩码的需求。

双路径建模的进展得益于基于Transformer架构的序列建模能力的发展[8, 9, 10, 11]。基于Transformer的双路径模型通过堆叠多个层已经展现出显著的性能改进。同时，虽然模型性能持续改善，但出现了一个明显的趋势：模型大小普遍减小，而计算复杂度增加。这一趋势在从基于U-Net的架构向基于Transformer的TF模型的转变中很明显。据报告，受计算复杂度影响的特征处理程度比模型中参数数量和非线性投影的不同表示之间的域转换复杂度更为关键[13]。

基于这一观察，我们考虑了一种替代方法，即重复使用一个处理块，而非堆叠多个处理块。这种跨层权重共享在自动语音识别[14, 15]或自然语言处理[16, 17]中得到了积极探索。然而，由于这些任务通常涉及将输入转换到不同域，权重共享可能导致性能限制[18]。为了缓解这一问题，引入了部分权重共享[14]或基于适配器的机制[18, 19, 20]。另一方面，在SS中，这种迭代块重用技术已被成功应用[21, 22]并显示有效。这可能是因为SS在输入和输出的同一域内操作。当输入和输出域保持相同时，我们假设通过迭代块重用的渐进式细化可以作为使用具有多个块的模型学习深层潜在表示的更高效替代方案。

基于这一动机，我们在SE中引入了块重用方法。与传统堆叠相比，重用块不会在SE中导致显著的性能下降。此外，我们研究了堆叠和重复块数量的影响。经验上，处理阶段数量本身比具有独特权重的块数量对性能更为重要。我们的实验还显示，使用重复块的训练自然地使单个块逐步细化输入，这与传统堆叠架构形成对比。此外，用于学习更深潜在表示的深度编码器-解码器结构可能是冗余的。

为了验证我们的假设，我们选择MP-SENet[11]作为基线，它具有被最近模型[1, 2, 5, 10]广泛采用的卷积编码器-解码器（CED）架构。然后，我们通过保持CED较浅同时增加中间层单个基于Transformer的双路径块的重复来最小化域变换。值得注意的是，尽管参数和计算复杂度减少，所提出的模型仍保持竞争性能。这表明重用块能够实现渐进式特征细化，同时显著减少参数，使其成为SE任务的高效方法，而无需依赖通过深度编码器和解码器学习的深层潜在特征。

![比较图](https://claude.ai/chat/image_placeholder) *图1：块(a)堆叠和(b)重复的比较*

{

#### 背景：语音增强技术的发展历程

#### 传统发展路径

**第1阶段**：简单滤镜方法

- 用数学公式直接处理声音频谱

**第2阶段**：U-Net架构

- 就像图像处理的"美颜相机"
- 能去掉噪音，保留人声

**第3阶段**：Transformer双路径模型

- 更智能，像AI一样理解声音
- **问题**：模型越来越复杂，计算量越来越大

#### 核心观察：一个重要发现

研究者发现了一个矛盾现象：

- **性能提升**：效果越来越好
- **模型变重**：参数和计算量急剧增加

**关键洞察**：

> "处理多少次" 比 "用多复杂的工具" 更重要

#### 灵感来源：其他领域的经验

#### 成功案例

**语音分离**：已经有人用"重复使用"的方法成功了

- 原因：输入和输出都是音频，在同一个"域"里
- 就像用同一把刷子反复刷同一面墙

#### 失败案例

**语音识别和自然语言处理**：重复使用效果不好

- 原因：输入是声音，输出是文字，跨越不同"域"
- 就像用画笔去修车，工具不匹配

#### 本研究的核心想法

#### 基本假设

**语音增强的特点**：

- **输入**：含噪音的音频
- **输出**：干净的音频
- **关键**：都是音频，在同一个域！

**推理逻辑**： 既然语音分离能成功，语音增强应该也可以用同样方法

#### 具体做法

**选择基础模型**：MP-SENet

- 这是个经典的、被广泛使用的模型
- 有"编码器-解码器"结构

**关键改进**：

1. **保持编码器-解码器简单**（浅层）
2. **中间用一个Transformer块反复处理**

**就像这样**：

```
简单编码器 → [同一个处理块] × N次 → 简单解码器
```

#### 实验发现

#### 惊人结果

- **参数更少**：模型更轻量
- **计算更快**：效率更高
- **效果不差**：性能保持竞争力

#### 重要观察

**渐进式改善**：

- 第1次处理：去掉大噪音
- 第2次处理：细化小噪音
- 第3次处理：精细调整
- ...

每次都用同一个"工具"，但每次都让声音更干净一点。

}

## 2 所提出的方法

SE任务基本上涉及生成一个输出信号，该信号仅保留所需的语音成分，同时保持与输入相同的域。因此，与其使用多个块学习复杂表示，通过单个块的迭代处理进行渐进式细化可以作为一种有效策略。

### 2.1 SE网络概述

如图1(a)所示，传统的SE方法通常基于具有堆叠序列建模块的CED架构[10, 11, 12]。当在TF域中给定具有$$T$$帧和$$F$$频率bin的噪声信号时，噪声输入被编码为TF域中形状为$$C \times T \times F$$的特征表示，其中$$C$$是特征维度。编码的特征然后被$$B$$个序列建模块的堆叠处理。最后，解码器将输出特征转换回其原始形状，以估计掩码值或直接语音信号。

### 2.2 所提出的块重用方法

与堆叠多个块不同，我们提出重复使用单个序列建模块，如图1(b)所示，这可能导致无显式中间损失的渐进式增强。特别是，这种通常被称为展开方法的策略，根据处理阶段之间的信息融合方案可以有一些变体[21]。当使用共享块的输出作为后续迭代中的输入时，我们可以简单地重用前一阶段的输出作为下一阶段的输入，作为直接连接（DC）。同时，我们也可以将初始特征添加到每个阶段输出中作为求和连接（SC），以鼓励利用输入特征。

另一方面，简单地重用单个块对于更复杂的噪声输入可能具有挑战性。因此，我们可以考虑重复块的堆叠来增强序列块的建模能力。

### 2.3 用于SE的双路径TF模型

为了验证和分析我们的块重用方法，我们选择TF-Locoformer[8]和MP-SENet[11]作为实验的基线模型。两个网络都使用双路径TF块来建模TF域中的输入特征。双路径TF块交替沿时间和频率维度处理特征，分别捕获时间和频间依赖性，如图2所示。

![基线网络图](https://claude.ai/chat/image_placeholder) *图2：使用双路径TF模型的两个基线网络*

#### 2.3.1 TF-Locoformer

最近提出的TF-Locoformer[8]在SS和SE中取得了令人印象深刻的性能。如图2(a)所示，它遵循CED框架，分别在编码器和解码器中仅包含单个Conv2D和Deconv2D层。从噪声输入的实部和虚部（RI）成分，TF-Locoformer直接估计增强信号的RI成分。此外，作为双路径TF块的单元模块，TF-Locoformer使用修改的前馈网络，通过具有Swish门控线性单元（Conv-SwiGLU）的卷积层作为Transformer中的macaron风格结构来增强局部建模。详细结构请参考[8]。

#### 2.3.2 MP-SENet

如图2(b)所示，MP-SENet基于CED架构结合了幅度掩码和相位映射，其中编码器由卷积块、扩张DenseNet[23]和另一个卷积块的级联组成，而两个并行解码器分别由扩张DenseNet后跟解卷积块组成，用于幅度掩码和相位映射。作为双路径TF块的单元，MP-SENet使用Conformer[24]，其中卷积模块被合并到macaron风格的Transformer中以增强局部建模能力。

{

#### 2.1 传统语音增强的工作方式

### 基本流程

想象语音增强就像**洗衣服**：

**传统方法**（图1a）：

1. **编码器**：把脏衣服放进洗衣机（转换格式）

2. 多个处理块

   ：用不同的洗涤程序

   - 第1个块：预洗
   - 第2个块：主洗
   - 第3个块：漂洗
   - 第4个块：脱水

3. **解码器**：把干净衣服拿出来（转回原格式）

**输入**：噪声语音（脏衣服） **输出**：干净语音（洁净衣服）

#### 2.2 新提出的"块重用"方法

#### 核心想法

**新方法**（图1b）：用**同一个洗衣程序反复洗**

```
脏衣服 → [同一洗涤程序] → [同一洗涤程序] → [同一洗涤程序] → 干净衣服
```

#### 两种连接方式

**直接连接（DC）**：

- 第1次洗完的衣服直接进入第2次洗
- 就像接力赛，一棒传一棒

**求和连接（SC）**：

- 每次洗完后，都和原来的脏衣服"对比"一下
- 确保不会洗坏，保留好的部分

#### 增强版本

**如果衣服太脏怎么办？**

- 可以把几个"同样的洗衣程序"叠起来用
- 增强处理能力

#### 2.3 两个具体的测试模型

为了验证这个想法，研究者选了两个已有的成功模型来测试：

#### 2.3.1 TF-Locoformer

**特点**：

- **超轻量级架构**：编码器和解码器都很简单

- 直接方法

  ：

  - 输入：含噪音的复数信号（实部+虚部）
  - 输出：干净的复数信号

- 核心处理单元

  ：使用改进的前馈网络

  - 就像一个**智能滤波器**
  - 既能处理时间维度，又能处理频率维度

**工作方式**：

```
噪声信号 → 简单编码 → [智能双路径处理] × N次 → 简单解码 → 干净信号
```

#### 2.3.2 MP-SENet

**特点**：

- 双重处理策略

  ：

  - **幅度掩码**：确定哪些频率保留多少
  - **相位映射**：调整声音的时间关系

- **更复杂的架构**：编码器和解码器都比较深

- **核心处理单元**：Conformer（结合了Transformer和CNN的优点）

**工作方式**：

```
                    ┌→ [幅度处理] → 幅度掩码
噪声信号 → 复杂编码 ┤
                    └→ [相位处理] → 相位映射
                              ↓
                         合并 → 干净信号
```

#### 双路径处理的核心思想

**为什么叫"双路径"？**

就像**修图软件的两个滑块**：

- **时间路径**：处理声音随时间的变化（比如去掉咔嚓声）
- **频率路径**：处理不同频率的音量（比如去掉嗡嗡声）

**交替处理**：

1. 先调时间维度
2. 再调频率维度
3. 反复交替

就像修图时先调亮度，再调对比度，来回调整直到满意。

#### 关键洞察

**为什么这样做有效？**

1. **同域处理**：输入输出都是音频，适合重复使用
2. **渐进改善**：每次处理都让声音更干净一点
3. **参数高效**：用一套参数反复使用，而不是训练很多套

**实际效果**：

- 模型更小（省内存）
- 训练更快（参数少）
- 效果不差（甚至更好）

}

## 3 实验设置

### 3.1 数据集和评估

为了训练和评估所提出的方法，我们对TF-Locoformer使用Interspeech DNS-Challenge 2020数据集[25]，对MP-SENet使用VoiceBank+DEMAND数据集[26]。DNS数据集包含500小时的清洁语音数据和超过180小时的噪声数据。我们通过在-5到15 dB的信噪比（SNR）范围内混合清洁语音和噪声来生成训练数据。对于评估，我们使用SNR范围为0到25 dB的DNS非盲测试集（无混响）。使用语音质量感知评估（PESQ）、短时客观可懂度（STOI）和尺度不变信号失真比（SI-SDR）作为评估指标。

VoiceBank+DEMAND数据集包括来自28个说话人的11,572个训练话语，SNR为0、5、10和15 dB，以及来自2个说话人的824个测试话语，SNR为2.5、7.5、12.5和17.5 dB。对于VoiceBank+DEMAND数据集，评估指标包括PESQ、分段SNR（SSNR）和STOI。作为额外指标，采用CSIG、CBAK和COVL来评估感知质量的不同方面[10]。在所有实验中，采样率设置为16 kHz。我们还比较了1秒输入的参数大小和乘累积操作（MAC）数量。

**表1**：DNS数据集上TF-Locoformer中$$B$$和$$R$$的各种配置的评估。最佳性能用粗体突出显示，第二佳性能带下划线。

| $$B$$ | $$R$$ | 参数量(M) | MAC(G/s) | PESQ-WB     | PESQ-NB     | STOI(%)      | SI-SDR(dB)   |
| ----- | ----- | --------- | -------- | ----------- | ----------- | ------------ | ------------ |
| Noisy | -     | -         | -        | 1.58        | 2.45        | 91.52        | 9.07         |
| 1     | 1     | 0.5       | 8.7      | 2.82        | 3.28        | 96.51        | 18.46        |
| 4     | 1     | 1.9       | 34.8     | 3.41        | 3.74        | 98.10        | 20.70        |
| 8     | 1     | 3.7       | 69.6     | 3.47        | 3.79        | 98.26        | 21.07        |
| 12    | 1     | 5.6       | 104.4    | 3.49        | 3.81        | 98.31        | 21.32        |
| 16    | 1     | 7.4       | 139.2    | **3.55**    | **3.86**    | **98.41**    | **21.67**    |
| 1     | 4     | 0.5       | 34.8     | 3.26        | 3.64        | 97.78        | 19.88        |
| 1     | 8     | 0.5       | 69.6     | 3.39        | 3.73        | 98.08        | 20.51        |
| 1     | 12    | 0.5       | 104.4    | 3.43        | 3.76        | 98.18        | 20.91        |
| 1     | 16    | 0.5       | 139.2    | 3.43        | 3.79        | 98.21        | 20.85        |
| 2     | 8     | 0.9       | 139.2    | 3.48        | 3.81        | 98.31        | 21.06        |
| 4     | 4     | 1.9       | 139.2    | <u>3.52</u> | <u>3.83</u> | <u>98.38</u> | <u>21.32</u> |
| 8     | 2     | 3.7       | 139.2    | 3.46        | 3.80        | 98.31        | 21.12        |

![PESQ vs MAC图](https://claude.ai/chat/image_placeholder) *图3：表1中$$B$$和$$R$$值组合的PESQ-WB vs. MAC图。每个圆圈的大小与参数大小成正比。*

### 3.2 训练和模型配置

所有模型使用AdamW优化器[27]训练100个epoch。TF-Locoformer使用4秒段进行训练，批量大小为1。STFT使用大小为256的Hanning窗口和128的跳跃大小计算。$$C$$设置为64。Conv-SwiGLU模块[8]中的隐藏维度和核大小分别设置为172和3。多头自注意力的头数为4。对于训练，使用了时域$$L_1$$损失和TF域多分辨率$$L_1$$损失[28]。注意，原始工作[8]中的输入归一化和损失函数中的缩放因子仅在第4.4节中应用。

另一方面，MP-SENet使用2秒段进行训练，批量大小为2。我们遵循原始工作中相同的模型配置和训练程序，包括STFT[11]。然而，注意度量判别器未用于模型训练。

## 4 结果

### 4.1 变化块数和重复数的影响

基于TF-Locoformer，我们首先探索了在DNS数据集上变化块数$$B$$和重复数$$R$$如何影响模型性能。注意所有结果都使用DC方法进行信息融合方案。在表1和图3中，我们观察到随着$$B$$增加，结果与相应的参数增加一致地改善。值得注意的是，使用仅0.5M参数的单个块$$B=1$$，增加$$R$$在所有指标上都带来显著的性能改善。

为了进一步探索通过约束计算成本下参数和模型性能之间的权衡，我们使用$$B \times R$$固定为16的$$B$$和$$R$$的各种组合进行实验。虽然性能因$$B$$和$$R$$的组合而异，但它们的差异并不显著，因为总处理阶段数$$B \times R$$是常数。特别地，值得注意的是，$$B=4$$和$$R=4$$的组合（记为$$B4R4$$）取得了与$$B16R1$$配置相当的性能，同时使用约四分之一的参数。这突出了所提出方法的有效性，证明它可以用比传统堆叠架构显著更少的参数获得竞争结果。

![中间输出图](https://claude.ai/chat/image_placeholder) *图4：DNS数据集上B16R1、B8R1、B1R16和B1R8中间输出的PESQ-WB和SI-SDR结果图。*

![频谱图](https://claude.ai/chat/image_placeholder) *图5：DNS数据集中样本话语在不同处理阶段的频谱图。*

### 4.2 块重用渐进增强的可视化

为了分析重用单个块的网络与具有多个块的网络的行为，我们绘制了使用相同解码器在每个网络中$$B16R1$$和$$B8R1$$中索引为$$1 \leq b \leq B$$的中间输出，以及$$B1R16$$和$$B1R8$$中索引为$$1 \leq r \leq R$$的中间输出的评估结果，如图4所示。$$B1R16$$和$$B1R8$$通过重复使用单个块，从噪声输入水平开始，固有地学习逐步增强信号。这种渐进式细化也可以在图5的样本频谱图中很好地观察到，这与图4的结果一致。

相比之下，$$B16R1$$和$$B8R1$$中的中间输出直到$$b=10$$和$$b=5$$才显示噪声输入的增强。这可能是由于$$B$$个块的功能分区为编码器和解码器角色，这可能使网络能够学习深层潜在特征。

### 4.3 块重用中融合方案的比较

**表2**：块重用中融合方案的比较。

| 系统      | 融合方案 | PESQ-WB  | PESQ-NB  | STOI(%)   | SI-SDR(dB) |
| --------- | -------- | -------- | -------- | --------- | ---------- |
| $$B16R1$$ | -        | 3.55     | 3.86     | 98.41     | 21.67      |
| $$B1R16$$ | DC       | 3.43     | 3.79     | 98.21     | 20.85      |
| $$B1R16$$ | SC       | 3.46     | 3.80     | 98.21     | 21.13      |
| $$B4R4$$  | DC       | 3.52     | 3.83     | 98.38     | 21.32      |
| $$B4R4$$  | SC       | **3.56** | **3.87** | **98.46** | **21.84**  |

在具有块重用的网络中，我们比较了DC和SC两种融合方案。表2评估了$$B1R16$$和$$B4R4$$的情况。结果表明，SC方法通过重复添加输入特征比DC方法取得更好的性能，这与[21]中的发现一致。使用SC方法的模型$$B4R4$$在参数更少的情况下（7.4M vs. 1.9M）超越了作为传统方法的模型$$B16R1$$。这些结果突出了所提出方法在SE中的效率。

### 4.4 与现有方法的比较

**表3**：所提出模型与DNS数据集上先前模型的比较。

| 系统             | 参数量(M) | MAC(G/s) | PESQ-WB     | PESQ-NB     | STOI(%)      | SI-SDR(dB)   |
| ---------------- | --------- | -------- | ----------- | ----------- | ------------ | ------------ |
| Noisy            | -         | -        | 1.58        | 2.45        | 91.52        | 9.07         |
| FullSubNet[29]   | 5.6       | 31.4     | 2.78        | 3.31        | 96.11        | 17.29        |
| CTSNet[30]       | 4.4       | 5.6      | 2.94        | 3.42        | 96.21        | 16.69        |
| TaylorSENet[31]  | 5.4       | 6.2      | 3.22        | 3.59        | 97.36        | 19.15        |
| FRCRN[4]         | 6.9       | 242.0    | 3.23        | 3.60        | 97.69        | 19.78        |
| MFNet[5]         | 6.1       | 6.1      | 3.43        | 3.74        | 97.98        | 20.31        |
| USES[32]         | 3.1       | 65.3     | 3.46        | -           | 98.1         | 21.2         |
| TF-Locoformer[8] | 15.0      | 255.1    | **3.72**    | -           | **98.8**     | **23.3**     |
| $$B1R16$$-SC     | 0.5       | 139.2    | 3.53        | 3.85        | 98.37        | 21.71        |
| $$B4R4$$-SC      | 1.9       | 139.2    | <u>3.64</u> | <u>3.92</u> | <u>98.54</u> | <u>22.12</u> |

我们将所提出的网络与DNS数据集上的先前研究进行了比较。为了公平比较，所提出的网络通过应用输入归一化和损失函数中的缩放因子$$\alpha$$进行训练[8]。在表3中，仅有0.5M参数的$$B1R16$$-SC尽管参数最少，但与大多数现有方法相比取得了优越或具有竞争力的性能。

接下来，我们检查$$B4R4$$-SC模型，该模型在所提出方法中展现了最高的参数效率和性能。该模型大幅超越了大多数先前工作。虽然所提出的模型与TF-Locoformer[8]相比性能略低，但值得注意的是，所提出的模型保持了更小的模型大小和约一半的计算成本，同时仍提供高性能。

### 4.5 深度编码器和解码器的研究

**表4**：VoiceBank+DEMAND数据集上MP-SENet中$$K$$、$$B$$和$$R$$各种配置的评估。

| $$B$$ | $$R$$ | 参数量(M) | MAC(G/s) | PESQ-WB | STOI(%) | SSNR(dB) | CSIG | CBAK | COVL |
| ----- | ----- | --------- | -------- | ------- | ------- | -------- | ---- | ---- | ---- |
| Noisy | -     | -         | -        | 1.97    | 91.00   | 1.68     | 3.35 | 2.44 | 2.63 |

**$$K=4$$的DenseNet编码器-解码器（默认）** | 4 | 1 | 2.1 | 43.9 | 3.37 | 95.87 | 10.68 | 4.68 | 3.89 | 4.13 |

**$$K=2$$的DenseNet编码器-解码器** | 4 | 1 | 1.3 | 27.2 | 3.39 | 95.80 | 10.53 | 4.69 | 3.89 | 4.15 | | 1 | 4 | 0.6 | 27.2 | 3.27 | 94.51 | 10.46 | 4.60 | 3.83 | 4.02 |

**$$K=1$$的DenseNet编码器-解码器** | 4 | 1 | 1.1 | 22.4 | 3.29 | 95.43 | 10.00 | 4.64 | 3.81 | 4.06 | | 1 | 4 | 0.4 | 22.4 | 3.26 | 95.44 | 10.25 | 4.62 | 3.81 | 4.03 | | 6 | 1 | 1.5 | 32.0 | **3.41** | **96.05** | **10.59** | **4.74** | **3.90** | **4.18** | | 1 | 6 | 0.4 | 32.0 | 3.36 | 95.65 | 10.21 | 4.68 | 3.86 | 4.13 |

最后，我们检查了MP-SENet编码器和解码器中扩张DenseNet[23]深度$$K$$变化的影响，这直接关系到域变换的程度。在表4中，从$$K=4$$的默认设置开始，我们评估了$$K=2$$和$$K=1$$的情况。当$$B=4$$时，减少$$K$$显著降低了参数和计算成本。特别地，与$$K=4$$相比，$$K=1$$的情况需要约一半的参数和计算成本。尽管模型大小和复杂度减少，$$K=1$$观察到的性能下降相对较小。更值得注意的是，$$K=2$$的模型达到了与$$K=4$$情况相当的性能。

使用$$K=1$$的浅编码器和解码器，$$B=6$$，$$R=1$$的情况超越了默认配置，即使参数和计算更少也达到了最佳性能。此外，$$B=1$$和$$R=6$$配置导致参数大幅减少，性能下降很小。特别地，$$B=1$$，$$R=6$$的模型即使参数显著更少（0.4M）也达到了与默认设置相当的竞争结果。这些结果一致支持这样的观点：具有深度编码器-解码器的复杂域变换并非必需，而处理迭代次数在SE任务中起更关键作用。

## 5 结论

我们提出了一个高效重用处理块的渐进式SE框架。我们确认处理迭代次数比参数大小更为关键。重复单个块能够实现渐进式细化，以更少参数获得有效性能。此外，我们证明了通过浅CED和块重用最小化域变换能够在SE任务中实现渐进式特征细化，同时显著减少参数，而无需依赖深层潜在表示。

然而，本工作有几个应在未来研究中解决的限制。首先，我们专注于非混响条件下的去噪任务。未来研究应调查模型在混响环境中的性能，以评估其在真实世界场景中的鲁棒性。其次，我们仅使用双路径TF模型进行实验。需要对各种模型进行进一步实验以验证所提出方法的通用性能。最后，我们的研究主要检查了非因果模型。由于SE任务通常需要实时应用，如助听器和电信系统，未来工作应考虑因果架构。

## 参考文献

[1] H.-S. Choi, J.-H. Kim, J. Huh, A. Kim, J.-W. Ha, and K. Lee, "Phase-aware speech enhancement with deep complex u-net," in Proc. Int. Conf. Learn. Represent. (ICLR), 2018.

[2] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie, "DCCRN: Deep Complex Convolution Recurrent Network for Phase-Aware Speech Enhancement," in Proc. Interspeech, 2020, pp. 2472–2476.

[3] Z. Kong, W. Ping, A. Dantrey, and B. Catanzaro, "Speech Denoising in the Waveform Domain With Self-Attention," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2022, pp. 7867–7871.

[4] S. Zhao, T. H. Nguyen, and B. Ma, "Monaural Speech Enhancement with Complex Convolutional Block Attention Module and Joint Time Frequency Losses," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2021, pp. 6648–6652.

[5] L. Liu, H. Guan, J. Ma, W. Dai, G. Wang, and S. Ding, "A Mask Free Neural Network for Monaural Speech Enhancement," in Proc. Interspeech, 2023, pp. 2468–2472.

[6] L. Yang, W. Liu, and W. Wang, "TFPSNet: Time-Frequency Domain Path Scanning Network for Speech Separation," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2022, pp. 6842–6846.

[7] Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim, and S. Watanabe, "TF-GridNet: Integrating Full- and Sub-Band Modeling for Speech Separation," IEEE/ACM Trans. Audio, Speech, Language Process., vol. 31, pp. 3221–3236, 2023.

[8] K. Saijo, G. Wichern, F. G. Germain, Z. Pan, and J. Le Roux, "TF-Locoformer: Transformer with local modeling by convolution for speech separation and enhancement," in Proc. Int. Workshop Acoust. Echo Noise Control (IWAENC), 2024, pp. 205–209.

[9] F. Dang, H. Chen, and P. Zhang, "DPT-FSNet: Dual-Path Transformer Based Full-Band and Sub-Band Fusion Network for Speech Enhancement," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2022, pp. 6857–6861.

[10] R. Cao, S. Abdulatif, and B. Yang, "CMGAN: Conformer-based Metric GAN for Speech Enhancement," in Proc. Interspeech, 2022, pp. 936–940.

[11] Y.-X. Lu, Y. Ai, and Z.-H. Ling, "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra," in Proc. Interspeech, 2023, pp. 3834–3838.

[12] R. Chao, W.-H. Cheng, M. La Quatra, S. M. Siniscalchi, C.-H. H. Yang, S.-W. Fu, and Y. Tsao, "An investigation of incorporating mamba for speech enhancement," arXiv preprint arXiv:2405.06573, 2024.

[13] W. Zhang, K. Saijo, J. weon Jung, C. Li, S. Watanabe, and Y. Qian, "Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement," in Proc. Interspeech, 2024, pp. 1740–1744.

[14] S. M. Hernandez, D. Zhao, S. Ding, A. Bruguier, R. Prabhavalkar, T. N. Sainath, Y. He, and I. McGraw, "Sharing low rank conformer weights for tiny always-on ambient speech recognition models," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP). IEEE, 2023, pp. 1–5.

[15] G. Wei, Z. Duan, S. Li, G. Yang, X. Yu, and J. Li, "Sim-T: Simplify the Transformer Network by Multiplexing Technique for Speech Recognition," arXiv preprint arXiv:2304.04991, 2023.

[16] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser, "Universal transformers," in Proc. Int. Conf. Learn. Represent. (ICLR), 2019.

[17] T. Ge, S.-Q. Chen, and F. Wei, "EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2Seq Generation," in Proc. Conf. Empir. Methods Nat. Lang. Process. (EMNLP), 2022, pp. 10 786–10 798.

[18] K. Shim, J. Lee, and H. Kim, "Leveraging Adapter for Parameter-Efficient ASR Encoder," in Proc. Interspeech, 2024, pp. 2380–2384.

[19] H. Tang, Z. Liu, C. Zeng, and X. Li, "Beyond universal transformer: block reusing with adaptor in transformer for automatic speech recognition," in Proc. Int. Symp. Neural Networks (ISNN), 2024, pp. 69–79.

[20] Y. Wang and J. Li, "Residualtransformer: Residual Low-Rank Learning With Weight-Sharing For Transformer Layers," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2024, pp. 11 161–11 165.

[21] X. Hu, K. Li, W. Zhang, Y. Luo, J.-M. Lemercier, and T. Gerkmann, "Speech separation using an asynchronous fully recurrent convolutional neural network," Adv. Neural Inf. Process. Syst., vol. 34, pp. 22 509–22 522, 2021.

[22] K. Li, R. Yang, and X. Hu, "An efficient encoder-decoder architecture with top-down attention for speech separation," in Proc. Int. Conf. Learn. Represent. (ICLR), 2023.

[23] A. Pandey and D. Wang, "Densely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2020, pp. 6629–6633.

[24] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, "Conformer: Convolution-augmented transformer for speech recognition," in Proc. Interspeech, 2020, pp. 5036–5040.

[25] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, P. Rana, S. Srinivasan, and J. Gehrke, "The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets, Subjective Testing Framework, and Challenge Results," in Proc. Interspeech, 2020, pp. 2492–2496.

[26] C. Valentini-Botinhao, X. Wang, S. Takaki, and J. Yamagishi, "Investigating rnn-based speech enhancement methods for noise-robust text-to-speech," in Proc. SSW, 2016, pp. 146–152.

[27] I. Loshchilov and F. Hutter, "Decoupled Weight Decay Regularization," in Proc. Int. Conf. Learn. Represent. (ICLR), 2019.

[28] Y.-J. Lu, S. Cornell, X. Chang, W. Zhang, C. Li, Z. Ni, Z.-Q. Wang, and S. Watanabe, "Towards Low-Distortion Multi-Channel Speech Enhancement: The ESPNET-Se Submission to the L3DAS22 Challenge," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2022, pp. 9201–9205.

[29] X. Hao, X. Su, R. Horaud, and X. Li, "Fullsubnet: A full-band and sub-band fusion model for real-time single-channel speech enhancement," in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), 2021, pp. 6633–6637.

[30] A. Li, W. Liu, C. Zheng, C. Fan, and X. Li, "Two heads are better than one: A two-stage complex spectral mapping approach for monaural speech enhancement," IEEE/ACM Trans. Audio, Speech, Language Process., vol. 29, pp. 1829–1843, 2021.

[31] A. Li, S. You, G. Yu, C. Zheng, and X. Li, "Taylor, Can You Hear Me Now? A Taylor-Unfolding Framework for Monaural Speech Enhancement," in Proc. IJCAI, 2022, pp. 4193–4200.

[32] W. Zhang, K. Saijo, Z.-Q. Wang, S. Watanabe, and Y. Qian, "Toward Universal Speech Enhancement For Diverse Input Conditions," in Proc. IEEE ASRU, 2023, pp. 1–6.