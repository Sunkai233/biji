# 基于变换器的固定翼无人机容错控制：使用知识蒸馏和上下文自适应

**Francisco Giral**¹，**Ignacio Gomez**¹，**Ricardo Vinuesa**²，**Soledad Le Clainche**¹

**摘要**——本研究提出了一种基于变换器的固定翼无人机(UAV)容错控制方法，旨在实时适应由结构损伤或执行器故障引起的动态变化。与依赖经典控制理论并在动态严重变化下难以应对的传统飞行控制系统(FCS)不同，我们的方法利用变换器的上下文学习和注意力机制，直接将外环参考值——高度、航向和空速——映射为控制指令，从而绕过内环控制器和故障检测层。采用师生知识蒸馏框架，所提出的方法通过从具有完全可观测性的特权专家智能体转移知识，训练一个具有部分观测的学生智能体，在各种故障场景下实现鲁棒性能。实验结果表明，我们基于变换器的控制器在标称条件和极端故障情况下都优于行业标准FCS和最先进的强化学习(RL)方法，保持高跟踪精度和稳定性，突出了其增强无人机运行安全性和可靠性的潜力。视频链接：https://youtu.be/ATW3LZFRqc0

（

#### 使用Transformer（变换器）技术

**什么是Transformer？**

- 原本用于语言翻译（如Google翻译）
- 能够理解上下文，关注重要信息
- 就像一个智能助手，能综合考虑各种因素

**用在无人机上的优势：**

- 能"看懂"当前飞行状态
- 直接从目标指令生成控制命令
- 绕过复杂的中间环节

#### 2. 直接映射控制

**传统方式：**

```
目标高度/航向/速度 → 内环控制器 → 故障检测 → 最终控制指令
```

**新方式：**

```
目标高度/航向/速度 → Transformer → 直接生成控制指令
```

就像从"层层汇报"变成"直达专线"。

#### 3. 师生知识蒸馏框架

**"老师"智能体：**

- 能看到所有飞行数据（全知视角）
- 像经验丰富的飞行教练
- 知道在各种情况下应该怎么操作

**"学生"智能体：**

- 只能看到部分数据（现实限制）
- 像正在学习的飞行员
- 通过模仿"老师"来学习应对各种故障

**学习过程：**

- "老师"示范如何在各种故障下安全飞行
- "学生"观察学习，逐渐掌握应对技巧
- 最终"学生"能在实际飞行中独立应对故障

）

**索引词**——空中机器人、基于学习的控制、强化学习、容错控制。

## I. 引言

近年来，无人机(UAV)被广泛用于在复杂和关键场景中执行各种应用，如搜救或自主医疗运输。由于系统故障的潜在影响，这些空中机器人的运行安全性和可靠性已成为主要关注点。

与操作和人形机器人行走等机器人领域——它们依赖先进的控制方法来管理复杂的关节运动——相比，工业中的无人机飞行控制系统(FCS)通常使用经典控制技术进行低级控制。尽管现代方法如模型预测控制(MPC)为轨迹规划和碰撞避免等高级任务提供了显著优势[1][2]，但它们需要精确的系统模型、广泛的不确定性处理和高计算资源，这通常使它们对低级无人机控制不实用。相反，经典控制技术——如级联比例-积分-微分(PID)控制系统——的简单性、可靠性和效率使它们成为无人机姿态控制的首选，并在PX4和Ardupilot等广泛部署的自动驾驶仪中实现。

然而，复杂的环境和苛刻的任务可能对无人机造成结构损伤，改变其气动特性和动力学。固定翼无人机尤其表现出高度复杂的非线性动力学，如果结构受损，这些动力学可能受到严重干扰。尽管当前的FCS是鲁棒的，但当车辆动力学偏离原始设计规范时，它们难以维持性能，有时会导致控制发散和灾难性故障。

容错飞行控制已成为安全关键型无人机操作的焦点。通常，容错方法依赖故障检测和诊断技术，识别故障，然后调整控制器参数以适应新的动力学[3][4][5][6]。这种方法很复杂，需要在高度非线性动态环境中进行实时故障识别和参数调整。其他研究探索了不需要明确故障识别的新颖非线性容错控制方法。这些包括基于模糊逻辑系统[7]、滑模控制[8]和鲁棒控制技术[9]的方法。

强化学习(RL)为该问题引入了替代解决方案[10][11][12][13]。凭借其处理高维非线性动力学的能力[14]，RL在系统故障管理方面很有前景。然而，RL算法通常设计用于马尔可夫决策过程(MDP)公式，而容错控制——具有突然的、不可观测的动力学变化——必须被构建为部分可观测马尔可夫决策过程(POMDP)，使RL算法学习更加困难，可能导致它们达到次优性能[15]。尽管最近的工作显示了这个问题的可能解决方案[15][16][17]，但这些为算法增加了相当大的复杂性。大多数研究关注多旋翼无人机的执行器故障[4][12]或固定翼无人机在内环姿态控制回路级别的容错，避免高度、航向和空速跟踪[10][11]。

尽管强化学习(RL)和模仿学习(IL)等基于学习的控制方法具有巨大潜力，但与传统控制理论算法相比，使用这些技术确保稳定性更为复杂，这是由于它们内在的非线性和缺乏系统模型。几项研究试图将稳定性保证整合到基于RL和IL的算法中[18][19]。其他基于李雅普诺夫理论的工作专注于学习候选李雅普诺夫函数——通常称为神经李雅普诺夫函数[20][21][22][23]。在参考文献[24]中，作者不是简单地惩罚李雅普诺夫条件的违反而没有用于直接回归的标记数据，而是利用基于Koopman算子理论的数值工具来生成用于训练的明确真实数据。

在这项工作中，我们提出了一种新颖的基于变换器的容错控制方法，直接将无人机姿态控制外环中的参考点——高度(h)、航向(Ψ)和空速(VT)——映射为控制面和节流阀指令，实现完整的无人机控制，无需内控制环，这简化了系统。我们的方法采用变换器模型的注意力机制和上下文学习能力来在推理期间适应控制动作的动态变化，从而消除了对故障检测或识别以及飞行控制系统(FCS)中参数调整的需要。我们基于变换器的控制器使用过去无人机状态的上下文窗口来自主检测和适应动态变化，类似于参考文献[25]中描述的思想。此外，我们将基于李雅普诺夫的概念纳入离线RL训练过程，以鼓励稳定的模型行为。图1显示了我们提出的基于变换器的控制器(蓝色轨迹)与行业标准FCS(红色轨迹)的比较。在图1a和图1b中，两个系统在标称条件下跟踪指令参考。图1c和1d说明了当无人机经历半翼损伤时的响应，显示FCS失去控制并导致坠毁，而我们的方法稳定无人机并跟随参考值。

本工作的贡献如下：

- 我们提出了一种基于师生知识蒸馏的新颖学习框架，用于学习固定翼无人机的容错策略。在这个框架中，教师智能体使用RL在特权环境信息上训练以解决部分可观测性限制，然后学生智能体使用教师与环境的交互在没有特权信息的情况下训练。

- 我们设计了一个基于变换器的飞行控制器，利用上下文学习随着无人机动力学因故障而改变实时适应其行为。该控制器采用过去状态的上下文来确定动作，消除了对故障检测方法的需要。此外，包含基于李雅普诺夫的概念以鼓励控制器稳定性。

- 我们进行了与最先进方法的比较研究，不仅解决执行器故障，还解决重大结构损伤场景。

（

#### 本研究的创新思路

#### 核心理念

**传统方法：**

```
外环指令 → 内环控制器 → 执行器指令
```

**新方法：**

```
外环指令（高度、航向、速度）→ Transformer → 直接控制指令
```

#### 关键优势

1. **绕过复杂的中间环节**：不需要故障检测和参数调整
2. **利用Transformer的优势**：注意力机制和上下文学习
3. **实时适应**：通过观察过去的状态自动适应动态变化

#### 实际效果对比（图1）

**正常情况：** 两种方法都能正常工作 **故障情况：**

- 传统FCS失控坠毁
- 新方法保持稳定并继续跟踪目标

#### 主要贡献总结

1. **师生知识蒸馏框架**：解决部分可观测问题
2. **基于Transformer的飞行控制器**：实现实时适应
3. **全面的对比研究**：不仅测试执行器故障，还包括结构损伤

）

## II. 相关工作

### A. 空中飞行器的基于学习的容错控制

由于复杂的动力学和有限的可控性，容错飞行控制问题仍然具有挑战性。为解决这一问题进行了许多尝试，数据驱动方法近年来越来越受欢迎。

许多研究关注多旋翼无人机，通常解决执行器故障[6][12][26]。这些工作证明了这个问题的可解性，通过预训练的故障感应网络[6]或RL方法[26]，即使在多个电机故障时也是如此。

对于固定翼无人机，虽然几项研究已将RL方法应用于飞行姿态控制[27][28]，但相对较少专门关注容错控制，这必须解决超出执行器故障的更广泛潜在故障范围。参考文献[10]中提出了使用RL进行低级容错飞行控制的早期探索，作者在内环控制器中采用演员-评论家架构。这种方法代表了基于RL的飞行控制弹性的重要步骤，专注于在较低控制级别维持稳定性。在参考文献[11]中，RL与遗传算法(GA)结合训练智能体群体，提供了一种增加实施复杂性的新颖方法，突出了混合方法的潜力。

### B. 变换器

以其注意力机制和在序列建模方面的优势而闻名的变换器最初应用于自然语言处理(NLP)领域[29]。由于其高质量的全局上下文学习和高效的并行计算，变换器架构已成为传统序列预测方法如循环神经网络(RNN)和长短期记忆网络(LSTM)的强有力替代品。从机器人和控制的角度来看，变换器在捕获长范围依赖关系和快速适应动态系统变化方面优于循环模型，如参考文献[30]所示。在这项工作后期，进行了这些模型之间的性能比较。

随着变换器的进步，它们的应用已扩展到RL[31][32][33]。研究人员在决策智能体中采用了变换器架构，提出了决策变换器(DT)模型，该模型将序列建模问题构建为基于历史状态和奖励递减序列的动作预测问题[31]。以监督方式训练，DT通过使用过去状态和奖励递减的序列寻求最小化预测和真实动作之间的误差。最近的工作表明DT可以超越许多最先进的离线RL算法[31]。在这项工作中，训练DT模型在故障场景中作为无人机的姿态控制器，优于其他最先进的方法。

### C. 知识蒸馏

知识蒸馏是一种旨在将知识从教师模型转移到学生模型的转移学习(TL)方法[34]。知识蒸馏(KD)的原始概念涉及从大型复杂模型(教师)向较小、更简单的模型(学生)转移知识[35]。这种转移通常通过最小化教师和学生输出之间的Kullback-Leibler(KL)散度来实现。

最近的工作将TL与记忆组件结合以解决部分可观测性。在参考文献[36]中，这种组合用于训练学生策略基于部分视觉观察踢足球。在参考文献[37]中，多个教师被蒸馏成一个多任务学生。参考文献[25]和[30]中的作者将知识蒸馏到基于变换器的学生策略中，以管理操作任务和人形运动中的部分可观测性。相比之下，在这项工作中，来自特权智能体的专家知识以非特权方式蒸馏到DT学生中，使其能够实时适应其行为以应对影响无人机动力学的故障。

（

#### 知识蒸馏（Knowledge Distillation）

##### 基本概念

**师生关系：**

- **老师模型**：复杂、强大，但"笨重"
- **学生模型**：简单、高效，但需要学习

**蒸馏过程：** 就像一个博士生导师把多年的研究经验传授给研究生

#### 在机器人控制中的应用

**部分可观测性问题：**

- **老师**：能看到所有信息（如无人机内部传感器数据）
- **学生**：只能看到有限信息（如外部可观测的状态）
- **目标**：让学生在信息有限的情况下也能做出正确决策

）

## III. 问题陈述

在这项工作中，我们旨在解决固定翼无人机的容错控制问题，其中，除了在故障后仅仅避免坠毁外，车辆仍能跟踪给定的参考值。在飞行姿态控制的高层次上，我们考虑跟踪高度(href)、航向(Ψref)和空速(VTref)中指定设定点的问题。

我们考虑显著改变无人机气动特性的故障，例如损坏的控制面或损坏的机翼。这些故障主要影响稳定性和控制导数，导致车辆动力学的实质性变化。稳定性和控制导数定义作用在无人机上的力和力矩，通常表示为：

$$C_L = C_{L_0} + C_{L_\alpha} \alpha + C_{L_q} \frac{qc}{2V} + C_{L_{\delta_e}} \delta_e$$ (1)

$$C_D = C_{D_0} + kC_L^2 + C_{D_{\delta_e}} \delta_e$$ (2)

$$C_Y = C_{Y_\beta} \beta + C_{Y_p} \frac{pb}{2V} + C_{Y_r} \frac{rb}{2V} + C_{Y_{\delta_a}} \delta_a + C_{Y_{\delta_r}} \delta_r$$ (3)

$$C_l = C_{l_0} + C_{l_\beta} \beta + C_{l_p} \frac{pb}{2V} + C_{l_r} \frac{rb}{2V} + C_{l_{\delta_a}} \delta_a + C_{l_{\delta_r}} \delta_r$$ (4)

$$C_m = C_{m_0} + C_{m_\alpha} \alpha + C_{m_q} \frac{qc}{2V} + C_{m_{\delta_e}} \delta_e$$ (5)

$$C_n = C_{n_0} + C_{n_\beta} \beta + C_{n_p} \frac{pb}{2V} + C_{n_r} \frac{rb}{2V} + C_{n_{\delta_a}} \delta_a + C_{n_{\delta_r}} \delta_r$$ (6)

这些方程定义了气动系数CL、CD、CY、Cl、Cm和Cn，分别表示升力、阻力、侧向力、滚转力矩、俯仰力矩和偏航力矩系数[38][39]。每个方程都受到无人机动态变量(例如，攻角α、侧滑角β、俯仰角速度q、滚转角速度p、偏航角速度r)和控制面偏转(例如，升降舵δe、副翼δa、方向舵δr)的影响。在这些表达式中，c指平均气动弦长，b是无人机的翼展，k是与诱导阻力相关的因子，V是无人机的空速。系数是稳定性和控制导数的函数，这些导数受到无人机气动配置的显著影响，在故障或损伤情况下可能发生重大改变。

从稳定性和控制导数得出的气动力和力矩决定状态变量的时间演化，通常表示为非线性、高度耦合的动力学系统，$\dot{x} = f(x,u)$。

当前最先进的控制算法无法在故障发生后适应车辆动力学的变化。因此，无人机的故障和损伤显著影响现有飞行控制律的性能，有时导致灾难性后果[40]。

为了克服这些限制，我们提出了一个基于学习的控制器，利用变换器模型的力量。这个新模型可以使用无人机状态变量的历史来在上下文中适应其行为，而不更新其权重。

{

#### 飞机飞行的数学原理

#### 气动系数方程组（方程1-6）

这些复杂的数学公式描述了飞机如何飞行，我用简单的话解释：

**升力系数方程（方程1）：**

- 描述飞机向上的力量
- 受攻角、俯仰速度、升降舵角度影响
- 就像纸飞机，角度不同，升力不同

**阻力系数方程（方程2）：**

- 描述空气阻力
- 与升力相关，升力越大通常阻力也越大

**侧向力系数方程（方程3）：**

- 描述左右方向的力
- 受侧滑角、滚转和偏航速度、副翼和方向舵影响

**力矩系数方程（方程4-6）：**

- 滚转力矩：控制飞机左右倾斜
- 俯仰力矩：控制飞机抬头或低头
- 偏航力矩：控制飞机左转或右转

#### 关键参数说明

**飞行姿态参数：**

- α（攻角）：机头相对于飞行方向的仰角
- β（侧滑角）：飞机侧向滑行的角度
- p, q, r：三个轴向的角速度

**控制面参数：**

- δe（升降舵）：控制俯仰
- δa（副翼）：控制滚转
- δr（方向舵）：控制偏航

**物理参数：**

- c（平均气动弦长）：机翼的平均宽度
- b（翼展）：机翼的跨度
- V（空速）：飞行速度



#### 基于Transformer的创新方法

**核心思想：** 不依赖固定的数学模型，而是：

- 观察无人机最近的飞行历史
- 根据历史数据推断当前的动力学特性
- 实时调整控制策略

}

## IV. 通过序列建模的容错飞行控制

我们将故障条件下的飞行控制问题表述为部分可观测马尔可夫决策过程(POMDP)。POMDP由元组⟨A,S,O,P,R⟩表示，包含动作a∈A、状态s∈S、观测o∈O、转移函数p∈P和奖励r∈R。在每个时间步t，RL智能体接收观测ot并生成动作at。然后环境根据转移函数p(st+1|st,at)更新并向智能体分配奖励。

智能体的目标是最大化累积折扣奖励$R_t = \sum_{t=k}^H \gamma^t r_t$，其中H、k和γ分别是地平线长度、当前时间步和折扣因子。

常见的POMDP任务涉及观测仅部分反映底层状态、不同状态可能看起来相同或观测受随机噪声影响的挑战。RL中POMDP公式内的关键子领域包括元强化学习、鲁棒RL和RL中的泛化[15]。在这些子领域中，智能体被设计为通过学习自适应行为来处理动力学变化和未见的测试环境。例如，元RL解决具有情节特定动力学变化的任务，而鲁棒RL专注于具有对抗性扰动的环境，RL中的泛化强调对分布外状态的弹性[15]。在损伤和故障下的无人机控制上下文中，问题与这些子领域一致，因为它涉及不直接可观测的动力学中的突然、实质性变化，需要控制策略在没有明确改变动力学知识的情况下适应。

{

#### 问题的数学框架：POMDP

#### 什么是POMDP？

**完整名称：** 部分可观测马尔可夫决策过程 (Partially Observable Markov Decision Process)

**核心概念：** 想象你在玩一个游戏，但你看不到游戏的全部状态，只能看到部分信息。

#### POMDP的五个组成部分 ⟨A,S,O,P,R⟩

**A (Actions 动作集合)：**

- 无人机可以执行的所有控制动作
- 如：调整升降舵角度、副翼角度、方向舵角度、油门等

**S (States 状态集合)：**

- 无人机的真实完整状态
- 包括：位置、速度、姿态、以及**故障信息**（关键！）
- 问题是：我们不能完全观测到这些状态

**O (Observations 观测集合)：**

- 我们实际能看到的信息
- 如：GPS位置、速度传感器读数、姿态传感器数据
- **重要：** 我们看不到具体的故障类型和程度

**P (Transition Function 转移函数)：**

- 描述"当前状态 + 动作 → 下一个状态"的规律
- 当无人机有故障时，这个规律会改变

**R (Reward 奖励)：**

- 评价动作好坏的标准
- 如：跟踪目标轨迹得正分，偏离轨迹扣分

#### 智能体的工作流程

```
时间t: 观测ot → 选择动作at → 环境更新 → 获得奖励rt → 时间t+1
```

**目标：** 最大化累积折扣奖励

```
Rt = rt + γrt+1 + γ²rt+2 + ...
```

其中γ是折扣因子（未来奖励的权重）

}

### A. 通过在线强化学习的特权智能体训练

RL智能体的主要目标是最小化允许无人机在空间中完全控制的三个变量——高度(h)、航向(Ψ)和空速(VT)——的实际值与指令设定点之间的误差。

智能体采取行动使用控制面——副翼(δa)、升降舵(δe)和方向舵(δr)——以及节流阀(δT)来修改无人机的动力学。这些动作基于一组观测，o∈O，包括跟踪误差(εh、εΨ、εVT)、高度(h)、气动角(α、β)、姿态角(φ、θ)、角速度(p、q、r)、线速度(u、v、w)和线加速度(nx、ny、nz)。这些观测定义了无人机的姿态。

奖励函数设计为跟踪参考值，同时保持平滑的姿态调整并最小化控制努力。总奖励计算为这些组件的加权和：

$$r = \lambda_1 \cdot r_h + \lambda_2 \cdot r_\Psi + \lambda_3 \cdot r_{V_T} + \lambda_4 \cdot r_{att} + \lambda_5 \cdot r_\delta$$ (7)

其中rh、rΨ和rVT是跟踪奖励，ratt鼓励平滑姿态，rδ惩罚大控制输入。权重λ1、λ2、λ3、λ4、λ5分别设置为0.24、0.2、0.16、0.2、0.2。

跟踪奖励组件采用形式：

$$r_x = -1 + \exp\left(-k \cdot \left|\frac{\varepsilon_x}{\lambda_s}\right|\right)$$ (8)

其中x表示h、Ψ或VT，k和λs是形状参数。所有奖励权重和形状参数最初基于领域知识选择，并通过试错进一步调整以达到期望的性能。

姿态奖励使用角速度(p、q、r)和滚转角(φ)计算：

$$r_{att} = -1 + (\hat{r}_p \cdot \hat{r}_q \cdot \hat{r}_r \cdot \hat{r}_\phi)^{1/4}$$ (9)

而控制奖励使用控制指令相对于前一时间步的偏差：

$$r_\delta = -1 + (\hat{r}_{\Delta\delta_a} \cdot \hat{r}_{\Delta\delta_e} \cdot \hat{r}_{\Delta\delta_r} \cdot \hat{r}_{\Delta\delta_T})^{1/4}$$ (10)

对于表达式(9)–(10)，$\hat{r}_x$采用形式：

$$\hat{r}_x = \exp\left(-\left(\frac{x}{\lambda_s}\right)^2\right)$$ (11)

其中$x \in [p,q,r,\phi,\Delta\delta_a,\Delta\delta_e,\Delta\delta_r,\Delta\delta_T]$。

为了解决部分可观测性，训练了一个特权RL智能体。通过将表征环境的物理参数添加到智能体的观测中，POMDP问题转换为经典MDP，产生更高效和鲁棒的智能体。如图2所示，领域随机化(DR)应用于环境的物理参数，在每个情节的开始和整个过程中改变无人机的动力学。通过将智能体的部分观测(ot)与每个时间步的环境物理参数结合，形成完整状态(st)，智能体学习在受气动、重心和重量变化影响的各种无人机配置和动力学中操作。

修改的物理参数及其随机化范围如表I所示。每个范围基于无人机故障或损伤引起的已知物理变换选择。在训练期间，初始和参考飞行条件(h、VT)也在无人机的飞行包线内随机化。

特权智能体使用DreamerV3算法[16]训练，选择该算法是因为其效率、鲁棒性以及在多个环境中泛化而无需大量超参数调整的能力。DreamerV3是一种在线基于模型的RL算法，从过去的轨迹学习环境的世界模型。

**表I：领域随机化范围**

| 参数                              | 随机化范围          |
| --------------------------------- | ------------------- |
| CLα, CLq̇, CLα̇                     | 缩放 U(0.6,1.0)     |
| CD0, CDi                          | 缩放 U(1.0,3.0)     |
| CDmach                            | 缩放 U(1.0,2.0)     |
| CDβ                               | 缩放 U(1.0,3.0)     |
| CYβ                               | 缩放 U(0.5,3.0)     |
| CYṗ, Clβ, Clp, Clr, Cnα, Cnβ, Cnṗ | 缩放 U(0.5,3.0)     |
| CYṙ                               | 缩放 U(0.5,2.0)     |
| Cm0                               | 相加 U(-0.02,0.001) |
| Cmα                               | 缩放 U(0.5,1.5)     |
| Cmq                               | 缩放 U(1.0,3.0)     |
| Cmα̇                               | 缩放 U(1.0,1.5)     |
| Cl0, Cn0                          | 相加 U(-0.02,0.009) |
| Cnr                               | 缩放 U(0.5,1.0)     |
| CLδe, CDδe, Cmδe, Clδa, Cnδa      | 缩放 U(0.5,1.0)     |
| Clδr, Cnδr, CYδr                  | 缩放 U(0.0,1.0)     |
| xCG, yCG, zCG                     | 缩放 U(0.70,1.30)   |
| W                                 | 缩放 U(0.70,1.05)   |

训练期间环境中物理参数的修改范围——稳定性和控制导数、重心位置和重量。值在情节中通过从均匀分布采样周期性改变，每个参数的限制基于领域知识选择。

{

我来用通俗的语言解释这个特权智能体训练的部分：

#### A. 特权智能体的训练目标

#### 控制目标

**三个核心任务：**

- **高度控制(h)**：保持或达到指定高度
- **航向控制(Ψ)**：保持或调整到指定方向
- **空速控制(VT)**：保持或调整到指定速度

就像开车时要控制车速、方向和车道位置一样。

#### 控制手段

**四个控制面：**

- **副翼(δa)**：控制左右倾斜（滚转）
- **升降舵(δe)**：控制抬头低头（俯仰）
- **方向舵(δr)**：控制左右转向（偏航）
- **节流阀(δT)**：控制发动机功率

#### 观测信息

**智能体能"看到"的信息：**

- **跟踪误差**：距离目标还有多远
- **位置信息**：当前高度
- **姿态信息**：飞机的倾斜和转向角度
- **运动信息**：各种速度和加速度

#### 奖励函数设计

#### 总体奖励公式（方程7）

```
总奖励 = λ1×高度奖励 + λ2×航向奖励 + λ3×速度奖励 + λ4×姿态奖励 + λ5×控制奖励
```

**权重分配：**

- 高度：0.24（最重要）
- 航向：0.2
- 速度：0.16
- 姿态：0.2
- 控制：0.2

#### 跟踪奖励的计算（方程8）

**形式：** `奖励 = -1 + exp(-k × |误差/λs|)`

**工作原理：**

- 误差为0时：奖励接近0（最好）
- 误差越大：奖励越接近-1（越差）
- 就像考试，满分100分，答错越多分数越低

#### 姿态奖励（方程9）

**目标：** 鼓励平滑飞行，避免剧烈摇摆

- 监控三个角速度（p, q, r）和滚转角（φ）
- 如果飞机晃动太厉害就扣分
- 就像开车要求平稳，不能左摇右摆

#### 控制奖励（方程10）

**目标：** 避免控制指令突变

- 比较当前控制指令与上一时刻的差异
- 变化太大就扣分
- 就像开车不能猛打方向盘，要平稳操作

#### 特权信息的使用

#### 什么是"特权"？

**特权智能体能看到：**

- 正常的飞行传感器数据
- **加上：** 环境的真实物理参数（故障信息）

**类比：**

- 普通飞行员：只能看仪表盘
- 特权飞行员：还有X光眼，能看到飞机内部哪里坏了

#### 解决部分可观测性

**POMDP → MDP转换：**

- **原来：** 不知道具体故障情况（部分可观测）
- **现在：** 知道所有物理参数（完全可观测）
- **结果：** 训练更容易，性能更好

#### 领域随机化（Domain Randomization）

#### 基本思路

**训练策略：**

- 不是在固定环境中训练
- 而是在千变万化的环境中训练
- 每次训练都改变飞机的物理特性

**类比：**

- 不是只在一种车上学开车
- 而是在各种不同性能的车上都练习
- 这样真正上路时适应性更强

#### 随机化的参数（表I）

**气动参数随机化：**

- **升力系数变化**：模拟机翼损伤
- **阻力系数变化**：模拟形状改变
- **力矩系数变化**：模拟控制面故障

**物理参数随机化：**

- **重心位置变化**：模拟载重分布改变
- **总重量变化**：模拟燃料消耗或载重变化

**控制效果随机化：**

- **控制面效果减弱**：模拟舵面损坏
- **完全失效（0.0-1.0）**：某些控制面可能完全不工作

#### 随机化范围的选择

**基于现实考虑：**

- **轻微损伤**：参数变化50%-150%
- **严重损伤**：参数变化30%-300%
- **完全失效**：某些功能降到0%

#### 训练算法：DreamerV3

#### 选择理由

- **效率高**：学习速度快
- **鲁棒性好**：适应性强
- **泛化能力强**：在不同环境下都能工作
- **调参简单**：不需要复杂的参数调整

#### 基于模型的方法

**工作原理：**

1. 从过去的经验中学习环境模型
2. 在想象中预演各种场景
3. 基于预演结果优化策略
4. 比纯试错方法更高效

#### 实际意义

这个特权智能体就像一个"全能教练"：

- 知道所有可能的故障情况
- 在各种极端条件下都能找到最优控制策略
- 为后续的"普通学生"提供标准答案

通过这种训练，我们得到了一个在完美信息下的最优控制策略，这将成为后续知识蒸馏的"金标准"。

}

### B. 通过决策变换器的知识蒸馏

基于训练的专家特权策略，我们采用离线强化学习通过知识蒸馏训练学生策略，仅使用环境的部分观测(o∈O)。

如图3所示，特权智能体用于收集多个专家轨迹的数据集，包含物理参数和飞行条件的随机变化。从这些轨迹中，我们仅保留部分观测(ot)、采取的动作(at)和奖励(rt)。这个数据集，包含从MDP简化为部分观测POMDP的专家轨迹，用于通过离线RL训练决策变换器(DT)[31]。

决策变换器是能够利用上下文学习在不更新权重的情况下适应行为的强大模型。因此，DT可以学习仅使用部分观测和动作的历史在不同系统动力学中采取最优动作。最终，在多个专家轨迹上训练的DT可以泛化到未见的故障场景，并仅使用过去观测的历史来识别变化而适应其行为到无人机动力学的变化。

最终模型是大小约0.8M的因果仅解码器变换器，包含4个头和3层，嵌入维度为128，最大序列长度为60步(6秒)。训练采用批大小256、学习率1e-04和权重衰减1e-04。模型在200万时间步的数据集上训练，包含200,000条轨迹。

{

#### 知识蒸馏的基本流程

#### 第一步：数据收集

**特权智能体的作用：**

- 就像一个"全知全能的老师"
- 在各种故障场景下飞行，收集经验数据
- 记录每个时刻的：观测数据、采取的动作、获得的奖励

**数据处理：**

- 收集到完整的专家轨迹后
- **关键操作：** 丢弃特权信息（物理参数）
- 只保留普通传感器能获得的信息

**类比：**

- 老师做题时能看到标准答案
- 但教学生时，只给学生题目和老师的解题步骤
- 不告诉学生标准答案是什么

#### 第二步：从MDP到POMDP的转换

**数据转换过程：**

```
原始数据：观测(ot) + 特权信息 + 动作(at) + 奖励(rt)
          ↓
筛选后：仅观测(ot) + 动作(at) + 奖励(rt)
```

**实际意义：**

- 模拟真实飞行条件：飞行员看不到飞机内部故障详情
- 只能根据仪表显示和飞机响应来判断情况

#### 决策变换器（Decision Transformer）的工作原理

#### 核心概念：上下文学习

**传统强化学习：**

- 需要不断试错来更新模型参数
- 遇到新情况需要重新学习

**决策变换器：**

- 不更新模型权重
- 通过观察历史数据来理解当前情况
- 就像经验丰富的飞行员，通过"手感"判断飞机状态

#### 工作机制类比

**人类飞行员的经验：**

1. 新飞行员：每次遇到问题都要查手册
2. 老飞行员：凭经验感觉，"这个响应不对劲，可能是...问题"

**决策变换器：**

1. 观察最近6秒（60步）的飞行历史
2. 分析模式："这种响应模式我见过，应该这样操作"
3. 输出相应的控制指令

#### 适应不同故障的能力

**训练数据的多样性：**

- 包含各种不同的故障场景
- 每种故障下的最优应对策略

**泛化能力：**

- 遇到训练中没见过的故障组合
- 能根据相似的响应模式推断应对策略
- 就像医生虽然没见过某种罕见病，但能根据症状推断治疗方案

#### 模型架构细节

#### 技术参数

**模型规模：** 0.8M参数

- 相对较小，适合实时控制
- 就像一个轻便但高效的"飞行大脑"

**架构设计：**

- **4个注意力头**：同时关注历史数据的不同方面
- **3层变换器**：足够的处理深度
- **嵌入维度128**：信息压缩和表示的平衡

**序列长度：** 60步（6秒历史）

- 足够捕获飞行动态变化
- 不会太长导致计算负担过重

#### 训练配置

**数据规模：**

- 200万个时间步
- 200,000条完整轨迹
- 覆盖各种飞行条件和故障场景

**训练参数：**

- 批大小256：每次处理256个数据样本
- 学习率1e-04：适中的学习速度
- 权重衰减1e-04：防止过拟合

#### 整体流程总结

```
特权专家收集数据 → 去除特权信息 → 训练决策变换器 → 部署到实际飞行
     (老师)           (标准化)        (学生学习)      (独立飞行)
```

#### 实际优势

#### 1. 实时适应能力

- 不需要重新训练模型
- 通过观察历史就能适应新情况
- 响应速度快

#### 2. 鲁棒性强

- 训练数据包含各种极端情况
- 对未见过的故障也有一定应对能力

#### 3. 计算效率高

- 模型相对较小（0.8M参数）
- 适合在无人机的有限计算资源上运行

#### 4. 无需故障诊断

- 不需要明确知道什么故障
- 根据飞行表现自动调整策略

这种方法本质上是把"飞行经验"压缩到了一个紧凑的模型中，让无人机能够像经验丰富的飞行员一样，在遇到异常情况时凭借"直觉"和"经验"快速做出正确反应。

}

### C. 神经李雅普诺夫函数学习

为了鼓励控制策略的稳定行为，我们将神经李雅普诺夫函数Vθ(s)纳入我们的训练过程，遵循参考文献[21][22][24]中开发的思想。受[24]启发，我们使用从轨迹数据即时计算的Koopman特征函数监督李雅普诺夫函数。

利用特权智能体生成的轨迹数据集，并将李雅普诺夫状态考虑为s∈[εh,εΨ,εVT,φ,p,q,r]，我们首先定义可观测量f(s)，选择为状态的L2范数：

$$f(s) = \|s\|_2$$ (12)

与初始状态s0相关的Koopman特征函数然后使用沿轨迹的拉普拉斯平均近似：

$$\phi(s_0) \approx \frac{\sum_{t=0}^{T-1} e^{-\lambda t} f(s_t)}{\sum_{t=0}^{T-1} e^{-\lambda t}}$$ (13)

其中λ > 0是衰减参数，T是限制为60步的序列长度。这种加权平均提供了沿轨迹衰减的度量，捕获李雅普诺夫函数所需的收缩性质。

神经李雅普诺夫函数，由多层感知器(MLP)参数化，训练以近似Koopman特征函数值。MLP包含两个具有256个神经元的隐藏层，后跟ReLU激活，以及具有平方激活函数的输出层。回归损失定义为均方误差(MSE)：

$$L_{MSE} = \frac{1}{N} \sum_{i=1}^N \left(V_\theta(s_0^{(i)}) - \phi(s_0^{(i)})\right)^2$$ (14)

其中N是批次中轨迹的数量。

除了MSE损失，我们还包含辅助损失项来强制李雅普诺夫性质：

- **正定性**：我们强制对所有s≠0有V(s) > 0且V(0) = 0。

- **递减条件**：为了促进稳定性，我们惩罚李雅普诺夫函数沿轨迹不递减的情况。例如，使用铰链损失：

$$L_{dec} = \frac{1}{N} \sum_{i=1}^N \max\{0, V_\theta(s_{t+1}^{(i)}) - V_\theta(s_t^{(i)}) + \varepsilon\}$$ (15)

其中ε > 0是边距。

整体李雅普诺夫损失然后定义为这些项的加权组合：

$$L_{Lyapunov} = L_{MSE} + \alpha L_{dec}$$ (16)

其中α是经验选择的权重因子。学习的李雅普诺夫函数可用于研究训练控制器的稳定性保证。

最后，为了鼓励决策过程中的稳定性，学习的神经李雅普诺夫函数被纳入决策变换器的损失中。决策变换器的最终损失变为：

$$L_{total} = L_{DT} + \gamma L_{Lyapunov}$$ (17)

其中LDT表示原始决策变换器损失，γ是平衡李雅普诺夫损失贡献的超参数。通过最小化Ltotal，决策变换器被引导选择不仅最大化奖励而且引导系统朝向更稳定状态的动作，如递减的Vθ(s)所指示。

## V. 实验

在本节中，我们提出几个实验来验证我们提出方法的有效性。我们首先描述为在特定故障场景而不是随机变化中验证我们方法而设计的测试案例，以及实验的实施细节。其次，我们通过不同案例的比较研究展示我们模型的能力。

### A. 测试案例和实施细节

我们在无人机的几个故障和损伤场景中评估我们的方法。与训练期间使用的随机和独立参数修改不同，测试案例涉及基于特定故障在无人机动力学中产生的变化对气动参数和控制面的修改，为智能体创造未见情况。定义的场景，受参考文献[10][11]启发，是：

- **卡死方向舵**：方向舵卡在15°偏转。

- **损坏副翼**：副翼效果降低50%，由于不对称力导致寄生滚转力矩增加。

- **饱和升降舵**：升降舵偏转限制在最大范围的±5%。

- **展开起落架**：起落架突然展开，导致阻力增加3CDgear，这在训练期间未见。

- **重心偏移**：无人机重心沿x、y和z轴分别偏移+20%、+40%和-30%。

- **损坏水平尾翼**：部分尾翼损伤导致升力减少(0.5CLδe)，阻力增加(1.2CDδe，1.2CD0)，并影响俯仰力矩(0.5Cmδe，Cm0，0.5Cmq，0.5Cmα，0.5Cmα̇)。

- **损坏半翼**：损坏机翼导致升力效果减少(0.7CLα，0.7CLq，0.7CLα̇)，阻力增加(1.2CD0，1.2CDi)，并影响侧向力(1.5CYβ，1.5CYp，1.5CYr)。滚转力矩改变(Cl0，-1.0Clβ，-1.0Clr，0.5Clδa)，偏航力矩修改(Cn0，0.8Cnr，1.5Cnβ，1.5Cnδa，1.2Cnα)。

在验证期间，运行多个情节，其中无人机的动力学在指定时间步从标称条件改变为每个故障场景定义的修改。(h，Ψ，VT)的设定点变化也在指定时间步发生。

实验和训练使用JSBSim进行，这是一个高保真6自由度飞行动力学模型。使用的无人机物理模型在h∈[4000,24000]英尺和VT∈[260,360]英尺/秒的飞行条件内操作。情节的最大长度为1000步。为了考虑现实世界的不确定性，我们在训练和测试期间都纳入了外部扰动和传感器噪声。风阵按照高斯分布N(0, 2英尺/秒)应用，引入随机扰动。此外，观测噪声——由高斯分布N(0, 0.02x)建模——被注入传感器测量以模拟现实世界的不准确性，确保教师和学生模型的鲁棒性。这些措施有助于弥合仿真到现实(sim-to-real)的差距，改善对真实无人机条件的泛化，并为未来的现实世界验证铺平道路。

### B. 比较研究

我们将我们提出的方法与几个最先进的飞行控制系统(FCS)进行评估，以展示我们方法在容错飞行控制中的优势。

- **特权策略**：用环境特权信息训练的教师策略。

- **RL策略**：在没有环境变化的情况下训练的RL策略，假设MDP。此策略在POMDP设置中测试并使用DreamerV3训练。

- **RL策略 + DR**：通过领域随机化在环境变化下训练的RL策略，将设置视为POMDP。

- **工业FCS**：遵循常见工业实践的基线FCS，使用嵌套PID控制器对横向-方向和纵向运动进行单独控制。

- **工业FCS + RL + DR**：训练在每步选择FCS的PID增益的RL策略，遵循容错控制的增益调度(GS)方法。策略在具有领域随机化的POMDP设置中训练。

为了评估DT在POMDP条件下的有效性，我们比较DT学生智能体和特权教师智能体的结果。两个策略在所有场景中测试100个情节，飞行条件和随机种子标准化以进行公平比较。图5显示结果，使用平均情节回报作为性能指标，其中理论最大值为零。结果显示智能体之间的相似性能，表明DT在POMDP设置下从专家数据集学习行为并适应上下文中动态变化的能力。学生模型尽管观测有限但能匹配或超越教师的能力源于离线强化学习(RL)的本质。学生不是直接模仿教师，而是从专家轨迹中提取基本决策模式，优先考虑关键上下文线索以实现鲁棒适应。这与参考文献[31]中的发现一致，其中DT通过使用回报条件在不确定性下更有效地泛化，证明了超越其训练数据的能力。尽管有特权信息，教师可能遭受不稳定训练，而在多样化重播数据上训练的学生学习了一种更适应的策略来处理未见故障。此外，将基于李雅普诺夫的损失项纳入离线RL训练进一步鼓励DT学生选择导致更稳定状态的动作，从而改善整体回报。

虽然变换器需要比传统序列模型更高的计算资源，但它们的并行化可能实现高效推理，使它们在适当硬件中对实时无人机控制可行。我们的模型保持轻量级，确保实时可行性，如参考文献[41][42][43][44]中类似大小模型所示。此外，应该在计算开销和性能之间进行权衡。如表III所示，我们的DT比LSTM和时间卷积网络(TCN)实现显著更低的跟踪误差，特别是在故障场景中。这些结果验证了基于变换器的控制在精度和鲁棒性方面的优越性。

安全关键应用中基于变换器的控制系统的一个关键问题是它们的黑盒性质。为了提高可解释性，我们分析决策变换器(DT)的注意力机制，以了解它如何识别故障并适应其控制策略。图6显示故障发生前后在60步上下文窗口上的注意力模式。在标称情况下(图6a)，注意力均匀分布在过去状态上。然而，在时间步100发生故障后(当前时间步为140)，DT通过更多关注故障后的时间步(图6b)来适应，表明基于最近动力学的控制策略变化。这种适应机制通过动态加权相关信息以响应无人机行为的突然变化来增强鲁棒性。

如表II所示的全面比较研究评估DT与其他算法的性能，包括最先进的在线RL智能体和商业无人机中常用的行业标准FCS。性能通过情节回报的均值和标准差以及由于完全控制失败导致的坠毁百分比来衡量。致命故障终止情节，在角速度或加速度极值下发生，或当无人机违反较低高度限制时。如前所述，每个故障场景运行100个情节。表II中的结果显示DT策略在所有故障和标称场景中优于其他方法，有效跟踪设定点并即使在严重损伤和极端条件下也保持控制。

具有领域随机化的RL智能体也获得了有希望的结果，显示高跟踪性能并在故障场景中实现零坠毁。这些结果突出了具有增加记忆的基于模型的RL算法(如DreamerV3)的潜力。然而，尽管DT智能体的参数大小(约0.8M)比DreamerV3(约26M)显著更小，但DT智能体的优越性能突出了基于变换器的模型通过上下文学习进行实时动力学适应的潜力。

如所述，当前使用的FCS由于其简单性、可靠性、模块化和计算效率而难以替换，同时避免了对准确无人机模型的需要并处理不确定性。然而，它们缺乏对重大动态变化的适应性。向FCS添加增益调度(GS)算法可以通过实时修改增益来增强故障条件下的系统适应性；然而，GS面临与计算需求和非线性动力学相关的挑战。即使为容错训练的最先进RL算法也难以在各种场景中实现高性能，如表II所示。

我们的方法利用变换器的上下文学习能力在飞行期间适应策略行为而不改变权重[30]，而不是修改控制器增益或使用在线重新参数化。图4展示了我们方法与比较研究中其他算法在跟踪h、Ψ和VT的标称和损坏机翼场景中的样本轨迹。在图4a中，显示了标称无人机动力学，其中所有设计的控制器以类似高性能跟踪参考值，在测量空速中由于轻微风阵有一些噪声。

图4b显示了损坏机翼场景的轨迹，由于重大动态变化，这是最具挑战性的。在此样本中，故障在时间步100发生(由垂直虚线红线指示)，设定点变化要求在时间步200发生。虽然基线FCS、基础RL智能体和带有增加GS的FCS(FCS+RL)发散，失去控制并导致坠毁和情节终止(由极端加速度或进入尾旋触发)，但具有DR训练的RL智能体(RL+DR)和我们的DT方法重新获得控制并尽管重大损伤有效跟踪设定点。在这些中，DT方法对所有三个参考值实现最高跟踪精度，显示优越性能。

图7说明了我们DT方法和基线FCS在损坏机翼场景中的角速度(p、q、r)。垂直红线标记故障开始，突出显示在滚转速率(p)中引起的突然扰动。在此点之后，控制器必须重新获得无人机控制以重新稳定到以前的飞行条件。在此情况下，我们的DT智能体成功稳定无人机，尽管气动和控制面损坏，收敛到先前的稳定飞行状态。相比之下，基线工业设计的FCS由于随时间累积误差开始发散，最终导致完全控制失败和失速。

### C. 稳定性分析

在部分可观测性下并缺乏明确的基于模型的设计，保证稳定性的传统方法在理论上可能不可行。然而，遵循参考文献[20][21][22][18][24]中的发现，我们可以使用学习的神经李雅普诺夫函数来获得稳定性和安全性的部分保证。因为学习的函数满足李雅普诺夫函数的性质，驱动系统朝向稳定的控制器必须在整个情节中表现出递减的李雅普诺夫函数Vθ(s)，如果系统在s∈[εh,εΨ,εVT,φ,p,q,r] = 0̂处完全稳定，最终达到零。

为了进一步检查在决策变换器(DT)训练中包含李雅普诺夫损失项LLyapunov的影响，我们评估基线DT和李雅普诺夫增强DT的基于李雅普诺夫的稳定性。我们在标称和故障场景(结合不同类型故障)下对每个模型进行100次实验。

图8展示了神经李雅普诺夫函数Vθ(s)在情节中时间步上的均值和标准差——提供系统中"能量"的度量。如果李雅普诺夫能量趋于零，表明系统正在收敛到稳定平衡状态。在图8a中，我们观察到在标称条件下，两个DT模型(有和没有李雅普诺夫训练)都显示渐近稳定性，收敛到最小能量。时间步200的峰值源于高度、航向和空速(h，Ψ，VT)参考值的故意变化；在此扰动之后，两个模型都稳定到稳定状态。

图8b显示故障场景。故障在时间步100发生，导致能量激增，两个模型都快速稳定。然而，一旦在时间步200引入新参考值，李雅普诺夫增强DT收敛到接近零的能量，而基线DT的能量逐渐增加直到情节结束。这种行为表明将基于李雅普诺夫的思想纳入DT训练可以在推理时促进稳定性。

### D. 讨论

虽然已经提出了几种非线性容错控制方法——如基于模糊逻辑[7]和滑模控制[8]的方法——但它们在现实世界应用中的采用仍然有限。主要原因之一是这些方法通常需要复杂、计算密集的例程来持续监控系统动力学并在线调整控制律。这种计算开销对嵌入式平台是关键缺点，其中处理能力和内存受到严格约束。

相比之下，在PX4和Ardupilot等广泛使用的自动驾驶系统中采用的传统级联PID控制器为低级姿态控制提供了更简单和计算高效的解决方案。它们的分层结构最小化了对在线计算和参数重新调整的需求，使它们特别适合基于微控制器的实现。然而，级联PID的简单性也限制了它们在无人机动力学显著偏离标称模型时的性能。

我们的方法通过将繁重的计算负担转移到离线训练阶段来解决这些计算约束。生成的决策变换器(DT)模型，仅有约0.8M参数，设计用于高效实时推理。值得注意的是，最近的研究表明，类似大小的变换器模型可以通过激进的量化技术成功部署在微控制器——如ARM Cortex-M设备——上，这些技术减少了内存占用和计算负载[41][42][43][44]。通过量化(例如，将权重和激活转换为8位表示)，DT模型的推理可以优化以满足机载飞行控制器施加的实时约束。

通过消除对在线重新参数化、权重适应或操作期间明确故障识别的需要，我们的DT模型规避了许多非线性容错控制方法固有的延迟和计算复杂性。这使其成为鲁棒无人机控制的有希望替代方案，有效弥合了先进基于学习策略与嵌入式系统严格硬件限制之间的差距。

## VI. 结论

在本文中，我们介绍了一种基于变换器的固定翼无人机容错飞行控制框架，绕过经典内环控制器，大幅降低对系统故障实时适应的复杂性。通过将师生知识蒸馏与在离线RL设置中训练的决策变换器结合，我们证明了部分可观测性可以有效缓解：学生仅从过去状态历史学习鲁棒策略，无需明确故障检测或在线权重适应。

我们的实验显示所提出的控制器在标称和故障场景中都实现了优越性能。与行业标准基于PID的FCS和其他最先进RL智能体相比，变换器策略表现出增强的跟踪精度和降低的坠毁率，即使在严重气动破坏如机翼和尾翼损伤下也是如此。通过其注意力机制，决策变换器重新分配对故障后观测的关注，从而在上下文中适应其控制动作。这种设计也规避了对实时重新参数化或调度增益更新的需要，为传统容错方法提供了简化的替代方案。

通过将基于李雅普诺夫的约束纳入离线训练过程，决策变换器被鼓励探索促进整体系统稳定性的动作，在极端条件下进一步提高安全性。生成的轻量级模型——包含少于一百万参数——可以在嵌入式处理器上高效部署，特别是与量化结合时。因此，我们的结果突出了变换器架构在无人机控制中统一实时适应、容错和计算可行性的潜力。

未来工作将专注于硬件在环和真实飞行实验，以在实际约束(如有限机载计算、传感器噪声和严格飞行包线要求)下验证这些有希望的仿真结果。总体而言，这项工作作为朝向可靠、自适应无人机操作的一步，将经典控制的鲁棒性与现代深度学习方法的灵活性相结合，为更安全和更多样化的空中机器人系统铺平道路。

## VII. 致谢

我们感谢Miguel Gomez-López和Rodney Rodríguez Robles在控制理论方面的有用讨论。

S.L.C.感谢由MCIU/AEI/10.13039/501100011033/FEDER, UE资助的PID2023-147790OB-I00项目。S.L.C.还感谢MODELAIR项目，该项目已从欧盟地平线欧洲研究和创新计划中获得资金，根据Marie Sklodowska-Curie资助协议No. 101072559。此出版物的结果仅反映作者的观点，不一定反映欧盟的观点。欧盟不能对此负责。

R.V.感谢ERC资助No.2021-CoG-101043998, DEEPCONTROL的财政支持。然而，表达的观点和意见仅是作者的，不一定反映欧盟或欧洲研究理事会的观点。欧盟和资助机构都不能对此负责。

