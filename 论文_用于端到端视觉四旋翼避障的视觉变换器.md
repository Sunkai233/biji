# 用于端到端视觉四旋翼避障的视觉变换器

**Anish Bhattacharya**∗，**Nishanth Rao**∗，**Dhruv Parikh**∗，**Pratik Kunapuli**，**Yuwei Wu**，**Yuezhan Tao**，**Nikolai Matni**，**Vijay Kumar**

**摘要** — 我们展示了基于注意力机制的端到端方法在密集、杂乱环境中进行高速视觉四旋翼避障的能力，并与各种最先进的学习架构进行了比较。四旋翼无人机(UAVs)在高速飞行时具有巨大的机动性；然而，随着飞行速度的增加，通过独立的感知、建图、规划和控制模块进行导航的传统基于模型的方法由于传感器噪声增加、误差积累和处理延迟增加而失效。因此，基于学习的端到端视觉到控制网络显示出在杂乱环境中在线控制这些快速机器人的巨大潜力。我们在高保真仿真中训练和比较了卷积、U-Net和循环架构与视觉变换器(ViT)模型进行深度图像到控制，观察到随着四旋翼速度增加以及在泛化到未见环境方面，ViT模型比其他模型更有效，而循环性的加入进一步提高了性能，同时降低了所有测试飞行速度下的四旋翼能耗。我们评估了仿真和硬件中高达7m/s速度的性能。据我们所知，这是第一个利用视觉变换器进行端到端视觉四旋翼控制的工作。

(

这篇研究主要讲的是**让无人机（四旋翼）在复杂的环境里以高速飞行并且能自动躲避障碍物**。传统的方法通常分成几步：先用传感器“看见”环境（感知），再建立地图，然后规划一条安全路线，最后控制无人机飞行。但当速度很快时，这些步骤容易出问题：

- **传感器噪声**变大（看不清楚），
- **误差不断累积**（路径越来越偏），
- **计算延迟**变长（反应不够快）。

结果就是：传统方法跟不上高速飞行。

于是研究者采用了**端到端的学习方法**：直接把摄像头看到的图像输入神经网络，网络直接输出控制指令（比如往哪飞，怎么转）。这样省去了中间环节，能更快、更稳。

他们比较了不同的网络结构：

- **卷积神经网络（CNN）**：最常见的图像网络。
- **U-Net**：一种常用的图像分割网络。
- **循环神经网络（RNN）**：能记住过去信息。
- **视觉Transformer（ViT）**：近年来特别火的模型，特别擅长从图像里提取全局信息。

结果发现：

1. **ViT 在速度越快、遇到新环境时表现更好**。
2. **把循环（记忆）结合进去后，性能更强，还能让无人机在所有速度下更省电**。
3. 在仿真和真实硬件测试中，无人机能在**最高 7 米/秒**的速度下安全飞行。

最后，这也是**第一篇把视觉Transformer应用到无人机端到端控制上的研究**。

)

## I. 引言

四旋翼无人机(UAVs)是小型、敏捷的飞行器，能够以高加速度和敏捷性飞行。随着机载传感和计算的小型化，现在通常将四旋翼视为一个单一、独立的机器人，它感知环境、执行高层规划、计算低层控制指令，然后使用机载计算的状态估计跟踪这些指令。在最近的工作之前，该领域的大多数研究都采用传统的机器人流水线，即独立的模块化感知、规划和控制块，每个都有一些计算需求和误差容忍度。然而，四旋翼在高速飞行时达到最高效用，从而在有限的电池寿命下实现更多覆盖；在这些速度下，通过模块化方法的感知到规划可能会失效[1]。最近的工作探索了使用学习策略开发单一的感知到规划算法，该算法输入到跟踪控制器；这具有快速反应时间，当学习模型使用域随机化技术训练时，能够泛化到传感器噪声和真实世界的伪影[2]。相比之下，我们探索了一种完全端到端的方法，将更多的跟踪任务封装到视觉变换器(ViT)模型中。

我们旨在探索基于注意力的学习模型，特别是视觉变换器架构的效用，这些架构以深度图像作为输入，用于高速避障任务的四旋翼端到端控制。我们采用ICRA 2022 DodgeDrone挑战赛：基于视觉的敏捷无人机飞行[3]的任务和仿真器，展示了各个团队开发四旋翼在杂乱仿真环境中飞行避障策略的努力。与ViT架构相比，我们选择了代表不同用例的一系列学习骨干作为基线：卷积、U-Net和循环网络。虽然这不是当今所有流行学习架构的详尽列表，但我们相信这涵盖了通常用于基于视觉的控制相关任务的广泛模型类型，包括目标检测、图像分割和时序建模。

卷积网络[4, 5]的有利特性推动了基于视觉的无人机控制的大部分工作。这些网络能够学习特征图，当使用相对于期望目标函数计算的梯度训练时，优于手工设计的特征图，并且具有平移不变性。然而，当用于动态系统控制时，它们不维持任何内部状态，因此对过去的状态或动作没有任何记忆。在机器人和感官输入都是动态的机器人任务中，尚未证明卷积网络是感知的最佳方法。循环架构(例如，长短期记忆[6])是维持内部状态的模型，可能为四旋翼等动态机器人提供更平滑、更可行的指令[7]。更新的序列建模技术使用注意力机制[8]构建变换器[9]，这些变换器已从自然语言处理领域适应到视觉[10]，在目标识别和跟踪方面表现出优越性能。

这些视觉变换器将图像的补丁作为序列输入，当提供大型训练数据集时，在越来越多的计算机视觉任务中优于卷积或循环网络。进一步的动机在于先前的工作表明，使用循环或基于注意力的模型可能由于动态机器人导航的时序性质而改善性能[11]。

本工作的目标是研究ViT作为四旋翼控制的端到端框架，并与基线架构进行比较。我们选择通过在仿真中从特权的障碍感知专家进行行为克隆来训练模型，这在现有的学习避障工作[2, 12]中很常见。这还消除了强化学习框架中可能存在的潜在混淆因素，如探索/利用权衡和额外的超参数调整。我们使用深度传感数据来训练和评估这些模型，因为在多种传感器类型、机器人平台和环境中，深度数据在先前工作中被证明在高级场景理解以及导航任务中既普遍又有效。我们通过比较在杂乱场景中前飞时的碰撞指标来评估模型彼此的性能，跨越增加前向速度的试验。我们进一步展示了每个模型通过给定仿真场景采取的路径、每个模型的指令特征(加速度和能耗)，并将我们的端到端方法与模块化基线方法进行比较。代表性的硬件实验进一步展示了围绕多个障碍物的敏捷避障和高速飞行(图1)。

我们的贡献如下：
- 首次使用视觉变换器模型进行四旋翼的高速端到端控制。
- 将视觉变换器模型与各种最先进的基于学习的架构进行比较，用于基于端到端深度的四旋翼控制。
- 演示和比较硬件上模型的真实实验。
- 开源代码、数据集和预训练权重，以重现和扩展本文的结果。¹

(

### 新的思路

于是，研究人员开始尝试用**学习方法（深度学习）**来做“端到端控制”：直接输入图像，输出控制动作，中间不再分模块。

- 好处是：反应更快，还能通过“域随机化”训练（在模拟中给数据加各种噪声），提升模型在真实世界中的鲁棒性。

本研究进一步提出：
 👉 直接把更多的跟踪和控制任务交给**视觉Transformer（ViT）**来完成。

### 为什么选择ViT？

- **卷积神经网络（CNN）**：在图像处理上表现好，但没有“记忆”，不太适合处理动态任务。
- **循环神经网络（RNN/LSTM）**：能记住过去的状态，更平滑，但可能不如最新模型强大。
- **Transformer/视觉Transformer（ViT）**：最初用于语言，现在也用于图像。它把图像切成小块（patch），像读句子一样处理，效果往往比CNN、RNN更好，特别是在需要长时间理解和全局信息的任务上。

因此，ViT被认为特别适合无人机这种需要同时考虑**时序特征（过去的动作）\**和\**全局环境感知**的任务。

)

## II. 相关工作

**端到端基于视觉的导航。** 端到端控制任务方法已在多种设置中进行了研究，如自动驾驶[13]，以及在野外自主四旋翼飞行中的部分应用[2]。这两项工作都表明这种方法具有高样本复杂性，在更结构化的自动驾驶任务中，它的泛化能力不如模块化方法。然而，随着敏捷性的增加，端到端和结构化方法的性能都会下降。现有的学习端到端四旋翼飞行工作主要关注相对较慢的飞行速度，并直接使用原始图像[12, 14]。

**用于控制的视觉变换器。** 一些研究使用架构组合来超越单个组件在目标检测任务中的性能。Swin Transformer[15]和基于ResNet的模块[16]此前已经被组合[17]，显示这种注意力+卷积网络架构改善了目标检测性能。我们在这项工作中类似地探索了基本骨干架构(卷积和U-Net、循环和注意力)之间的组合模型。当考虑如像素控制等下游任务时，ViT等变换器架构已与卷积神经网络(CNNs)进行比较，显示由于归纳偏置较弱和需要更多训练数据，其性能比CNN架构差[18, 19]。ViT在许多强化学习(RL)工作中作为表示学习器用于下游控制任务已被广泛研究[20-22]。我们的方法研究了利用类似架构使用来自特权专家的模仿学习而不是RL。

**四旋翼的视觉变换器。** 自ViT开发以来，一些工作已经将这些模型与四旋翼的机载图像一起使用，但不是在用于规划或控制的同一模块中。一些在无人机上执行目标检测[23, 24]。过去已经尝试过目标跟踪然后伺服，其中孪生变换器网络在无人机上执行目标跟踪，然后独立地，PD视觉伺服算法从边界框命令偏航角和前向速度[25]。据我们所知，我们的工作代表了首次研究将ViT以端到端方式用于四旋翼控制。

**来自深度图像的四旋翼飞行。** 在不确定环境中导航时，深度图像为学习的反应控制提供了输入模态[2, 26, 27]，甚至可以被视为下游控制的中间可学习任务[28]。已经采取了各种方法从深度图像输入学习控制输出，如建图然后规划[26]、使用运动原语[27]和采样无碰撞轨迹[2]。这些方法使用深度图像确定某些中间表示，然后用于控制四旋翼。与这些方法相比，我们展示了使用各种架构直接从深度图像学习的纯端到端控制，无需建图、规划或轨迹生成。

(

以前的研究要么是慢速端到端，要么是用ViT做“感知模块”，要么是深度图像→中间表示→控制。
 而这篇工作是**第一次真正做到“深度图像 → ViT → 控制”完全端到端的无人机高速飞行**。

)

## III. 方法

### A. 任务表述

我们将避障任务表述为以各种前向速度飞行固定距离，同时避开杂乱环境中的静态障碍物，该环境是非结构化和未知的。特权教师专家策略是一个反应式规划器，设计为短视并模拟低技能飞行员可用的数据，从而避免昂贵的真实世界、高风险的现场数据收集，这可能依赖于训练飞行员数据但可能更时间优化。在专家策略展开期间收集监督数据以训练基于端到端学习的学生模型。由于这项工作旨在研究ViTs与其他模型在开发反应式避障行为方面的比较，所有模型输出世界坐标系中的线速度指令，这与先前工作[2]中用于不太反应策略的长期轨迹规划形成对比。为了比较模型性能，我们考虑多个障碍物碰撞指标、计算复杂性和推理时间，以及其他定量和定性因素(见第IV节)。

形式上，我们考虑一个障碍感知专家$$\pi_{expert}(s_t, o) = a_t$$，输入四旋翼状态$$s_t$$和局部障碍物位置$$o$$，在每个时间步$$t$$产生动作$$a_t$$。通过在专家下进行许多轨迹$$\tau$$收集数据集$$D$$，其中转移动力学$$T$$在专家$$s_{t+1} = T(s_t, a_t), a_t \sim \pi_{expert}(s_t, o)$$下产生，深度图像$$im_{depth,t}$$在每个时间步记录。我们寻求训练学生策略$$\pi_{student}(s_t, im_{depth,t}) = a_t$$，不直接感知障碍物位置，而是依赖深度图像来重现专家策略的动作(1)。

(

无人机要在**未知的、杂乱的环境里**，以不同的速度往前飞一段固定的距离，同时不能撞到障碍物。

- 环境是“非结构化”的：没有预先的地图，也没有规则的布局。
- 障碍物是**静止**的，但分布很复杂。

#### 教师-学生思路

研究人员不可能让真实无人机去“盲飞”采集失败数据（风险太大，也太贵），所以用了一个**专家（教师）策略**来模拟：

- 专家就像一个“低水平的人类飞行员”，只能根据局部环境短视地做决定，而不是最优的长远规划。
- 专家的作用是：提供大量安全的飞行数据，作为“老师”去教神经网络。

然后，神经网络作为**学生模型**：

- 输入：无人机自身的状态 + 深度相机看到的图像。
- 输出：在世界坐标系里的线速度指令（告诉无人机往哪个方向、以多快速度飞）。

这样，学生模型不需要直接知道障碍物的具体位置（不像专家那样可以直接访问环境信息），而是要**通过深度图像来学会模仿专家的决策**。

📌 **一句话总结**：
 研究人员先用一个“虚拟老师”安全地飞很多次收集数据，再让神经网络学生只凭深度图像学会模仿老师的飞行方式，从而实现无人机在杂乱环境中高速的实时避障。

)

### B. 学习框架

学生模型以单个深度图像$$im_{depth} \in [0, 1]^{60×90}$$(类似于[2])、大小为60×90、四旋翼姿态$$q_{att} = (w, x, y, z)$$和前向速度$$v_{fwd} \in \mathbb{R}$$作为输入，并预测速度向量$$v_{pred} \in \mathbb{R}^3$$(见图2)。由于模型可以访问前向速度，我们不包括模型输入的变化，如堆叠图像序列；虽然这种输入可能改善仅卷积模型的时序预测，但我们转而提出旨在建模序列数据的循环模型。$$v_{pred}$$通过监督行为克隆从特权专家$$v_{cmd}$$学习，在轨迹中的所有时间步上使用L2损失，给定学生模型参数$$\theta$$。

$$L(\theta) = E_{\tau \sim D}\left[\frac{1}{T}\sum_{t=0}^{T-1}\|v_{cmd}(t) - v_{pred}(t; \theta)\|_2^2\right]$$ (1)

(

### 模型输入

学生模型（神经网络）每次要处理三个信息：

1. **一张深度图像**（60×90 像素，像是小型3D“雷达图”），
2. **无人机的姿态**（四元数表示，w, x, y, z → 表示它怎么转、怎么倾斜），
3. **无人机的前向速度**（飞得多快）。

这些一起作为输入，交给模型。

------

### 模型输出

模型要预测一个 **三维速度向量** $v_{pred}$：

- 就像告诉无人机：“往哪个方向、以多快速度飞”。
- 输出是连续的三维速度，而不是离散动作。

------

### 为什么不用图像序列？

有些方法会把连续几帧图像（比如最近5张）叠在一起输入，这样能让模型“记住过去”，改善预测。

- 但是在这篇研究里，研究者没这么做。
- 他们选择另一条路：用**循环网络**来直接处理时序（让模型自己带“记忆”），而不是单纯堆叠图像。

------

### 学习方式（行为克隆）

训练过程叫**行为克隆**：

- 学生模型看老师（专家）的飞行数据，
- 学生模型输出预测速度 $v_{pred}$，
- 然后和老师的真实指令 $v_{cmd}$ 比较，
- 用一个**平方误差（L2损失）**来衡量差距。

公式 (1) 的意思就是：
 在所有训练数据和所有时间步上，计算学生预测和老师指令的差的平方，把它们平均起来，作为模型的训练目标。

------

📌 **一句话总结**：
 学生模型每次看一张深度图 + 自己的状态，就要学会像老师一样给出下一步的飞行速度，训练目标就是“让预测和老师指令尽量接近”。

)

### C. 仿真设置和模型

**仿真和渲染。** 我们使用开源的Flightmare仿真器[29]，它包含四旋翼动力学的快速物理仿真和高视觉保真游戏引擎渲染器Unity的桥接。除了启用ROS外，这个仿真器包允许我们从各种环境中收集训练数据并部署四旋翼，具有准确的深度传感器测量和在笔记本电脑上的实时计算。具体来说，我们使用为DodgeDrone ICRA 2022竞赛[3]提供的仿真器的修改版本，该版本包含具有各种大小浮动球形障碍物的环境(后来称为"Spheres"环境)。仿真四旋翼质量为0.752kg，直径为0.25m。

**特权专家策略和数据收集。** 为了收集行为克隆的监督数据对$$\{(im_{depth}, q_{att}, v_{fwd}), v_{pred}\}$$，特权专家通过随机化障碍场运行短期避碰算法。注意粗体变量是多维的。总共588次专家运行产生了112k深度图像，其中27.5k图像来自至少有一次碰撞的试验。由于专家是反应性的并且缺乏动力学可行的规划，数据中并非没有碰撞；然而，我们发现一些模型(见第IV-A节)在高速时超越了这个专家。

特权专家可以访问当前无人机位置$$s_{t,pos}$$10m内那些障碍物的障碍物位置和半径信息$$o$$。它搜索从无人机当前位置到沿着设置在无人机前方某个地平线的横向-垂直平面中2D网格上的每个航点的直线无碰撞轨迹，如图3所示。选择最接近网格中心的航点，并对这个相对位置应用增益以生成速度指令动作$$a_t$$。转移动力学$$T$$由Flightmare四旋翼仿真提供。

**端到端学生模型。** 我们提出五个学生模型，用于比较基于ViT的模型与其他最先进的图像处理架构。每个模型以单个深度图像作为输入，使用由卷积或注意力组成的类似编码器的模块生成低维中间表示，然后与四旋翼姿态(四元数)和前向速度(标量)连接，然后输入LSTM或全连接层。除ConvNet模型外，所有模型大约为3M参数(表I)。这种模型大小确保在机器人控制回路(硬件平台上30Hz，见第III-D节)中进行飞行内处理的足够快速计算。尝试了某些模型，如完全基于变换器的架构，但表现不佳，未呈现(除非使用全连接头，如下面描述的ViT)。架构细节在线提供[30]。

**ViT。** 我们利用受Segformer[31]启发的基于变换器的编码器，分层应用两个变换器块，然后使用像素洗牌[32]进行上采样，并使用卷积操作跨层级混合信息。这纳入了多尺度的信息。这个ViT后跟全连接层。

**ViT+LSTM。** 这与ViT模型相同，但在全连接层之前包含多层LSTM。

**ConvNet。** 一个轻量级CNN，作为参考模型，类似于第一个也是仍然广泛使用的成功深度学习架构，用于目标检测或机器人的端到端基于视觉的控制。这个模型是唯一明显小于3M参数的学习基线。ResNet-18[16]，一个学习期望层输出残差的大型CNN模型，也进行了训练，但发现与ConvNet模型性能相似，比UNet模型变体(下面)表现更差，表明简单增加ConvNet中的参数数量并不会带来更好的性能。

**LSTMnet。** 在卷积和全连接层之间放置多层LSTM模块，以鼓励速度预测的时序一致性。

**UNet+LSTM。** 在分割任务中流行的对称编码器-解码器架构包括跳跃连接，结合局部和全局信息并帮助防止梯度消失。降维到低维特征空间也可能改善对未见环境的泛化能力。多层LSTM跟随UNet。我们的模型包括多个跳跃连接以改善性能并形成强比较。

(

### 1. 仿真环境

研究人员用的是一个开源模拟器 **Flightmare**：

- 里面包含了无人机的物理飞行动力学（重量、惯性、推力等），
- 还接入了 **Unity 游戏引擎** 来生成逼真的视觉画面（深度图像）。
- 这样既能模拟真实飞行，又能实时运行在笔记本电脑上。

他们使用的是 **ICRA 2022 DodgeDrone 比赛**的修改版本：

- 环境里布满了**不同大小的漂浮小球**（作为障碍物），
- 模拟无人机的重量是 **0.752kg**，直径 **0.25m**。

------

### 2. 教师专家策略（数据收集）

为了训练“学生模型”，需要“专家”来提供飞行示范：

- 专家能直接看到无人机周围 **10米范围内所有障碍物的位置和大小**，这是学生模型拿不到的“特权信息”。
- 专家用一个**短视的避障算法**：
  - 从无人机当前位置向前方画一个“横向-垂直平面”网格，
  - 在网格上的候选点中，找一条无碰撞的直线路径，
  - 选择最接近中心的航点，生成对应的速度指令。

这样收集到的飞行数据：

- 一共做了 **588次专家飞行**，
- 得到 **112k 张深度图像**（其中 27.5k 来自失败/碰撞的尝试）。

⚠️ 专家并不是完美的，也会撞，但这没关系，因为学生有机会在某些情况下超过老师。

------

### 3. 学生模型（对比实验）

研究者设计了 **五种不同的神经网络模型** 来比较：

1. **ConvNet**
   - 传统卷积神经网络（CNN），轻量级。
   - 是最经典的图像处理架构，用来当参考。
   - 也试过更大的 ResNet-18，但效果不比小模型好。
2. **ViT（视觉Transformer）**
   - 借鉴 **Segformer** 思路，
   - 用多层 Transformer 来提取多尺度特征，
   - 之后用卷积融合不同层级的信息，最后接全连接层输出。
3. **ViT+LSTM**
   - 在 ViT 的基础上，加上 LSTM（循环神经网络模块），
   - 让模型带有“记忆”，预测的速度指令更平滑、更一致。
4. **LSTMnet**
   - 先用 CNN 提取特征，再接 LSTM，
   - 专门强化时序建模能力。
5. **UNet+LSTM**
   - 用 U-Net（常用于图像分割的网络，有跳跃连接，能同时利用局部和全局信息），
   - 再接多层 LSTM，
   - 这种设计既能防止梯度消失，又可能提升对新环境的泛化能力。

所有模型参数量大约 **300万**（除了小的ConvNet），保证能在真实无人机上以 **30Hz 的频率**运行，足够快用于在线飞行控制。

------

📌 **一句话总结**：
 他们在一个高仿真的虚拟环境里，用“能看穿障碍物位置的专家”来生成训练数据，然后设计了 5 种不同的深度学习模型（CNN、ViT、LSTM、U-Net 等）来对比谁最适合做无人机的端到端视觉避障。

)

### D. 硬件设置

我们使用Falcon250定制四旋翼平台[34](图7a)运行kr_mav_control[35]开源四旋翼控制堆栈。所有实验在运动捕获竞技场中进行以获得准确的状态估计。Intel Realsense D435相机利用结构化红外光和立体匹配计算深度。这些图像被调整大小，无效深度像素被设置为固定值；模型推理在Intel NUC 10机载计算机(i7-10710U CPU)上完成。速度指令预测被发送到控制堆栈，输出SO(3)指令(方向和推力)，发送到开源PX4控制器[36]。虽然我们在仿真中训练浮动球形物体的模型，但我们在硬件实验中测试独立的圆柱体和块。

| 模型      | 参数      | CPU (ms) | GPU (ms) |
| --------- | --------- | -------- | -------- |
| ConvNet   | 235,269   | 0.2      | 0.4      |
| LSTMnet   | 2,949,937 | 4.8      | 0.7      |
| UNet+LSTM | 2,955,822 | 5.6      | 1.7      |
| ViT       | 3,101,199 | 5.6      | 1.6      |
| ViT+LSTM  | 3,563,663 | 9.2      | 3.8      |

**表I：** 每个模型的模型大小和推理时间。时间来自第12代Intel i7-12700H CPU和Nvidia GeForce RTX 3060 GPU，通过TorchBench[33]进行1000次单线程迭代获得的基准时间。

(

### 1. 无人机平台

研究人员用的是一架 **定制的 Falcon250 四旋翼无人机**：

- 控制系统用的是开源的 **kr_mav_control** 框架，保证飞行稳定。
- 实验场地是一个 **带运动捕捉系统的室内场地**（就像电影拍动作捕捉用的那种），这样可以精确测量无人机的位置和姿态。

### 2. 传感器与计算机

- 无人机搭载了一台 **Intel Realsense D435 深度相机**：
  - 通过红外结构光 + 立体视觉获取深度图像。
  - 图像会预处理（缩小尺寸、把无效像素设为固定值）。
- 机载计算机是 **Intel NUC 10** 小型主机，CPU 为 i7-10710U。
  - 神经网络模型在这个小主机上实时运行，预测飞行速度指令。

### 3. 控制流程

- 模型预测的结果是 **速度指令**。
- 这些指令会交给控制栈，转化为无人机的 **方向 + 推力指令**（数学上叫 SO(3) 指令）。
- 最后由开源的 **PX4 飞控系统**执行，驱动无人机电机飞行。

### 4. 实验环境差异

虽然训练数据里障碍物是**漂浮的小球**，
 但真实实验里，他们测试了 **圆柱体和方块**，验证模型的泛化能力。

------

### 5. 模型性能（表 I）

表格展示了 5 种模型的 **参数规模** 和 **运行速度**：

| 模型          | 参数量 | CPU推理时间 | GPU推理时间 | 特点                            |
| ------------- | ------ | ----------- | ----------- | ------------------------------- |
| **ConvNet**   | 23.5万 | **0.2ms**   | 0.4ms       | 最轻最小，跑得最快              |
| **LSTMnet**   | 295万  | 4.8ms       | 0.7ms       | 加了记忆功能，稍慢              |
| **UNet+LSTM** | 296万  | 5.6ms       | 1.7ms       | 融合局部+全局特征，速度中等     |
| **ViT**       | 310万  | 5.6ms       | 1.6ms       | 全局注意力，性能强但比CNN重     |
| **ViT+LSTM**  | 356万  | **9.2ms**   | **3.8ms**   | 最复杂，带注意力+记忆，速度最慢 |

结论：

- **ConvNet** → 超轻量，几乎没延迟，但能力有限。
- **ViT+LSTM** → 最强大但计算最重。
- 其他模型介于中间，可以在 **速度和精度之间做权衡**。

)

## IV. 结果

### A. 碰撞指标

我们在图4中展示了两个仿真环境的碰撞率；Spheres(4a)具有与训练期间看到的类似障碍物，Trees(4b)包含模型此前未见的树木。平均碰撞率在变化的障碍物配置中进行的多次试验上计算，速度为3-7m/s，Spheres环境中每个模型90次试验，Trees环境中每个50次试验。正如先前工作支持的，碰撞率随着前向速度的增加而增加。然而，ViT+LSTM在飞行速度超过6m/s时优于其他模型，是唯一在超过5m/s时始终优于专家的模型。这可能是由于ViT+LSTM的循环性质，与在每个重新规划时间步独立计算期望航点和相应速度的特权专家形成对比。值得注意的是，仅由ViT+LSTM组件组成的模型，即ViT和LSTMnet模型，在速度达到7m/s时呈现比ViT+LSTM更差的碰撞率。

两个环境的成功率(即零碰撞率)图可以在本文在线版本的补充材料中找到[30]。

每次试验中每个障碍物的平均碰撞时间也用于分析模型行为(图在在线材料中呈现[30])。由于专家展开期间的任何碰撞都会导致无碰撞航点搜索失败，短暂停顿四旋翼规划器，专家在多次试验中在这个指标上呈现大值(3m/s时1.20s，7m/s时0.45s)。然而，所有端到端学习模型在这个指标上表现出更低的值(最差值3m/s时0.30s，7m/s时0.16s)，表明它们没有学习这种停顿行为，ViT+LSTM模型在4.5m/s及更高速度时再次优于所有其他模型。

### B. 路径和指令特征

图4c中的轨迹路径展示了每个模型在固定Spheres环境中60m前飞时的路径分布。ConvNet和ViT+LSTM在通过场景的路径中表现出显著较少的方差，而ViT模型在朝向目标前进时在选择的路径中表现出变化的方差。值得注意的是，ViT+LSTM采取通过障碍物的最直接路径，从起始y = 0位置的最大平均横向偏差为0.75m。

图4d展示了如[37]中描述的计算能耗，对于真实无人机来说，这转化为有限电池寿命下更长的飞行时间(较低的能耗更好)。ViT模型通过添加循环(ViT+LSTM)显著改善了能耗。有趣的是，虽然所有模型和专家策略随着前向速度增加都呈现恶化的能耗，ViT+LSTM并没有显著增加。Spheres和Trees的加速度和能耗图在在线材料中显示[30]。

### C. 网络特征分析

图5突出显示了输入区域，对于仿真和真实实验，通过ConvNet和UNet中的卷积层(通过空间激活图)和ViT中的注意力层(通过注意力图)返回最强信号。ConvNet突出显示完整障碍物，对其形状几乎没有特异性；UNet特别突出显示边缘并忽略周围区域；ViT模型似乎既捕获障碍物边缘又捕获周围上下文，从而比UNet更多地捕获附近障碍物，这可能是ViTs学习图像各部分之间关系的已知行为的结果[10]。在真实实验(图5d, 5e)中，我们发现ConvNet注意力图在障碍物上更分散，而ViT更具体。

### D. 泛化能力

为了测试模型泛化能力，我们零次部署模型在包含随机放置在场景中的真实树模型的仿真环境("Trees"环境)中。我们在图4b中展示碰撞率，在论文的在线版本中显示额外的指标[30]。我们看到基于ViT的模型优于所有其他模型，泛化能力良好。我们进一步发现基于ViT的模型是唯一能够泛化到飞行穿越窗户试验(通常称为"窄隙"试验)的模型，其中从起始到目标只存在紧密分布的可行无碰撞路径，相比于Spheres或Trees环境中的许多这样的可行路径。

### E. 与模块化基线的比较

我们在仿真Spheres环境中将提出的端到端ViT+LSTM与两个最先进的模块化基线进行比较(图6)。FastPlanner[38, 40]在循环地平线方式中给定多个深度图像观测找到最优初始路径，并拟合B样条，迭代优化以保证动态可行性并且非保守。然而，由于这种方法是为3m/s左右的飞行设计的，我们另外创建了FastPlanner-scaled方法，适当缩放输出速度以更接近1-5m/s范围内的期望速度。然而，这种强制降低或增加飞行速度要么导致FastPlanner算法超时(对于<3m/s，只呈现有效试验的指标)或跟不上观测(导致碰撞增加)，类似于[2]中报告的FastPlanner基线指标。Double Description[39, 41]类似地设计用于3m/s飞行，但产生较少碰撞，与ViT+LSTM相当。与这些模块化基线相比，我们发现我们的端到端方法是唯一不仅在任何期望速度下适当飞行(图6b)，而且通过保持与模块化基线方法相比始终较低的碰撞率更好地随变化速度扩展的方法(图6a)。

### F. 对状态信息的消融

我们通过在没有这些数据的情况下重新训练来对提供四旋翼方向和前向速度给模型进行消融；表II中展示了Spheres和Trees中三个速度的成功率。这种消融对高速飞行特别相关，因为非结构化或多风环境可能导致嘈杂的状态估计和可变速度。我们发现ViT+LSTM在慢速和快速以及跨环境中使用状态信息表现更好，而ConvNet仅在Spheres环境中受益，但在未见的泛化Trees环境中测试时表现明显更差，特别是随着速度增加。

| 速度(m/s)        | 3    | 5    | 7    | 3    | 5    | 7    |
| ---------------- | ---- | ---- | ---- | ---- | ---- | ---- |
| 包括方向和速度？ | ✗    | ✓    | ✗    | ✓    | ✗    | ✓    |
| **模型**         |      |      |      |      |      |      |
| **Spheres**      |      |      |      |      |      |      |
| ConvNet          | 27   | 63   | 18   | 54   | 10   | 36   |
| LSTMnet          | 27   | 45   | 18   | 9    | 9    | 9    |
| UNet+LSTM        | 36   | 27   | 18   | 9    | 27   | 9    |
| ViT              | 81   | 54   | 18   | 27   | 18   | 18   |
| ViT+LSTM         | 54   | 72   | 45   | 45   | 9    | 36   |
| **Trees (未见)** |      |      |      |      |      |      |
| ConvNet          | 90   | 100  | 81   | 63   | 60   | 42   |
| LSTMnet          | 90   | 90   | 40   | 63   | 36   | 63   |
| UNet+LSTM        | 100  | 72   | 60   | 63   | 45   | 54   |
| ViT              | 90   | 100  | 81   | 81   | 81   | 80   |
| ViT+LSTM         | 90   | 100  | 81   | 80   | 50   | 70   |

**表II：** 向模型提供四旋翼方向和前向速度的消融研究，显示每个模型在Spheres和先前未见的Trees环境中的成功率(%)。ViT+LSTM(在Spheres和Trees中)和ConvNet(在Spheres中)受缺乏这些数据影响最大。部分中的非粗体数字表示差异很小。

### G. 真实世界实验

我们零次部署ViT+LSTM在单一和多障碍物配置中，需要避碰和飞行穿越缝隙行为，速度高达7m/s。该模型在所有试验期间机载以30Hz(CPU)运行推理。图1显示了两个这样试验的代表性示例，其中1c显示了模型的深度图像输入，红色箭头表示模型相应的输出速度指令。四旋翼在真实世界中避开障碍物，无需模型重新训练或大量系统调整；虽然深度相机图像表现出训练数据中未见的伪影，如高频模式、变化的背景外观和视野中的螺旋桨(未裁剪的深度图像可在项目网站上查看)，端到端学习模型在许多场景中以1-7m/s的速度有效执行。图7另外演示了ConvNet和ViT+LSTM模型之间避碰的真实世界差异。

(

### A. 碰撞情况

- **仿真环境**：
  - **Spheres**：训练时见过的环境（很多漂浮小球）。
  - **Trees**：测试时没见过的新环境（树木）。
- **结论**：
  1. 飞得越快，撞得越多（符合直觉）。
  2. **ViT+LSTM** 在高速（6m/s 以上）表现最好，是唯一能 consistently 超过“专家”的模型。
  3. 单独的 ViT 或 LSTM 不如 ViT+LSTM 强，说明 **注意力+记忆**的结合效果最好。
  4. 所有学习模型在“停顿时间”指标上都比专家好 → 学到的行为更流畅，不会像专家那样“卡住”。

------

### B. 路径和能耗

- **路径选择**：
  - ConvNet 和 ViT+LSTM 飞得比较稳定，路径变化小。
  - ViT 虽然能飞，但路径波动大。
  - ViT+LSTM 通常选择**最直、最短的路径**，横向偏差只有 0.75m。
- **能耗**：
  - 飞快时无人机耗电多（大家都一样）。
  - 但 **ViT+LSTM 的能耗增加不明显**，比其他模型更省电 → 电池能飞更久。

------

### C. 模型“看哪里”

- 研究人员分析了神经网络在看图时最关注的区域：
  - **ConvNet**：看到了整个障碍物，但对形状不敏感。
  - **UNet**：主要盯着边缘，忽略周边信息。
  - **ViT**：既看边缘又看周围上下文，能理解障碍物和环境关系。
  - 在真实图像上，ViT 的注意力更集中，ConvNet 的更分散。
     👉 这解释了为什么 ViT 泛化能力更强。

------

### D. 泛化能力

- 在新环境 **Trees** 中测试：
  - **ViT 系列模型表现最好**，能适应从没见过的树木场景。
  - 甚至能完成 **“穿窗户”窄隙飞行**（需要非常精准路径选择），而其他模型基本不行。

------

### E. 对比传统模块化方法

- 拿 ViT+LSTM 和两个传统的模块化规划方法（FastPlanner 和 Double Description）比：
  - 这些传统方法一般只适合低速（~3m/s），速度太快就失效。
  - **ViT+LSTM** 不受速度限制，**在各个速度下都能稳定飞，还比它们更少碰撞**。
     👉 说明端到端学习的优势明显。

------

### F. 消融实验（去掉部分输入信息）

- 研究人员试过不给模型“无人机方向”和“前向速度”的信息。
- 结果：
  - **ViT+LSTM** 的性能掉得最多，说明它很依赖这些状态信息。
  - ConvNet 在训练环境还行，但在没见过的 Trees 环境下性能掉得很厉害。
     👉 状态信息对泛化很重要。

------

### G. 真实世界实验

- 在真实无人机上测试（速度 1–7m/s）：
  - **ViT+LSTM** 在 30Hz 实时运行，无需额外调参，就能避开障碍物。
  - 训练时没见过的“噪声和伪影”（比如螺旋桨、奇怪的背景），模型依旧能应对。
  - 和 ConvNet 对比：ViT+LSTM 的避障更可靠。

------

📌 **一句话总结**：
 实验结果表明，**ViT+LSTM 是最强的组合**：它能在高速下更少碰撞，飞得更稳、更省电，还能适应新环境和真实世界，超越了传统模块化方法和单一架构。

)

## V. 结论

我们展示了首次使用视觉变换器进行从深度图像的端到端四旋翼控制，全面讨论了仿真中的碰撞率和指令特征、与其他学习架构和传统模块化方法的比较，以及在具有挑战性的真实世界试验中的部署。我们发现基于注意力的模型(ViT和ViT+LSTM)在分布内仿真环境中的高速下优于其他模型，在泛化环境中的所有速度下大大优于其他架构，甚至从前飞避障扩展到飞行穿越窗户任务，其他架构在此失败。最佳模型结合了注意力和循环(ViT+LSTM)，始终呈现低碰撞率，同时在3-7m/s速度范围内保持所有模型中最低的能耗。

消融研究显示，在各种速度和环境中，注意力-循环模型通过包含四旋翼状态信息得到改善。我们还发现，与所呈现的端到端架构相比，模块化基线方法在扩展到变化的飞行速度时未能保持低碰撞率。我们在项目网站(第I节)上提供额外的实验和可视化，以及所有代码、数据集和预训练权重，以促进基于注意力的端到端机器人控制的进一步研究。

## 参考文献

[1] Davide Falanga, Suseong Kim, and Davide Scaramuzza. "How fast is too fast? the role of perception latency in high-speed sense and avoid". In: IEEE Robotics and Automation Letters 4.2 (2019), pp. 1884–1891.

[2] Antonio Loquercio, Elia Kaufmann, Rene Ranftl, Matthias Muller, Vladlen Koltun, and Davide Scaramuzza. "Learning high-speed flight in the wild". In: Science Robotics 6.59 (Oct. 2021). ISSN: 2470-9476.

[3] Yunlong Song, Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, and Davide Scaramuzza. DodgeDrone: Vision-based Agile Drone Flight (ICRA 2022 Competition). [2022年12月2日访问]. 2022. URL: https://uzhrpg.github.io/icra2022-dodgedrone/.

[4] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. "Handwritten digit recognition with a back-propagation network". In: Advances in neural information processing systems 2 (1989).

[5] Pierre Sermanet, Soumith Chintala, and Yann LeCun. "Convolutional neural networks applied to house numbers digit classification". In: Proceedings of the 21st international conference on pattern recognition (ICPR2012). IEEE. 2012, pp. 3288–3291.

[6] Sepp Hochreiter and Jurgen Schmidhuber. "Long short-term memory". In: Neural computation 9.8 (1997), pp. 1735–1780.

[7] Jiajun Ou, Xiao Guo, Ming Zhu, and Wenjie Lou. "Autonomous quadrotor obstacle avoidance based on dueling double deep recurrent Q-learning with monocular vision". In: Neurocomputing 441 (2021), pp. 300–310.

[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate". In: arXiv preprint arXiv:1409.0473 (2014).

[9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. "Attention is all you need". In: Advances in neural information processing systems 30 (2017).

[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. "An image is worth 16x16 words: Transformers for image recognition at scale". In: arXiv preprint arXiv:2010.11929 (2020).

[11] Yunlong Song, Kexin Shi, Robert Penicka, and Davide Scaramuzza. "Learning perception-aware agile flight in cluttered environments". In: 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2023, pp. 1989–1995.

[12] Antonio Loquercio, Ana I Maqueda, Carlos R Del-Blanco, and Davide Scaramuzza. "Dronet: Learning to fly by driving". In: IEEE Robotics and Automation Letters 3.2 (2018), pp. 1088–1095.

[13] Matthias Mueller, Alexey Dosovitskiy, Bernard Ghanem, and Vladlen Koltun. "Driving Policy Transfer via Modularity and Abstraction". In: Proceedings of The 2nd Conference on Robot Learning. Ed. by Aude Billard, Anca Dragan, Jan Peters, and Jun Morimoto. Vol. 87. Proceedings of Machine Learning Research. PMLR, 2018, pp. 1–15.

[14] Xi Dai, Yuxin Mao, Tianpeng Huang, Na Qin, Deqing Huang, and Yanan Li. "Automatic obstacle avoidance of quadrotor UAV via CNN-based learning". In: Neurocomputing 402 (2020), pp. 346–358.

[15] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. "Swin transformer: Hierarchical vision transformer using shifted windows". In: Proceedings of the IEEE/CVF international conference on computer vision. 2021, pp. 10012–10022.

[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition". In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 770–778.

[17] Willy Fitra Hendria, Quang Thinh Phan, Fikriansyah Adzaka, and Cheol Jeong. "Combining transformer and CNN for object detection in UAV imagery". In: ICT Express 9.2 (2023), pp. 258–263.

[18] Tianxin Tao, Daniele Reda, and Michiel van de Panne. Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels. 2022. arXiv: 2204.04905 [cs.LG].

[19] Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, and Deheng Ye. "A Survey on Transformers in Reinforcement Learning". In: Transactions on Machine Learning Research (2023). Survey Certification. ISSN: 2835-8856.

[20] Nicklas Hansen, Hao Su, and Xiaolong Wang. "Stabilizing deep q-learning with convnets and vision transformers under data augmentation". In: Advances in neural information processing systems 34 (2021), pp. 3680–3693.

[21] Amir Ardalan Kalantari, Mohammad Amini, Sarath Chandar, and Doina Precup. "Improving sample efficiency of value based models using attention and vision transformers". In: arXiv preprint arXiv:2202.00710 (2022).

[22] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. "Masked world models for visual control". In: Conference on Robot Learning. PMLR. 2023, pp. 1332–1344.

[23] Reenul Reedha, Eric Dericquebourg, Raphael Canals, and Adel Hafiane. "Transformer neural network for weed and crop classification of high resolution UAV images". In: Remote Sensing 14.3 (2022), p. 592.

[24] Tao Ye, Wenyang Qin, Zongyang Zhao, Xiaozhi Gao, Xiangpeng Deng, and Yu Ouyang. "Real-Time Object Detection Network in UAV-Vision Based on CNN and Transformer". In: IEEE Transactions on Instrumentation and Measurement 72 (2023), pp. 1–13.

[25] Xiaolou Sun, Qi Wang, Fei Xie, Zhibin Quan, Wei Wang, Hao Wang, Yuncong Yao, Wankou Yang, and Satoshi Suzuki. "Siamese Transformer Network: Building an autonomous real-time target tracking system for UAV". In: Journal of Systems Architecture 130 (2022), p. 102675.

[26] Boyu Zhou, Fei Gao, Luqi Wang, Chuhao Liu, and Shaojie Shen. "Robust and efficient quadrotor trajectory generation for fast autonomous flight". In: IEEE Robotics and Automation Letters 4.4 (2019), pp. 3529–3536.

[27] Pete Florence, John Carter, and Russ Tedrake. "Integrated perception and control at high speed: Evaluating collision avoidance maneuvers without maps". In: Algorithmic Foundations of Robotics XII: Proceedings of the Twelfth Workshop on the Algorithmic Foundations of Robotics. Springer. 2020, pp. 304–319.

[28] Anish Bhattacharya, Marco Cannici, Nishanth Rao, Yuezhan Tao, Vijay Kumar, Nikolai Matni, and Davide Scaramuzza. "Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor". In: 8th Annual Conference on Robot Learning. 2024.

[29] Yunlong Song, Selim Naji, Elia Kaufmann, Antonio Loquercio, and Davide Scaramuzza. "Flightmare: A Flexible Quadrotor Simulator". In: Conference on Robot Learning. 2020.

[30] Anish Bhattacharya, Nishanth Rao, Dhruv Parikh, Pratik Kunapuli, Nikolai Matni, and Vijay Kumar. "Vision Transformers for End-to-End Vision-Based Quadrotor Obstacle Avoidance". In: arXiv preprint arXiv:2405.10391 (2024).

[31] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers". In: Neural Information Processing Systems (NeurIPS). 2021.

[32] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network". In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016, pp. 1874–1883. DOI: 10.1109/CVPR.2016.207.

[33] Will Constable, Xu Zhao, Victor Bittorf, Eric Christoffersen, Taylor Robie, Eric Han, Peng Wu, Nick Korovaiko, Jason Ansel, Orion Reblitz-Richardson, and Soumith Chintala. TorchBench: A collection of open source benchmarks for PyTorch performance and usability evaluation. Sept. 2020. URL: https://github.com/pytorch/benchmark.

[34] Yuezhan Tao, Yuwei Wu, Beiming Li, Fernando Cladera, Alex Zhou, Dinesh Thakur, and Vijay Kumar. "Seer: Safe efficient exploration for aerial robots using learning to predict information gain". In: 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2023, pp. 1235–1241.

[35] GitHub - KumarRobotics/kr_mav_control: Code for quadrotor control — github.com. https://github.com/KumarRobotics/kr_mav_control. [2024年3月14日访问].

[36] Lorenz Meier, Dominik Honegger, and Marc Pollefeys. "PX4: A node-based multithreaded open source robotics framework for deeply embedded platforms". In: 2015 IEEE international conference on robotics and automation (ICRA). IEEE. 2015, pp. 6235–6240.

[37] Hang Yu, Guido CH E de Croon, and Christophe De Wagter. "AvoidBench: A high-fidelity vision-based obstacle avoidance benchmarking suite for multi-rotors". In: 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2023, pp. 9183–9189.

[38] Boyu Zhou, Fei Gao, Luqi Wang, Chuhao Liu, and Shaojie Shen. "Robust and efficient quadrotor trajectory generation for fast autonomous flight". In: IEEE Robotics and Automation Letters 4.4 (2019), pp. 3529–3536.

[39] Zhepei Wang, Hongkai Ye, Chao Xu, and Fei Gao. "Generating Large-Scale Trajectories Efficiently using Double Descriptions of Polynomials". In: 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021, pp. 7436–7442. DOI: 10.1109/ICRA48506.2021.9561585.

[40] Yuwei Wu. Fast-Planner. 访问：2024-07-01. 2023. URL: https://github.com/yuwei-wu/Fast-Planner.

[41] Yuwei Wu. Double Description. 访问：2024-07-01. 2023. URL: https://github.com/yuwei-wu/DoubleDescription/tree/dodge.

---

## 补充材料

### 用于端到端视觉四旋翼避障的视觉变换器

#### S1. 代码、数据集和预训练权重

我们在此GitHub上提供所有仿真和训练代码和数据集、每个模型的预训练权重以及真实机器人实验代码(ROS1)：
https://github.com/anish-bhattacharya/ViT-for-quadrotor-obstacle-avoidance.

#### S2. 结果：泛化到飞行穿越窗户

我们另外在所有模型中以3 m/s的速度在仿真中进行零次飞行穿越窗户实验(图S1a)。虽然Spheres和Trees环境包含自由空间内的障碍物，具有多个可行的无碰撞路径，但这个环境仅在给定无人机起始位置的情况下有紧密分布的可行无碰撞路径。只有ViT模型(ViT和ViT+LSTM)在这项任务中成功，所有其他模型都失败。这提供了额外证据，表明ViT模型在端到端四旋翼控制的泛化方面优于其他最先进的视觉模型，包括循环模型。

我们进行了飞行穿越窗户实验的真实世界试验，在图S1b中呈现，使用ViT+LSTM模型以2m/s的速度进行，没有额外的微调。

#### S3. 结果：进一步的定量指标

图S2显示了如主文中讨论的碰撞指标(每次试验的碰撞率和每个障碍物每次试验的碰撞时间)，以及额外的成功率图。预期随着前飞速度增加(在每个图中从左到右移动)，碰撞率增加，碰撞时间减少(由于无人机更快穿过障碍物)，成功率降低。

图S3显示了每个模型通过固定Spheres环境在多次试验中采取的路径分布。如主文中指出的，ViT+LSTM采取通过给定障碍场的最直接路径到目标，在多次试验中相比其他模型具有较少的路径方差。图S3c显示了每个模型路径分布通过给定障碍场的平均值。

我们类似地在泛化Trees环境中呈现碰撞指标，图S4；注意由于这个环境没有用于训练，我们不为专家策略呈现指标。ViT模型呈现最低的碰撞率，尽管ViT+LSTM在6m/s时被ViT超越。我们还注意到碰撞时间图中缺乏下降趋势，这可能是树木大小均匀且通常比球形障碍物小的伪影，导致碰撞时间比Spheres中低约一个数量级。可能由于类似原因，成功率更高，ViT和ViT+LSTM优于其他模型，在7m/s时分别呈现80%和70%的成功率。

Spheres和Trees环境的指令加速度和能耗如图S5所示。在两个环境中，我们看到当循环添加到ViT模型(变成ViT+LSTM)时，每个指标都得到改善。Spheres中的ViT+LSTM模型随着前向速度增加呈现相似的加速度和能耗，与通常随着前向速度增加而增加的其他模型形成对比。然而，在较稀疏的Trees环境中，模型(除了ViT，在这些图中也称为ViT+FC)具有更相似的特征。

| 结构      | 组件 |      |      |      |      |
| --------- | ---- | ---- | ---- | ---- | ---- |
| 模型      | UNet | ViT  | Conv | LSTM | MLP  |
| ConvNet   | ✗    | ✗    | ✓(2) | ✗    | ✓(4) |
| LSTMnet   | ✗    | ✗    | ✓(2) | ✓(5) | ✓(3) |
| UNet+LSTM | ✓    | ✗    | ✓(2) | ✓(2) | ✓(3) |
| ViT       | ✗    | ✓    | ✓(1) | ✗    | ✓(3) |
| ViT+LSTM  | ✗    | ✓    | ✓(1) | ✓(3) | ✓(2) |

**表S1：** 每个模型的结构和层组件。ConvNet和LSTMnet由基本层类型本身构成。复选标记旁边的数字表示模型中该类型的层数。

#### S4. 结果：ConvNet与ViT+LSTM的真实注意力图

图S6显示了分别使用ConvNet和ViT+LSTM模型在真实实验中生成的注意力图。值得注意的是，ViT+LSTM在整个飞行过程中始终突出显示对象的相关部分(边缘)。每个图像左上角和右上角的三角形黑色区域是四旋翼的螺旋桨护罩，深度无法准确计算。

图S6显示了ConvNet和ViT+LSTM模型在避开真实世界室内障碍物期间的注意力图。ConvNet模型在障碍物上产生分散的注意力(空间激活)，随时间不一致，而ViT+LSTM在接近和避障机动期间始终突出显示对象的边缘。

#### S5. 模型网络架构

我们提供关于本文主文中提出的五种架构的额外细节。表S1包含每个模型具体结构的信息，图S7、S8、S9、S10、S11包含相应的图表。如主文中描述的，除ConvNet外，每个架构都设计为接近3M参数，例如决定用于循环架构的LSTM层数。

[此处包含网络架构图的描述，但由于这些是复杂的架构图，在文本格式中不易表示]