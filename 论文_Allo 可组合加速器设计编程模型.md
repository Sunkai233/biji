# Allo: 可组合加速器设计编程模型

**HONGZHENG CHEN***，康奈尔大学，美国
**NIANSONG ZHANG***，康奈尔大学，美国  
**SHAOJIE XIANG**，康奈尔大学，美国
**ZHICHEN ZENG**†，中国科学技术大学，中国
**MENGJIA DAI**†，中国科学技术大学，中国
**ZHIRU ZHANG**，康奈尔大学，美国

专用硬件加速器对于维持新兴应用中的性能改进变得越来越关键，特别是随着技术缩放的好处继续减少。然而，设计师目前缺乏有效的工具和方法来以高效的方式构建复杂、高性能的加速器架构。现有的高级综合(HLS)工具通常需要侵入性的源级更改才能获得满意的结果质量。尽管引入了几种新的加速器设计语言(ADL)旨在增强或替代HLS，但它们的优势在相对简单的单核应用中更为明显。现有的ADL对于具有多个核心的现实层次化设计效果较差，即使设计层次结构被扁平化。

在本文中，我们介绍Allo，这是一个用于高效空间加速器设计的可组合编程模型。Allo将硬件定制（包括计算、内存、通信和数据类型）与算法规范分离，并将它们封装为一组定制原语。Allo通过以自下而上、类型安全的方式组合来自不同函数的定制来保持输入程序的层次结构。这种方法促进了跨越函数边界的整体优化。我们在常用的HLS基准测试和几个现实的深度学习模型上进行了全面实验。我们的评估显示，Allo在PolyBench的所有测试案例中都能超越最先进的HLS工具和ADL。对于GPT2模型，Allo生成的加速器推理延迟比NVIDIA A100 GPU快1.7×，能效高5.4×，展示了Allo处理大规模设计的能力。

{

好的 👍 我来把这段关于 **Allo 加速器设计语言 (ADL)** 的介绍翻译成通俗解释：

------

#### 背景问题

- 现在，很多新兴应用（比如深度学习）都需要 **专用硬件加速器** 才能跑得快。
- 但问题是：随着传统芯片制程优化的红利越来越少，仅靠硬件“堆料”不够了，设计复杂加速器的难度越来越高。
- 现有的 **高级综合 (HLS) 工具** 虽然能把代码转成硬件，但往往需要大量改源码，调起来很麻烦，效果也不一定好。
- 一些新的 **加速器设计语言 (ADL)** 出现了，试图改进 HLS。它们在简单的小加速器上有优势，但在大规模、多核心的复杂设计上，表现并不好。

------

#### Allo 的思路

作者提出了一个新的 **Allo 编程模型**，它的核心思想是：

- **把硬件定制和算法逻辑分开**。
  - 算法逻辑：写代码时只管计算流程。
  - 硬件定制：计算单元、存储器、数据通信、数据类型等，都用单独的“定制原语”来表达。
- 这种分离方式的好处：
  - **层次结构保留**：程序还是分层的，不需要扁平化。
  - **组合灵活**：可以像搭积木一样，把不同函数的硬件定制组合起来。
  - **整体优化**：能在跨函数的范围内做统一优化。

简单理解：Allo 就像是一个 **既懂软件逻辑、又懂硬件设计的“翻译官”**，能更优雅地把程序转成高效硬件电路。

------

#### 实验效果

研究人员在常见的 HLS 基准和真实深度学习模型上测试 Allo：

- 在 **PolyBench 基准** 上：Allo 在所有测试案例中都优于最先进的 HLS 工具和其他 ADL。
- 在 **GPT-2 模型** 上：
  - Allo 生成的加速器推理延迟比 **NVIDIA A100 GPU** 快 **1.7 倍**。
  - 能效比 GPU 高 **5.4 倍**。
  - 这说明 Allo 能驾驭大规模复杂设计，而不仅限于小玩具程序。

------

📌 **一句话总结**：
 Allo 是一种新的加速器设计语言，它让算法逻辑和硬件定制解耦，用“积木式”的方式来组合硬件功能，在复杂、大规模设计上比传统 HLS 工具和已有 ADL 更快、更省电。

}

## 1 引言

随着技术缩放的最新趋势，计算机工程师越来越转向专用硬件加速器来满足新兴应用（如大型语言模型(LLMs)[62, 74, 87]）不断增长的计算需求。一种获得流行的架构范式是空间架构[27, 31, 42, 43, 56, 97]，它通过直接连线或流式缓冲区实例化专用处理引擎相互连接，以提高吞吐量并减少片外存储器访问。虽然硬件专门化可以显著提高性能和能源效率，但确实需要大幅提高开发工作量。具体来说，手动构建空间架构一直很具挑战性，特别是使用传统的寄存器传输级(RTL)设计抽象。因此，现代加速器设计越来越多地采用高级综合(HLS)来加速RTL代码生成并能够快速探索各种设计替代方案[15, 16, 51]。然而，为了实现高性能，HLS用户必须广泛重构源程序以指导工具实现专门的架构，如脉动阵列。此外，他们需要使用各种供应商特定的数据类型和编译指示，降低了设计的可重用性和可移植性。

在这种背景下，我们确定了高性能加速器高效开发的两个主要挑战。

**挑战1：平衡手动控制与自动化编译器优化。** 专家手动创建的核心提供高性能实现，但需要大量手动设计和验证工作。此外，这些核心通常遵循特定的数据类型和函数签名，这限制了它们跟上快速发展的应用程序和硬件进步的能力。越来越多地使用自动化编译器技术，如多面体编译，从简单的C/C++代码生成片上缓冲区[71]、流式数据流架构[13]或脉动阵列[17, 92]，而无需复杂的循环注释。然而，这些工具通常不为设计师提供足够的控制来探索各种性能/成本权衡，并为新应用定制内存层次结构和通信方案。采用领域特定语言(DSLs)简化了程序员和编译器的任务[25, 28, 35, 57, 84]，但大多数DSL本质上针对特定应用领域量身定制，如图像处理、机器学习和网络处理，缺乏对加速器硬件设计必需的通用语言构造的支持[51]。

**挑战2：从单核优化到复杂多核加速器设计的差距。** 在最近的趋势中，硬件设计DSL正在发展得更加通用，结合灵活的命令式语言构造或嵌入通用宿主语言如C++、Python或Scala[40, 47, 49, 59, 83]。我们将这类编程模型称为加速器设计语言(ADLs)。受Halide[75]和TVM[11]启发，几个最近的ADL进一步将算法定义与硬件优化分离[49, 83]，这提高了生产力和可移植性。然而，现有的ADL主要专注于优化单个应用核心，如卷积和矩阵乘法。在现实的多核应用情况下，这些ADL倾向于生成整体扁平化设计，回避组合不同核心的复杂性，这些核心可能呈现不兼容的接口或冲突的优化。对可组合性的不充分支持损害了模块化、可调试性，并经常导致次优性能，因为预优化的核心无法轻松集成到分层程序结构中。

为了解决这些挑战，我们提出Allo，一个用于高性能空间加速器架构可组合设计的新编程模型¹。Allo的关键设计原则是提供解耦的硬件定制原语、模块化加速器设计过程，并促进各个组件的类型安全组合。

**渐进式硬件定制。** 我们继承了流行的调度语言（如TVM[11]和Halide[75]）的思想，将硬件定制（例如缓存和流水线）与算法规范分离。每个硬件定制都是在程序上执行重写的原语。我们不仅分离基于循环的转换，还将分离扩展到内存、通信和数据类型。每个定制原语可以单独验证并逐步应用于原始程序以进行优化。

**可重用参数化核心模板。** Allo支持在核心创建期间声明类型变量，并在构建硬件可执行文件时实例化核心，这是大多数硬件ADL[40, 47, 49]中缺少但对构建可重用硬件核心库很重要的功能。Allo引入了用于创建核心模板的简洁语法，消除了用户拥有复杂元编程专业知识的需要。

¹Allo意味着非典型，这反映了我们对创建非传统硬件架构的重视。该框架是开源的：https://github.com/cornell-zhang/allo.

**可组合调度。** Allo使用户能够自下而上逐步构建核心，一次添加一个定制，同时验证每个子模块的正确性。最终，使用`.compose()`原语将多个调度逐步集成到完整设计中。这种方法是先前自上而下方法无法实现的，显著提高了生产力和可调试性。

**整体数据流优化。** 我们引入分层数据流图来支持复杂设计中多个核心的组合，同时保持函数边界。为了确保集成不同核心时接口的正确性，我们将接口统一问题建模为类型推断问题，并通过数据流分析有效解决。利用分层数据流图，我们可以有效调整阶段之间流式缓冲区(FIFO)的大小。

为了提高Allo的可用性，我们在Python中实现了前端语言，允许灵活的编程风格和最少的类型注释。我们还为Allo提供了端到端优化编译器，允许用户编写Python程序并生成硬件比特流。此外，我们提供了支持IR级别解耦硬件定制的MLIR方言，并可能支持多种不同的输入语言。总之，我们的贡献如下：

- 我们介绍Allo，一个可组合编程模型，支持渐进式硬件定制，将原始程序转换为高性能设计，每个步骤都是可验证的。

- 我们提出可组合调度，使用户能够通过组合定制核心和外部IP从头开始构建模块化硬件加速器。还提出了内存布局的类型系统以确保调度组合期间的类型安全。此外，我们引入整体数据流优化以确保功能正确性并进一步提高性能。

- 我们在现实基准测试和大型神经网络上进行了全面实验。对于PolyBench[69]，我们在所有设计案例中都超越了几个最先进的HLS工具和ADL[40, 49, 59, 102]。此外，我们在大型神经网络设计的背景下展示了编程模型的适用性。据我们所知，我们是首个使用此类ADL对FPGA上的LLM进行完整评估的。我们的实验结果显示，与A100 GPU相比，GPT2模型有1.7×加速和5.4×更高的能效。

{

#### Allo 的目标

为了解决这些问题，作者提出了 **Allo** ——一个新的 **可组合的加速器设计语言/编程模型**。
 关键点：

1. **渐进式硬件定制**
   - 学习了 TVM、Halide 的思路，把算法逻辑和硬件优化分开。
   - 把硬件优化拆成一个个“原语”（比如缓存、流水线、通信方式）。
   - 可以逐步叠加，每次都能验证正确性。
      👉 相当于“积木式”构建硬件，而不是一次写死。
2. **可重用核心模板**
   - 支持声明类型变量，可以在不同场景下复用同一个核心。
   - 提供简洁语法，避免复杂元编程，让硬件库更容易搭建。
3. **可组合调度**
   - 可以自下而上地逐步把不同核心组合起来。
   - 每一步都能验证，提升可调试性和生产力。
   - 提供 `.compose()` 接口来拼装完整设计。
4. **整体数据流优化**
   - 用分层数据流图来表示复杂多核设计。
   - 自动检查和解决不同核心之间接口的兼容性（用类型系统保证安全）。
   - 自动调整 FIFO（流式缓冲区）的大小来优化性能。

------

#### 易用性

- **前端语言**：用 Python 写程序，直接生成硬件电路（比特流）。
- **编译器**：支持端到端优化。
- **MLIR 支持**：在 IR 层支持硬件定制，未来可以兼容多种输入语言。

}

## 2 ALLO示例

现有的HLS工具通常要求用户重构应用程序代码并插入供应商特定的数据类型和编译指示以实现高性能，这些不可移植且难以维护。此外，随着基于Python框架[65, 93]在深度学习模型中的普遍使用，手动将这些模型转换为HLS C++是不切实际的。因此，在设计Allo时，我们强调以下特征作为关键原则：(1) **Python化**：拥抱Python生态系统使Allo编码体验类似于使用原生Python，有效减少学习负担；(2) **关注点分离**：解耦的硬件定制使高性能程序更易编写和维护；(3) **可组合性**：所有核心、原语和调度都应该可组合以形成复杂设计。

在下面的内容中，我们首先在Allo中实现通用矩阵乘法(GEMM)核心来说明基本语法，并提供为什么Allo能够比HLS C++提供更高生产力的线索。如图1a所示，我们首先定义GEMM核心的算法规范(第5-11行)，指定核心计算什么。由于Allo是Python嵌入式编程语言，它支持Python中的所有命令式语法(例如if-else、for和while)，区别在于用户必须为函数参数和变量声明提供显式类型注释。这个要求源于Python的动态类型特性，可能不适合硬件生成，因为需要静态数据类型来确定准确的数据位宽。Allo中的类型注释包括基本元素类型和形状。类型的正式定义可在补充材料A中找到。除了Python中的原生整数和浮点数据类型外，Allo还支持任意位宽整数和定点类型。这种通用性对于设计高性能加速器很重要，这些加速器只在需要时声明位宽，确保对各种硬件要求的适应性。

一旦算法被指定，我们通过调用`allo.customize`创建调度(第14行)。传递给`.customize()`的函数被视为Allo核心，将由Allo编译器解析。调度是一系列优化，指定核心在真实硬件上如何执行。这些优化可以应用于不同的算法，并且独立于任何特定算法，这允许我们将它们与算法分离并将每个定制封装为原语。我们将最内层循环展开8倍，并使用提供的原语为数组A和B提供多个存储库以进行并行访问(第15-17行)。Allo利用MLIR作为中间表示(IR)，并提供MLIR方言在IR级别分离这些硬件定制，如图1b所示。

最后，我们调用`s.build`(第20行)将MLIR模块降低到目标后端，生成如图1c所示的HLS代码。插入的编译指示与前端程序中的调度一致，生成的加速器以8的并行度执行GEMM核心。

{

## 2 ALLO 示例（通俗版）

### 背景问题

- 传统的 HLS 工具（把代码转硬件的工具）要求用户：
  - 重写程序代码，
  - 插入很多和供应商绑定的特殊数据类型、编译指令。
- 这样做的问题是：
  - **不可移植**（只能跑在特定硬件上），
  - **难以维护**（代码一改，很多地方都要重新写）。
- 另外，现在深度学习模型大多用 **Python 框架** 写，如果要手动把 Python 模型改成 HLS C++，几乎不现实。

所以 Allo 在设计时特别强调三个原则：

1. **Python 化**：写 Allo 代码的体验尽量接近写 Python，降低学习成本。
2. **关注点分离**：算法逻辑和硬件优化分开写，方便维护。
3. **可组合性**：不同的核心、优化（原语）、调度方式都能组合在一起，拼成复杂设计。

------

### 示例：GEMM（矩阵乘法）

研究者用一个 **通用矩阵乘法 (GEMM)** 的例子来展示 Allo 的用法。

1. **写算法**（图 1a）
   - 在 Allo 里，先写出矩阵乘法的计算逻辑（比如三重 for 循环）。
   - 语法跟 Python 一样，支持 if-else、for、while 等语句。
   - 唯一的不同：**变量要加类型注释**（比如整型、浮点型，甚至任意位宽的整数/定点类型），因为硬件需要静态数据类型来确定电路位宽。
      👉 这样，算法部分只关心“算什么”，不用管“怎么在硬件上跑”。
2. **加调度（硬件优化）**（第14-17行）
   - 调用 `allo.customize`，告诉 Allo 要对刚才的 GEMM 算法应用哪些硬件优化：
     - **循环展开 8 倍**（相当于一次并行算 8 个乘加）。
     - **数组分块**（让 A 和 B 矩阵的数据能并行访问）。
   - 这些优化都被封装为“原语”，可以单独拿出来复用。
      👉 算法和优化解耦：一个算法可以用多种优化方式，不需要重写代码。
3. **编译生成硬件**（第20行）
   - 调用 `s.build`，Allo 编译器会：
     - 把程序转成 **MLIR 中间表示**（图 1b），
     - 插入和调度对应的硬件指令，
     - 最终生成 HLS 代码（图 1c）。
   - 生成的硬件加速器会以并行度 8 来跑 GEMM，比串行快得多。

------

📌 **一句话总结**：
 用 Allo 写加速器就像写 Python：先写算法逻辑，再单独写硬件优化策略，最后自动生成高性能硬件代码。这比传统 HLS 更灵活、更高效，也更接近现代深度学习开发习惯。

}

## 3 基于HLS的硬件加速器设计中的陷阱

在本节中，我们深入探讨现有HLS工具的限制，这促使了Allo的设计。我们识别了HLS中的两个常见陷阱，并进行了几个实验来证明这些问题。对于实验，我们使用广泛使用的商业HLS工具，目标是AMD Alveo U280 FPGA[96]，频率设置为300 MHz。

### 3.1 单核设计

我们仍然利用GEMM核心作为示例。即使在这个简单的案例中，实现高性能也不是直接的。

**陷阱I：简单插入编译指示不能带来高性能。** 如图2左侧所示，HLS程序员最初定义大小为1024×1024的原始浮点GEMM核心，包含三级循环嵌套。如果直接将此代码输入HLS，尽管HLS工具尝试自动流水线化内层循环，生成的延迟仍为25,074毫秒。

为了进一步利用核心的并行性，直观的想法是展开和流水线化最内层循环。程序员可以使用`#pragma pipeline`指定设计的目标启动间隔(II)。考虑到最内层循环以32倍展开，数组A和B需要分区为多个存储库以便并行访问。令人惊讶的是，延迟没有减少到1/32，而是原始设计延迟的70%，II增加不利。这主要是由于C[i][j]浮点累加中的循环依赖，需要超过一个周期完成，阻止了II等于1的有效流水线化[20]。此外，增加的II导致频率降低，可能在后端综合期间引起路由问题。

为了解决这个问题，我们可以改变循环顺序以避免在连续迭代中更新相同的矩阵元素。如图2右侧所示，通过交换j和k循环(第18-25行)，我们将累加模式转换为行式乘积，确保相邻迭代更新输出矩阵的不同元素。此外，引入大小为1024的缓冲区来存储中间结果(第8行)，只有在遍历一行后才写回内存。结果，我们可以实现112毫秒的延迟和II=1，与原始实现相比实现223×加速。

这个例子强调了源级转换在基于HLS的硬件加速器设计中的重要性。仅添加编译指示不会导致高性能；相反，需要仔细的程序重构以启用所需的优化。不幸的是，即使使用HLS编译器中最新的设计空间探索(DSE)技术[81, 82, 102, 104]，识别此类优化可能具有挑战性。这些DSE方法通常搜索与循环分割、流水线化或展开相关的参数，但它们通常缺乏对关键内存定制的支持，如§8.2所述。Allo通过提供内存定制原语解决这个问题，允许用户在给定轴处插入缓冲区(§5.1)。完整的Allo示例可在补充材料C中找到。

进一步优化GEMM核心可能采用脉动阵列架构，这需要多个处理元素之间的流式连接并构建复杂的I/O网络以实现高性能。这些优化需要大量代码重写，也无法通过简单的编译指示插入实现。例如，GEMM的高性能2×2脉动阵列已经需要超过1100行C++代码[92]，这证明了单核HLS设计的复杂性。

### 3.2 多核设计

一旦我们有了优化的单核GEMM设计，下一个挑战是将其用作大型设计的基本构建块。在这种情况下，我们旨在构建两层前馈网络(FFN)模块，这是Transformer模型中常用的组件[23, 74, 90]。然而，即使我们已经有了优化的GEMM核心，这仍然是一个非琐碎的任务。

**陷阱II：简单调用优化核心不能保证高质量设计。** 如图3左侧所示，在顶层函数内，我们输入初始张量X和两个权重参数W_A和W_B，然后输出写入Y(第7-10行)。在主体中，我们创建中间张量Z(第11行)，重用图2中定义的rp_gemm核心，并调用它两次执行线性层计算(第12-13行)。这种方法直观地将两个函数调用链接在一起。

基于图2中的结果，级联两个GEMM核心应该产生224毫秒的延迟，因为单核GEMM延迟为112毫秒。然而，图3中的HLS报告显示延迟为280毫秒，比预期慢1.25×。此外，重用GEMM核心进行这两个函数调用应该将资源使用维持在与单核相同的水平，但HLS报告显示资源利用率翻倍。仔细检查发现，HLS生成了名为rp_gemm和rp_gemm_1的两个不同GEMM核心副本，其中rp_gemm_1表现出比rp_gemm更差的延迟。根本原因是函数接口，在图3中，我们为rp_gemm函数分区第二和第三个参数(A和B)。这两个参数对应于顶层函数中的数组W_A和Z。然而，Z作为已分区的数组，再次作为第一个参数传递到rp_gemm中，触发rp_gemm函数第一个参数的分区。这种分区差异导致HLS将两个rp_gemm核心视为不同，第一个核心分区后两个参数，而第二个核心分区所有三个参数。因此，生成了两个不同的rp_gemm核心副本，意外的分区方案导致HLS对循环变量依赖关系做出错误假设，导致延迟增加。

为了纠正这个问题并确保功能单元的正确共享，同时生成具有预期延迟的设计，我们致力于统一函数接口。如图3右侧所示，我们显式分区rp_gemm核心的第一个参数，确保所有输入和输出一致分区。此外，我们强制使用分配编译指示确保只生成一个函数实例。结果，HLS产生rp_gemm核心的单个副本，如图3左下方的资源使用所示。此外，延迟是单核延迟的两倍，总计224毫秒，符合我们的预期。

这个例子突出了组合多个核心的固有复杂性，需要仔细考虑每个核心的适当接口并通过中间缓冲区有效连接。Allo引入可组合调度和整体优化来解决这个问题。进一步的见解将在§6中讨论。

{

### 总结

- **陷阱 I（单核）**：编译指示≠高性能，需要大量源代码重构。
- **陷阱 II（多核）**：简单地调用优化核心≠高质量设计，接口和内存布局稍有不统一，就会导致性能下降、资源浪费。

Allo 通过 **内存定制原语**（解决单核问题）和 **可组合调度 + 整体优化**（解决多核问题），避免了这些坑。

}

## 4 ALLO概述

最近，提出了各种加速器设计语言(ADL)来减轻HLS的限制。这些方法中的一些在更高级语言中暴露硬件定制，要求用户遵循特定编码风格并依赖编译器生成高性能实现[26, 47, 86]。虽然如果编译器能够为设计生成适当的内存层次结构，这种方法可以部分解决陷阱I，但它要求用户用函数式语言或自定义格式编写代码，然后生成Verilog或Chisel[4]的HDL代码。这对程序员造成了翻译应用程序和在这些语言中调试的重大负担。相反，其他ADL构建在原始HLS C++工具链之上[40, 49, 59]。HeteroCL[49]引入了硬件设计中关注点分离的概念，并为用户提供优化程序的原语。Dahlia[59]提出类型系统确保内存分区和循环展开的一致性，但缺乏流水线化和数据流的关键定制。ScaleHLS[102]和PyLog[40]都自动化硬件设计并产生HLS C++代码作为输出。尽管如此，这些ADL大多专注于优化单个核心，无法有效解决陷阱II。

在本节中，我们概述Allo编程模型和编译流程。表1列出了Allo与其他高级硬件语言和编译器的比较。Allo完全将硬件定制与算法规范分离，特别关注增强内存和通信定制。这种方法有效解决了陷阱I(见§5)。此外，Allo提供声明参数化核心的能力，从而提高单核设计的可用性。Allo与其他ADL的区别是其组合个别核心和构建大规模高性能设计的能力。Allo提出可组合调度和整体数据流优化来有效解决陷阱II(见§6)。此外，我们利用Allo为大型语言模型(LLM)设计空间架构并在FPGA上执行设计。FPGA板载评估显示其功能和高性能，这是先前ADL无法实现的(§8.3)。

如图4所示，Allo提供Python嵌入式ADL以提高生产力，Allo编译器遵循传统编译工作流程。用户可以使用Python前端编写Python核心，或利用PyTorch前端直接从TorchVision[55]和HuggingFace[93]导入深度学习模型，这将在§7中进一步讨论。对于Python核心，Allo首先将其解析为抽象语法树(AST)。然后AST通过类型系统，该系统基于用户提供的注释执行类型检查、类型推断和类型转换等基本任务(§7)。类型化AST随后传递到中间表示(IR)构建器。我们在MLIR生态系统内开发了Allo方言，促进IR级别硬件定制的分离(§5)。IR构建器生成用于定制和代码生成的MLIR程序。获得IR后，使用提供的定制原语应用程序转换(§5)。这种方法与不同的树内MLIR方言无缝集成，为各种后端目标生成高性能设计。最后，我们为CPU仿真生成LLVM IR[52]，为硬件综合生成HLS C/C++[100](§7)。注意大多数硬件定制是目标无关的，允许我们的编译流程也针对ASIC设计。我们计划集成CIRCT[14]项目作为我们的后端以支持自定义电路生成。

{

#### Allo 的编译流程（图4）

Allo 的编译器和传统编译器类似，但面向硬件：

1. **前端输入**
   - 用户用 **Python** 写程序（体验类似写普通 Python）。
   - 也可以直接从 **PyTorch** 导入模型（比如 TorchVision、HuggingFace 里的网络）。
2. **语法分析**
   - 程序先被解析成 **抽象语法树 (AST)**。
   - 类型系统会检查并补全用户写的类型注释（因为硬件必须要有静态类型，比如指定位宽）。
3. **中间表示 (IR)**
   - AST 转换成 **MLIR** 格式。
   - Allo 在 MLIR 中引入了自己的 **Allo 方言**，用来区分算法逻辑和硬件定制。
   - 用户的“优化原语”（比如循环展开、缓冲区定制）在这一层应用。
4. **后端生成**
   - 可以生成 **LLVM IR** → CPU 上做仿真。
   - 可以生成 **HLS C/C++** → 做 FPGA/ASIC 综合。
   - Allo 的硬件定制是目标无关的，所以不仅能支持 FPGA，也能扩展到 **ASIC 芯片**。未来计划集成 **CIRCT**，支持更底层的电路生成。

------

📌 **一句话总结**：
 Allo 是一个基于 Python 的加速器设计语言，它把算法和硬件优化分开写，既能高效优化单核，又能灵活组合多核，还能直接从深度学习框架导入模型并自动生成高性能 FPGA/ASIC 设计。

}

## 5 可定制硬件转换

在本节中，我们使用脉动阵列[48]作为示例来说明Allo的语言特性，展示其在处理单核设计复杂转换方面的能力。脉动阵列是在Google TPU[43]和AWS Inferentia[3]等深度学习加速器中广泛使用的流行空间架构。它包含一组重复执行重复操作的处理元素(PE)。通过在这些PE之间重用数据，它最小化片外内存访问，以最小能耗实现高性能。

### 5.1 调度构建

给定图5第2-6行的初始算法定义，我们将算法规范转换为具体的硬件实现。在这里，我们正式定义程序$$P_0$$的调度$$S$$为一系列转换$$(p_i)_{i=1}^N$$，使得

$$P_0 \xrightarrow{p_1} P_1 \xrightarrow{p_2} \cdots \xrightarrow{p_N} P_N$$ (1)

其中$$\xrightarrow{p_i}$$表示使用原语$$p_i$$的程序重写，$$N$$是此调度中定制原语的数量。

表2列出了Allo支持的原语。计算定制转换循环并附加必要属性，继承现有调度语言的思想[11, 41, 49, 75]。注意，Allo采用类似于Exo[41]的方法，将原语视为程序重写，确保每个转换的正确性，而不是为原语实现整体编译器通道[11, 49, 75]。用户可以在每次定制后打印中间模块以检查实时程序转换，提供对定制原语的深入洞察。此外，我们开发了Allo-MLIR方言在IR级别实现这些定制原语，每个原语对应Allo方言中的一个操作。这使Allo能够作为中间语言并支持不同前端。在后续讨论中，我们主要关注内存和通信定制，这些区别了Allo与其他ADL。

如图5所示，用户可以通过调用`allo.customize`函数创建调度，并逐步将原语应用于新形成的调度(第9行)。每个原语完成一个转换，如图5右侧所示。我们首先为A和B数组创建中间缓冲区(第10-11行)，这为外围PE创建行缓冲区以有效从片外内存加载数据。之后，我们可以通过调用`.unfold()`沿第0和第1轴轻松生成$$M \times N$$ PE(第12行)。可以通过使用`.split()`原语的循环分块控制PE数量的进一步配置。由于这些PE是实际硬件实例，每个PE需要并行内存访问，C在两个维度上分区以适应输出静态累加的性质(第13行)。此外，A和B数组也被分区以创建多个存储库，便于并行数据输入到行缓冲区(第14-15行)。更重要的是，用户需要指定内核通信。Allo通过`.relay()`原语提供将相邻PE与FIFO连接的无缝方式。如第16-17行所示，buf_A沿第1轴连接，而buf_B沿第0轴连接，这些连接随后将在硬件中合成为FIFO。此脉动阵列生成的HLS C++代码附在补充材料D中供参考。

注意这种方法足够通用，允许用户以不同形式表达程序。用户可以使用我们的编程模型从任意程序状态$$P_i$$开始，并应用定制原语$$p_i$$获得转换后的程序$$P_{i+1}$$。例如，在图5中，应用$$p_0$$后的程序$$P_1$$仍然是功能性的，因为它本质上将数据从DRAM移动到中间缓冲区，并让计算逻辑从此缓冲区加载相同数据。这是AutoSA[92]等专用脉动阵列编译器无法实现的。AutoSA中的优化过程不透明，程序员无法轻易配置生成脉动阵列的架构。相比之下，我们的方法允许定制计算、内存、数据类型和通信。我们可以使用`.buffer_at()`原语为buf_A和buf_B进一步创建额外的内存层次结构，这将扇出减少到一个。此外，Allo比半手动脉动阵列生成器如SuSy[50]和T2S[83]提供更多灵活性，这些需要用户编写冗长的均匀递归方程并手动进行复杂的时空变换。相反，Allo可以从原始GEMM核心开始，使用八行调度代码逐步将其转换为功能脉动阵列。利用提供的原语，Allo能够在基于编译器的优化和手动优化之间取得适当平衡。如§8.2所示，我们的编程模型不仅可用于生成脉动阵列，而且足够通用以支持不同应用。

### 5.2 验证

确保生成加速器的正确性非常重要。Allo采用两个关键验证程序来增强生成代码的可靠性。首先，Allo利用CPU后端进行功能仿真测试(§7)。其次，Allo集成等价性检查器[70]来正式验证定制前后程序的等价性，前提是程序具有静态可解释控制流(SICF)。SICF要求问题大小在编译时已知，不支持参数循环嵌套分析。

图6显示了由错误定制引起的数据访问顺序错误。在此示例中，图6a的第17-18行将第二个矩阵乘法从内积转换为行式乘积，以行主序读取输入C。然而，第20行重排第一个矩阵乘法循环以列主序发送输出C。接收和发送顺序的这种差异违反了流FIFO上按序访问的要求。第26行的`.verify()`调用等价性检查器，该检查器获取定制前(第14行)和定制后(第20行)的调度以正式验证程序语义等价性。在此示例中，定制破坏了加速器设计并在定制代码中造成语义差异。符号表示中的差异被等价性检查器检测并报告以便调试。注意验证可以在应用每个原语后进行，确保每步转换的正确性。

### 5.3 参数化核心模板

我们最初构建了具有固定维度和数据类型的脉动阵列，在处理可变大小输入矩阵时缺乏灵活性。在下面的内容中，我们利用先前定义的脉动阵列引入处理任意大小输入的分块设计。

Allo提供用户友好的参数化模板以促进多态性。如图7所示，我们使用类型参数参数化脉动函数。用户可以使用语法`def <func>[<type params>](<args>)`定义函数签名。同样，考虑到Allo将数据类型与算法规范分离，数据类型和形状都可以作为类型参数。Allo允许参数化数据类型的附加约束。例如，`Ty: (int32, float32)`指定数据类型Ty必须是int32或float32。如果使用任何其他数据类型，将引发错误。在tiled_systolic函数内，我们部分特化内部脉动阵列为固定8×8大小，允许我们派生出能够容纳不同维度输入的脉动阵列分块版本。

在Allo中，我们已将几个模板核心封装为库，每个都伴有预定义调度。这些模板包括常用的深度学习操作符和用于矩阵-矩阵乘法和矩阵-向量乘法的高性能脉动阵列。这种方法允许用户方便地重用这些核心进行自己的工作负载，减少编写高效调度的负担。此外，这种参数化接口促进自动调优和自动调度[12, 78, 107]，我们计划在未来工作中探索。

## 6 可组合调度

在本节中，我们探索组合多个调度以构建完整设计的过程。我们首先介绍`.compose()`原语并深入了解分层数据流图的实现细节。然后我们介绍调度重放和内存布局组合的算法。最后，我们扩展Allo以支持外部核心的组合，并提出整体优化算法。

### 6.1 .compose()原语

我们利用图7中的脉动阵列实现来说明如何级联两个脉动阵列创建更大的设计。如图8所示，两个脉动阵列按顺序排列，中间张量Z便于数据传输。这与当今神经网络实现[65]中的常见做法一致。

基于§4中提出的原语，我们可以独立优化各个核心。然而，我们需要一种机制来连接这些较小的核心。因此，我们提出新原语`s.compose(<new_schedule>,<id>)`，用于将`<new_schedule>`合并到原始调度s中。在图8中，我们调用`s_top.compose(s)`将tiled_systolic的优化调度s集成到s_top中(第13行)。附加的`<id>`参数用于区分不同的函数调用。例如，第6行和第8行中有两个对tiled_systolic的调用者。通过在类型参数列表中包含附加标识符(例如"FFN1"、"FFN2")，用户可以对它们进行不同定制。Allo编译器将为这些函数生成两个不同实例。这种细粒度控制使用户能够定制特定函数以满足需求，提供传统编译器在编译器通道中固定模式无法获得的灵活性。最后，两个脉动阵列与另一个FIFO链接以在顶层建立数据流(第14行)。

此外，`.compose()`原语可以作为IP集成器从现有基于HLS的核心库[98, 99]导入C++核心。Allo解析HLS IP的函数接口，函数参数将被翻译为标记有Allo支持数据类型的内部表示。只要用户指定正确的参数数量，Allo自动生成带有PyBind11[72]的包装函数，使其更通用和可扩展到不同定制。

### 6.2 分层数据流图

编译器用于分析的传统数据流图通常是扁平化的，这消除了函数间的边界，可能错过图级别的潜在优化机会[11, 49, 76]。为了在调度期间保持模块的层次结构，我们需要能够以便于分析函数间接口的方式表示数据流图的新数据结构。作为解决方案，我们提出分层数据流图，以分层方式连接嵌套函数。

考虑到构建的MLIR是SSA形式，数据流图中的每个节点代表IR中的操作。如图9所示，top函数在其主体内调用两个子函数mul和add。为了表示数据流图中的节点，我们使用以函数名为前缀的变量名。我们在数据流图中维护调用者和被调用者信息，并使用边显式连接函数参数与调用者操作数。例如，数组C传递到函数add中，所以从top:C到add:A有一条边。由于我们主要关心数据如何流动而不是需要多少迭代，控制流从图中消除。这简化了第6.4节讨论的组合过程。

### 6.3 调度重放

我们将调度组合过程形式化如下。考虑程序$$P$$的调度，表示为原语序列$$S_P = (p_0^P; \ldots; p_{N_P}^P)$$。将此调度$$S_P$$与另一个调度$$S_Q$$组合涉及将$$S_Q$$的定制原语附加到$$S_P$$的原语之后，即$$S_Q \circ S_P$$，其中$$\circ$$表示序列连接。

我们在算法1中概述了组合两个调度的通用算法，很容易扩展到多个调度。此算法遍历输入调度中的原语并在新程序空间的上下文中重放它们。由于一些函数可能由于`<id>`接口而被重命名或复制，我们必须更新原语的参数以确保它们适用于正确的函数和操作(第3行)。在应用新原语之前，我们验证它不会与先前应用的原语冲突(第4行)。对于计算定制，只有当同一操作被定位时才发生冲突。在这种情况下，引发错误，因为操作已被转换(第5行)。与`.partition()`和`.relay()`相关的冲突将在§6.4中进一步讨论。然后原语被附加到新调度中(第6行)。

仅当所有子调度都集成到顶层函数并准备好后端可执行构建时，原语才应用于程序$$P$$(第7行)，这节省了冗余转换时间。生成的$$S_{out}$$可用于后续组合。这种渐进组合过程允许我们逐步组合小设计，最终构建大型设计，每个子模块都经过彻底测试，如§5.2所述。

### 6.4 内存布局组合

当定制原语只影响内部计算时，使用算法1重放它们很直接。然而，当调度通过函数接口重叠时，复杂性出现。如果子调度改变函数接口，父程序也必须改变以避免冲突，如§3.2所述。保持函数调用参数和实际函数定义之间的一致性很重要。数组分区就是这种挑战的例子。在图5中，当调度集成到图7中时，local_A、local_B和local_C数组也应被分区。这是因为这些数组在脉动函数内被分区并作为参数传递到函数中。

由于硬件内存分区本质上改变数据布局，我们可以将数据布局显式表示为类型[53, 66]并在此类型系统内进行分析。如图10左侧所示，我们考虑$$N$$维数组的分区类型。分区类型$$\tau$$是由每个维度的基本类型$$\hat{\tau}$$组成的复合类型。每个基本类型可以采用四种选择之一。$$\perp$$表示完全分区此维度，允许并行访问所有元素。$$\top$$表示此维度无分区，硬件上只有一个内存库。$$C_\alpha$$表示因子为$$\alpha$$的循环分区，原始数组中的元素交错。$$B_\alpha$$表示因子为$$\alpha$$的块分区，原始数组被分割为连续块。设$$s_i$$为维度$$i$$的大小，$$\alpha$$应该是$$s_i$$的整数因子(不包括1和它本身)。我们可以为这些基本类型构建子类型关系。如果$$X <: Y$$，意味着期望分区类型$$Y$$内存的代码也兼容分区类型$$X$$的内存。例如，$$\perp$$是$$C_2$$的子类型，因为完全分区已经将数组分区为因子为2的循环。如果核心要求数组循环分区为两个库以并行访问元素，传入完全分区数组也是可以的，因为它提供更多内存库。值得注意的是，这种子类型关系是协变的，这意味着基本类型$$\hat{\tau}$$的子类型关系也适用于复合类型$$\tau$$。这种子类型关系实际上形成格，其中类型定义中每对元素都有唯一上确界$$\top$$和唯一下确界$$\perp$$。图10右侧显示了示例Hasse图。

构建内存布局的简单语言后，我们可以为这些分区类型定义类型规则。如图11所示，第一行展示了基本类型的子类型关系，第二行显示了复合规则和函数应用。对于函数应用，确保函数签名和调用者操作数的分区类型兼容很重要，这导致$$\tau_3$$和$$\tau_1$$之间的子类型关系。例如，在图3中，原始函数rp_gemm已经在$$C_{32}$$中分区数组Z，而程序试图传入Z，它没有分区(即$$\top$$)。因此，我们的类型系统直接拒绝此程序，避免陷阱II中可能的性能问题。

基于类型规则，需要在调度组合后为程序中每个变量分配新的分区类型。由于子类型关系的存在，函数式语言中常用的统一算法不适合这种情况[18, 19, 37]。一般来说，使用子类型进行类型推断可能是具有挑战性的任务，需要复杂的代数运算或利用SMT求解器来解决约束[24, 64, 68]。然而，考虑到子类型关系的格性质以及§6.2中构建的分层数据流图，我们可以在此数据流图上应用数据流分析来有效为转换程序中的变量分配类型。

考虑具有$$M$$个节点的数据流图，我们可以使用算法2计算每个节点的适当内存布局。这种迭代算法类似于静态数据流分析中使用的工作列表算法[44]。我们使用图9中的具体示例来说明过程。假设我们在数组C上应用完全分区，即$$t'_{in} = \perp$$。我们首先将目标节点top:C和目标分区类型$$t'_{in}$$添加到工作列表(第1行)。在第一次迭代中，我们计算top:C的类型为$$t_{top:C} \leftarrow \perp \sqcap \top = \perp$$(第4行)，其中$$\sqcap$$是最大下界(GLB)操作符。由于其类型改变(第5行)，我们遍历其前驱(即mul:C)和后继(即add:A)(第6行)，并将它们附加到工作列表中，因为它们与C不在同一函数(top)中(第7-8行)。类似地，在后续迭代中，$$t_{mul:C}$$和$$t_{add:A}$$更新为类型$$\perp$$，工作列表中没有更多数据流节点更新类型，这完成了算法。注意第4行中的$$\sqcap$$操作符用于处理合并两个分区类型的更一般情况，这确保例如图10中两个不同类型$$C_4$$和$$B_2$$也可以转换为公共类型(例如$$\perp$$)。

算法2保证在线性时间内终止，如以下正式陈述：

**定理6.1。** 算法2可以在$$O(M)$$步内终止。

这种终止条件可以通过Knaster-Tarski不动点定理[85]建立，考虑到格的深度是不相关于$$M$$的固定常数。正式证明可在补充材料B中找到。由于在每次迭代中，$$t_i$$只从格上的一种分区类型向另一种分区类型在一个方向上改变，此算法在推断分区类型方面是高效的。§8.2中的实验结果证明其开销可忽略不计。

值得注意的是，算法2也可应用于流类型传播。由于空间限制，我们在此不提供完整细节。

### 6.5 整体优化

为了实现高性能空间架构，函数使用FIFO相互连接，形成不同的数据流阶段。虽然这创建了功能架构，但数据流可能遭受性能问题，特别是当有数据停滞时。仅HLS无法确定阶段之间理想的FIFO大小。数据流停滞可能源于两种主要情况：(1)当生产速率超过消费速率时，可能填满FIFO并导致停滞，(2)或当生产速率较慢时，导致后续阶段饥饿。因此，确定适当的FIFO大小对高性能至关重要。

我们将问题表述如下。假设源阶段可以每$$II_{src}$$个周期生成$$C_{src}$$个输出，其中$$II_{src}$$是前一阶段的启动间隔。目标阶段每$$II_{dst}$$个周期需要$$C_{dst}$$个输入进行计算。两阶段之间的通信量记为$$V$$。我们有函数$$f_{prod}$$(生产速率)和$$f_{con}$$(消费速率)，跟踪在任何给定时间$$t$$生成和消费的样本数量。如果FIFO中没有数据，后续阶段无法执行计算，因为它没有接收到任何数据。因此，消费速率总是小于或等于生产速率。

$$f_{prod}(t) = \begin{cases} C_{src} \lfloor t/II_{src} \rfloor & t \leq V/C_{src} II_{src} \\ V & t > V/C_{src} II_{src} \end{cases}$$ (2)

$$f_{con}(t) = \max\{C_{dst}\lfloor t/II_{dst}\rfloor, f_{prod}(t)\}$$ (3)

因此，源和目标阶段之间的FIFO深度可以计算为：

$$d = \max_t \{f_{prod}(t) - f_{con}(t) + 1\}, t \in \left[0, \arg\min_{t'} \text{abs}\left(V - \sum_{t'}f_{con}(t)\right)\right]$$ (4)

其中最大$$t$$表示接收所有输入所需的时间。$$II_{src}$$和$$II_{dst}$$可以通过运行高级综合过程获得。

这种方法对优化两阶段间的单个连接有效。然而，在涉及多个阶段的情况下，前一阶段的生产速率可能影响所有后续阶段。基于方程3，设$$f_{con}(t) = f_{prod}(t)$$，我们可以获得新的$$II'_{dst} = II_{src}C_{dst}/C_{src}$$。然后将此信息通过数据流图自上而下传播。在多个生产者输入单个消费者的情况下，结果$$II$$是它们的最大值，因为最慢的阶段决定整体节奏。因此，我们可以确定整个数据流图的适当FIFO大小。值得注意的是，这种优化只消除第一种数据流停滞，但不解决核心内可能导致第二种停滞的潜在设计问题。

## 7 实现

Allo使用9K行Python代码实现前端ADL，10K行C++代码实现MLIR方言和后端代码生成。在本节中，我们提供前端、类型系统和代码生成的实现细节。

**前端。** Allo支持命令式和声明式编程。与依赖基于跟踪技术生成AST和相应IR的几种调度语言[11, 49]不同，Allo利用Python的AST，提供有效处理控制流的能力。因此，Allo可以无缝适应最新的Python语言特性并与庞大的Python库生态系统保持兼容。因此，Allo核心可以使用原生Python运行时执行以验证功能正确性，将Python程序迁移到Allo表示所需的工作量最小。

为了适应更大的设计，如神经网络，Allo提供从TorchVision[55]导入视觉模型和从HuggingFace[93]导入语言模型的直接支持，以实现最大灵活性。Allo在此充当中间语言，显示其对硬件加速器设计的通用性。我们在PyTorch 2.0[2]中提供TorchDynamo[73]后端，用户可以调用`torch.compile(model,"allo")`来调用Allo编译器。我们采用torch.fx[76]作为高级IR，并将每个PyTorch操作符翻译为Allo内的库函数调用。PyTorch级别的优化(例如操作符融合)与Allo的优化正交。只要模型可以在torch.fx中表示，Allo就可以接受并执行硬件特定定制。与在Python中编写Allo核心相比，这种方法消除了用户重写模型和自己构建调度的需要。PyTorch前端通过允许用户直接导入模型并利用高性能Allo调度进一步简化了编程。由于我们的IR构建在MLIR之上，我们也计划在未来支持MLIR生态系统[22, 58, 88]内的其他前端。

**类型系统。** Allo配备类型推断引擎，设计用于管理内置和自定义数据类型。Allo类型系统与Python原生系统不同，因为它包括任意位宽整数、定点类型和类型提示中的附加形状信息。Allo的类型系统一致防止任意位宽整数和定点数的溢出，必要时提升更大位宽的数据类型。基于预定义的类型规则，类型推断引擎从顶层函数的注释开始，尝试推断每个内部变量的数据类型。在推断数据类型与用户注释不符的情况下，引擎在认为可行时尝试自动类型转换。此外，Allo在类型声明中包含形状信息，便于形状推断，从而支持数组切片和自动广播。

**代码生成。** 定制程序后，用户可以调用`s.build(<target>)`生成用于CPU仿真或FPGA比特流的有效程序。对于CPU后端，Allo将自定义操作和数据类型降低为LLVM IR，并使用即时执行引擎[53]运行程序。对于FPGA后端，Allo为AMD Vivado/Vitis HLS[100]生成HLS C++代码。由于这些工具接受用C/C++编写的程序，Allo直接从仿射和memref方言生成代码，绕过降低到更低级别方言的需要。在代码生成期间，流水线和展开等注释属性被转换为HLS编译指示。

## 8 实验

在本节中，我们首先介绍实验设置，并在全面基准测试和大型神经网络模型上评估Allo与几个基线的对比。

### 8.1 实验设置

对于单核评估，我们将Allo与ScaleHLS[102]、PyLog[40]、HeteroCL[49]、Merlin[101]和Dahlia[59]进行比较，所有这些都生成HLS C++代码作为输出。它们代表公开可用的最先进ADL和编译器。我们在PolyBench[69]上评估它们，这是一个基于C的基准测试套件，包含科学计算中常用的核心。所有实验使用标准中等问题大小和float32数据类型。

对于多核评估，我们评估三种不同的卷积神经网络(CNN)：ResNet-18[34]、VGG[80]和MobileNet[39]。这些模型在PyTorch[65]中实现并从TorchVision[55]库导入。我们运行模型推理并将结果与ScaleHLS比较，ScaleHLS是唯一提供从PyTorch直接模型导入的前端。表1中列出的其他ADL不提供Python绑定[26, 47, 86]，不生成用于后端综合的HLS C++代码。因此，在这些输入语言中重新实现这些设计很困难，特别是对于大型深度神经网络，使公平比较困难。

为了展示Allo生成在真实硬件上运行的大规模设计的实用可行性，我们实现了GPT2[74]模型的加速器，这是GPT家族中唯一的开源模型。GPT2是广泛用于文本生成任务的基于Transformer的仅解码器架构，具有355M参数、24个隐藏层、注意力模块中16个头和1024的隐藏大小。我们将模型量化为4位权重和8位激活(W4A8)以实现高效部署[30, 95, 106]，并根据PyTorch中的量化模型验证结果以保持准确性。我们为Allo生成的设计运行后端综合并在FPGA上部署比特流。对于如此规模的加速器，所有基线ADL都无法生成满足资源约束的有效设计。即使尝试减少这些设计的大小，它们仍然由于过多的内存访问导致冗长的板载连线而在布线阶段遇到错误。因此，我们直接将Allo与用SystemVerilog编写的最先进Transformer加速器DFX[38]比较。我们进一步将加速器与两个GPU设备比较，即NVIDIA GeForce GTX 1080Ti GPU(广泛使用的商业GPU)和NVIDIA Tesla A100 GPU(常用于大规模模型训练和推理的高端GPU)。注意GPU需要"假"量化，因此实际低位性能低于优化的fp16性能，特别是对于少于十亿参数的模型[21]。因此，我们在实验中报告GPU的最佳fp16性能。

所有实验都使用Vitis HLS v2022.1[100]针对AMD Alveo U280 FPGA。U280 FPGA具有4032个BRAM 18K块、9024个DSP切片、260万触发器、130万LUT和960个URAM块。Allo为OpenCL中的Vitis生成主机程序。AMD Xilinx RunTime(XRT)和Xilinx Board Utility(xbutil)用于硬件执行和功率测量。我们为高级综合设置频率为300 MHz，为板载评估设置250 MHz，因为大型设计在进一步增加频率时可能有路由问题。

### 8.2 单核评估

图12显示了多个ADL相对于Vitis HLS基线的延迟加速。对于Vitis HLS，除了默认自动循环流水线化外，我们不应用任何编译指示或硬件定制。Merlin自动插入编译指示。ScaleHLS使用设计空间探索(DSE)引擎从Pareto最优前沿搜索最佳优化。我们使用可用的定制原语为Allo、HeteroCL、PyLog和Dahlia手动设计最佳优化方案。尽管PyLog提供自动编译指示插入，我们发现手动定制能实现更好性能。HeteroCL具有内核级解耦定制功能，但缺乏内核间优化支持。与HeteroCL相比，PyLog的定制原语集合有限。例如，PyLog支持循环展开、流水线、重排序和分块等计算定制，但无法构建自定义内存层次结构。Dahlia专注于HLS结果的可预测性而非性能优化。例如，Dahlia保证一致的内存存储体和循环展开因子，但缺乏循环流水线化的重要支持。Allo在不同案例中相比Vitis HLS实现高达1099×延迟加速，相比ScaleHLS实现1478×，相比HeteroCL实现34×，相比PyLog实现837×，相比Merlin实现775×，相比Dahlia实现1405×。

我们从PolyBench套件中选择了五个Allo显著优于ScaleHLS的设计。从表3可以看出，尽管ScaleHLS使用自动设计空间探索(DSE)引擎搜索最佳优化，DSE结果的流水线II仍然很高。高流水线II以两种方式损害设计性能：(1)整体延迟增加，(2)由于DSP重用，HLS生成大型多路复用器。这些大型多路复用器成为关键路径并降低时钟频率。频率恶化对atax特别明显。ScaleHLS由于循环依赖、过度内存访问和整体内核的复杂嵌套循环结构无法完全流水线化设计。例如，atax有两个具有循环依赖的矩阵-向量乘积。我们使用Allo实现§3.1中讨论的行式乘积以打破循环依赖，并将内层循环流水线化为II=1。jacobi-2d是典型的模板内核，具有滑动窗口数据访问模式。ScaleHLS无法完全流水线化此类情况，因为它不支持数据重用。Allo构建两级重用缓冲区，大幅减少片外内存访问并完全流水线化设计。对于correlation、symm和trmm，Allo组合每个使用上述优化定制的较小内核，形成数据流流水线设计。表3中标记为蓝色的DSP使用比例因子和延迟加速因子。使用Allo定制的完全流水线设计每DSP提供更高性能，同时获得更高时钟频率。此外，Allo以更少代码行和短编译时间提高加速器设计生产力。解耦定制和内核组合在Allo原语的不到20行代码中简洁表达。我们在补充材料C中展示了定制gemm和jacobi-2d设计的完整示例。

### 8.3 多核评估

#### 8.3.1 卷积神经网络。

表4显示了CNN的结果。与ScaleHLS相比，Allo可以实现高达12.7×加速，同时在大多数情况下保持更小的资源使用。这是因为ScaleHLS不执行内核融合且缺乏重用缓冲区支持。Allo结合这些优化以显著增强数据局部性。此外，ScaleHLS在充分利用设计的内核间数据流方面不足，错过了使用FIFO优化的机会。相比之下，Allo在高效利用数据流特性方面表现出色，实现了改进的性能和资源利用率。我们的评估展示了Allo在组合不同内核形成分层结构和优化其协调方面的熟练程度，与ScaleHLS遇到的挑战相比呈现巨大优势。

#### 8.3.2 大型语言模型。

GPT2模型的加速器架构如图13所示。典型的Transformer块包括利用标量点积(SDP)计算注意力分数的MHA模块和级联两个线性层的FFN。GPT2等LLM本质上比CNN更复杂，无论是模型大小还是MHA模块内需要分割头并在最后合并结果的复杂连接。现有ADL和编译器通常缺乏显式内存和数据流管理[40, 49, 102]。因此，它们生成的设计通常需要显著更多资源——超过可用片上资源的两倍。相比之下，Allo能够单独设计和优化每个子模块，提供对内存层次结构和数据编排策略的控制。在验证正确性后，这些子模块可以使用`.compose()`原语自下而上组合，然后使用`.relay()`原语连接形成完整设计。

在此实验中，我们考虑单批次低延迟设置中LLM的生成推理场景[9]。我们调整输入和输出序列长度并测量从内核启动包括CPU-GPU/FPGA通信时间的端到端延迟。如图14所示，Allo始终优于最先进的加速器DFX，在延迟方面实现高达2.80×加速。这主要归功于高度定制的高性能脉动阵列内核以及Allo内多个内核的高效组合。从图14右侧可以看出，Allo也比DFX使用更少资源。这是因为Allo中设计的空间架构最大程度地减少了片上中间缓冲区。相比之下，DFX采用重叠设计，为各种操作符重用硬件单元，从而增加资源利用率。因此，我们可以实现更高频率但比DFX使用更少资源。值得注意的是，GPT2模型使用PyTorch前端直接导入(§7)，不需要用户为模型重写任何代码。加速器充分利用Allo定制原语，用不到50行定制代码优化这个复杂设计，比在硬件描述语言(HDL)中实现更有生产力。

此外，我们扩展性能评估以与两个GPU设备比较。当输出序列长度较小时，基于FPGA的设计与GPU相比表现出显著性能差距。这种差异主要由于生成推理初始阶段的大量计算需求，即预填充阶段，这更有效地与GPU等面向吞吐量的设备对齐[9, 67]。然而，随着输出序列长度增加，Allo生成的FPGA加速器超越GPU性能。特别是，我们与1080Ti GPU相比实现5.05×加速。即使与高端A100 GPU比较，我们在较长输出序列时仍实现1.70×加速。此外，FPGA执行只需要30瓦的测量功率，而A100对应物需要96瓦，这意味着Allo加速器比A100 GPU的能效高5.44×。

## 9 相关工作

**调度语言。** Halide[75]首先在图像处理领域引入算法和调度解耦的概念。TVM[11]将此思想扩展到深度学习并支持将神经网络模型映射到不同硬件设备的端到端优化工作流程。还有其他DSL利用调度语言在其特定领域生成高性能代码[5, 7, 8, 33, 45, 49, 89, 103]。Allo也采用这种解耦思想，但进一步增强定制和调度的可组合性。它利用分层数据流图将较小设计组合成较大设计，在大型设计上实现高性能。

最近的发展探索程序重写技术作为传统调度树的替代，使处理更多命令式程序成为可能。例如，TensorIR[29]扩展TVM以支持描述计算的更灵活语法，为GPU上TensorCore实现更好张量化。Exo[41]使用效应系统形式化程序重写规则以保证转换正确性。MLIR中的xform方言[108]也支持程序重写。虽然这些努力主要关注内核级优化，它们缺乏有效组合多个内核优化的能力，限制了它们对更大更复杂设计的可扩展性。

**加速器设计语言(ADL)和编译器。** 许多领域特定语言(DSL)已出现以促进不同应用的硬件设计[25, 35, 36, 54, 57, 63, 77, 79, 84, 91]，为特定领域提供高度优化的操作符。随后，引入了各种ADL以解决更通用的加速器设计[6, 26, 40, 46, 47, 86]。然而，它们的算法和定制纠缠在一起，导致生产力降低和对不同定制组合探索的限制。HeteroCL[49]将硬件定制与算法分离，但主要关注单核设计。几个ADL也强调数据流优化[6, 26, 47, 86, 94]，但它们难以保持数据流的分层结构，无法有效将小核心组合成更大设计。因此，在扩展以适应大型复杂模型时遇到挑战。

最近的努力利用MLIR工具链生成C/C++ HLS代码[1, 102, 105]。然而，现有编译器通道和设计空间探索(DSE)引擎通常无法产生高性能加速器，如第8.2节所示。这主要是因为MLIR缺乏对量化数据类型、内存和数据流定制等关键组件的固有支持。最后，Calyx[61]等低级硬件设计语言(HDL)旨在促进后端综合过程。Filament[60]也是利用时间线类型推理时序安全的低级HDL。这些努力与我们的正交，我们计划在未来支持CIRCT[14]项目作为后端。

## 10 结论和未来工作

在本文中，我们提出Allo，一个用于加速器设计的可组合编程模型。Allo提出渐进式硬件定制，允许用户逐步应用可证明的程序转换，并进一步引入可组合调度将小核心组合成大型设计。尽管如此，我们正在进行的工作中有几个未探索的方向。

对于内核内的优化，我们计划设计自动调度器以减少开发人员的编程工作。对于组合多个内核，§6.5中提出的技术只解决FIFO大小调整，但不自动确定在何处建立这些连接。一些内核可能具有阻止直接与FIFO连接的依赖关系，可能需要额外缓冲区以确保顺序内存访问。我们计划开发自动缓冲化技术在阶段之间创建缓冲区并保证正确性。

实际硬件设计不仅涉及定制和转换代码；还涉及连接组件并指导它们通过整个后端综合过程生成比特流。将数据流区域映射到多芯片FPGA板可能具有挑战性。我们为LLM实验的几个大型设计由于路由阶段问题而无法满足时序要求。为了提高设计频率，必须显式将数据流区域绑定到特定硬件区域并最小化跨芯片通信。尽管AutoBridge[32]等努力旨在将设计分解为较小部分并组装它们，但它们无法适应复杂的分层数据流或创建双缓冲区。我们计划创建能够并行编译整个设计并有效链接组件的构建系统，类似于软件链接器的工作方式。这种方法将有助于进一步优化硬件设计过程并提高性能。

## 致谢

这项工作部分由ACE支持，ACE是JUMP 2.0中的七个中心之一，这是由DARPA和NSF奖项#2019306和#2118709赞助的半导体研究公司(SRC)项目，以及Intel ISRA奖项。我们感谢匿名审稿人、Adrian Sampson教授、Louis-Noël Pouchet教授、Rachit Nigram和Jialu Bao对这项工作初稿的宝贵反馈。我们感谢Jiahao Zhang提供参考LLM加速器实现。我们还感谢Jin Yang、Jeremy Casas和Zhenkun Yang对Allo框架初始版本的深刻反馈。

## 工件

Allo代码是开源的，可在GitHub的allo存储库中获得。详细的可重现性和可重用性说明在Zenodo[10]的存档版本和GitHub的allo-pldi24-artifact存储库中提供。

## 参考文献

[参考文献1-108列表略，包含完整引用信息]

## 附录A 正式数据类型注释

图15显示了Allo中类型注释的语法。Allo支持不同的Python内置类型、任意位宽整数类型和定点数据类型。